{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "identified-hayes",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hipopy.hipopy as hp\n",
    "#import os\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import awkward as ak\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch_geometric as tg\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "#import torch\n",
    "from torch_geometric.data import InMemoryDataset, download_url\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "#NOTE: NEW 2/20/23 \n",
    "from typing import List, Union\n",
    "\n",
    "from torch_geometric.data import Data, HeteroData\n",
    "from torch_geometric.data.datapipes import functional_transform\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "\n",
    "#import modeloss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "homeless-patrick",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyOwnDataset(105)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = '/work/clas12/users/mencke/pyg_test_rec_traj_dataset_5_28_24'\n",
    "class MyOwnDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.load(self.processed_paths[0])\n",
    "        # For PyG<2.4:\n",
    "        # self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['some_file_1', 'some_file_2']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "    def process(self):\n",
    "        # Read data into huge `Data` list.\n",
    "        data_list = None\n",
    "\n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    "\n",
    "        self.save(data_list, self.processed_paths[0])\n",
    "        # For PyG<2.4:\n",
    "        # torch.save(self.collate(data_list), self.processed_paths[0])\n",
    "\n",
    "# Create PyG Dataset\n",
    "#root = '/work/clas12/users/mfmce/pyg_test_rec_traj_dataset_5_28_24/' # 3_14_24 #OLD\n",
    "dataset = MyOwnDataset(\n",
    "            root,\n",
    "            transform=None, #T.Compose([T.ToUndirected(),T.KNNGraph(k=6)]),\n",
    "            pre_transform=None,\n",
    "            pre_filter=None\n",
    "        )\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adolescent-conflict",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(6, 64)\n",
      "  (conv2): GCNConv(64, 64)\n",
      "  (conv3): GCNConv(64, 64)\n",
      "  (lin1): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (lin2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (lin3): Linear(in_features=64, out_features=2, bias=True)\n",
      "  (bn1): GraphNorm(64)\n",
      "  (bn2): GraphNorm(64)\n",
      "  (bn3): GraphNorm(64)\n",
      ")\n",
      "\n",
      "dataset[0].pos =  None\n"
     ]
    }
   ],
   "source": [
    "#making the model \n",
    "\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn.norm import GraphNorm, BatchNorm \n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GCN, self).__init__()\n",
    "#         torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)#.jittable() #NOTE: NEEDED FOR DEPLOYMENT IN CMAKE\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)#.jittable()\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)#.jittable()\n",
    "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin3 = Linear(hidden_channels, out_channels)\n",
    "        self.bn1 = torch_geometric.nn.norm.GraphNorm(hidden_channels)\n",
    "        self.bn2 = torch_geometric.nn.norm.GraphNorm(hidden_channels)\n",
    "        self.bn3 = torch_geometric.nn.norm.GraphNorm(hidden_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "#         print(\"x = \",x)\n",
    "#         print(\"DEBUGGING: in GCN: begin: x.requires_grad = \",x.requires_grad)\n",
    "        x = self.conv1(x, edge_index) #input layer\n",
    "#         print(\"DEBUGGING: in GCN: self.conv1(x, edge_index): x.requires_grad = \",x.requires_grad)\n",
    "        x = self.bn1(x) #normalize it\n",
    "#         print(\"DEBUGGING: in GCN: self.bn2(x): x.requires_grad = \",x.requires_grad)\n",
    "#         print(\"self.conv1(x,edge_index) = \",x)\n",
    "        x = x.relu() #activation\n",
    "#         x = torch.nn.function.elu(x)\n",
    "#         print(\"DEBUGGING: in GCN: x.relu(): x.requires_grad = \",x.requires_grad)\n",
    "#         print(\"x.relu() = \",x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "#         print(\"DEBUGGING: in GCN: self.bn2(x): x.requires_grad = \",x.requires_grad)\n",
    "#         print(\"self.conv2(x,edge_index) = \",x)\n",
    "        x = x.relu()\n",
    "#         print(\"x.relu() = \",x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = self.bn3(x)\n",
    "#         print(\"self.conv3(x,edge_index) = \",x) \n",
    "#         print(\"DEBUGGING: in GCN: self.bn3(x): x.requires_grad = \",x.requires_grad)\n",
    "\n",
    "#         # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch) #what is this for.           # [batch_size, hidden_channels]\n",
    "#         print(\"self.conv2(global_mean_pool(x, batch)) = \",x)\n",
    "#         print(\"DEBUGGING: in GCN: global_mean_pool(x): x.requires_grad = \",x.requires_grad)\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training) #for overfitting\n",
    "#         print(\"DEBUGGING: in GCN: F.dropout: x.requires_grad = \",x.requires_grad)\n",
    "#         print(\"F.dropout(x, p=0.5, training=self.training) = \",x)\n",
    "        x = self.lin1(x) #why threee output layers thigns \n",
    "        x = self.lin2(x)\n",
    "        x = self.lin3(x)\n",
    "#         print(\"DEBUGGING: in GCN: self.lin*(x): x.requires_grad = \",x.requires_grad)\n",
    "#         print(\"self.lin3(x) = \",x)\n",
    "#         x = torch.sigmoid(x) #NOTE: DON'T SOFTMAX IF USING BCELOSS, USE SIGMOID INSTEAD\n",
    "#         print(\"torch.sigmoid(x) = \",x)\n",
    "#         print(\"DEBUGGING: in GCN: torch.sigmoid(x): x.requires_grad = \",x.requires_grad)\n",
    "        return x\n",
    "\n",
    "model = GCN(dataset.num_node_features,64,2)\n",
    "# model = GIN(in_channels=dataset.num_node_features,out_channels=2)\n",
    "print(model)\n",
    "#print(\"\\ndataset[0].pos = \",dataset[0].pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-falls",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change to GPU if I did not forget to do this. \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device = \",device)\n",
    "model = model.to(device)\n",
    "print(\"DEBUGGING: torch.cuda.is_available() = \",torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "occupied-senior",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data into training and testing. \n",
    "\n",
    "from torch.utils.data import random_split #TODO: SEE IF YOU CAN USE THIS\n",
    "# torch.manual_seed(12345)\n",
    "# print('DEBUGGING: BEFORE: dataset.y.shape = ',dataset.y.shape)\n",
    "dataset = dataset.shuffle() #shuffle (randmoize placement of it) not sure if this is needed. \n",
    "#print('DEBUGGING: AFTER:  dataset.y.shape = ',dataset.y.shape)\n",
    "\n",
    "#print(len(dataset)) \n",
    "\n",
    "fracs = [0.8, 0.1, 0.1] #percent of dataset used for training testing and validatoin 80%,10%,10% #NOTE: SHOULD CHECK np.sum(fracs) == 1 and len(fracs)==3\n",
    "fracs = [torch.sum(torch.tensor(fracs[:idx])) for idx in range(1,len(fracs)+1)] #get the indexes for training ... parts to use. \n",
    "#print(fracs)\n",
    "split1, split2 = [int(len(dataset)*frac) for frac in fracs[:-1]] \n",
    "train_dataset = dataset[:split1]\n",
    "val_dataset = dataset[split1:split2]\n",
    "test_dataset = dataset[split2:] \n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of validation graphs: {len(val_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')\n",
    "\n",
    "#not sure what this is for \n",
    "'''\n",
    "print(\"train_dataset\")\n",
    "for idx, d in enumerate(train_dataset):\n",
    "    if d.y.shape[0]>2:\n",
    "        print(\"d.y = \",d.y)\n",
    "        print(\"idx = \",idx)\n",
    "        \n",
    "print(\"val_dataset\")\n",
    "for idx, d in enumerate(val_dataset):\n",
    "    if d.y.shape[0]>2:\n",
    "        print(\"d.y = \",d.y)\n",
    "        print(\"idx = \",idx)\n",
    "        \n",
    "print(\"test_dataset\")\n",
    "for idx, d in enumerate(test_dataset):\n",
    "    if d.y.shape[0]>2:\n",
    "        print(\"d.y = \",d.y)\n",
    "        print(\"idx = \",idx)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-impossible",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "radio-bench",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m train_dataset\u001b[38;5;241m.\u001b[39my:\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i, \u001b[43mtrain_counts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "for i in train_dataset.y:\n",
    "    print(i, train_counts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "reduced-instrumentation",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 76 is out of bounds for axis 0 with size 76",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m train_weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(train_dataset\u001b[38;5;241m.\u001b[39my))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_dataset\u001b[38;5;241m.\u001b[39my)):\n\u001b[0;32m---> 11\u001b[0m     train_weights[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[43mtrain_counts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     12\u001b[0m train_sampler \u001b[38;5;241m=\u001b[39m WeightedRandomSampler(weights\u001b[38;5;241m=\u001b[39mtrain_weights, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataset), replacement\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m _, val_counts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(val_dataset\u001b[38;5;241m.\u001b[39my, return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 76 is out of bounds for axis 0 with size 76"
     ]
    }
   ],
   "source": [
    "#honestly do not really understand how this works/does \n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "# print(\"DEBUGGING: train_dataset.y.shape = \",train_dataset.y.shape)\n",
    "_, train_counts = np.unique(train_dataset.y, return_counts=True)\n",
    "#print(\"DEBUGGING: np.unique(train_dataset) = \",_,train_counts)\n",
    "#train_weights = [1/train_counts[i] for i in train_dataset.y]\n",
    "train_weights = np.zeros(len(train_dataset.y))\n",
    "for i in range(len(train_dataset.y)):\n",
    "    train_weights[i] = 1/train_counts[int(i)]\n",
    "train_sampler = WeightedRandomSampler(weights=train_weights, num_samples=len(train_dataset), replacement=True)\n",
    "_, val_counts = np.unique(val_dataset.y, return_counts=True)\n",
    "#print(\"DEBUGGING: np.unique(val_dataset) = \",_,val_counts)\n",
    "val_weights = [1/val_counts[i] for i in val_dataset.y]\n",
    "val_sampler = WeightedRandomSampler(weights=val_weights, num_samples=len(val_dataset), replacement=True)\n",
    "_, test_counts = np.unique(test_dataset.y, return_counts=True)\n",
    "# print(\"DEBUGGING: np.unique(test_dataset) = \",_,test_counts)\n",
    "test_weights = [1/test_counts[i] for i in test_dataset.y]\n",
    "test_sampler = WeightedRandomSampler(weights=test_weights, num_samples=len(test_dataset), replacement=True)\n",
    "\n",
    "batch_size = 32\n",
    "use_weighted_samplers = True \n",
    "if not use_weighted_samplers:\n",
    "    train_sampler, val_sampler, test_sampler = None, None, None\n",
    "train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size, shuffle=False)#, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, sampler=val_sampler, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)#NOTE: #TODO: Try no sampling here for evaluation...\n",
    "\n",
    "\n",
    "for step, data in enumerate(train_loader): \n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data)\n",
    "    print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-priest",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-continent",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-september",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rocky-finder",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
