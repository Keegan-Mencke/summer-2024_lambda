{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6efdf3c1-8db8-49a4-b3bd-5a80f3dc737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "code =\"\"\"print('testing_1')\n",
    "#import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "#import numpy.ma as ma \n",
    "#import awkward as ak\n",
    "#from tqdm import tqdm\n",
    "from typing import List, Union\n",
    "import torch\n",
    "import torch_geometric as tg \n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "#import torch\n",
    "from torch_geometric.data import InMemoryDataset, download_url\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "#NOTE: NEW 2/20/23 \n",
    "#from typing import List, Union\n",
    "\n",
    "from torch_geometric.data import Data, HeteroData\n",
    "from torch_geometric.data.datapipes import functional_transform\n",
    "from torch_geometric.transforms import BaseTransform  \n",
    "import os\n",
    "#taskID=int(os.environ['SLURM_ARRAY_TASK_ID'])\n",
    "roots = ['/hpc/group/vossenlab/kam264/pion_x_cx_rec_20000', '/hpc/group/vossenlab/kam264/pion_x_cx_recfull_20000',\n",
    "'/hpc/group/vossenlab/kam264/pion_x_cx_reczfull_20000', '/hpc/group/vossenlab/kam264/mom_magnitude_pi_x_cx_recmomfull_20000',\n",
    "'/hpc/group/vossenlab/kam264/mom_magnitude_pi_x_cx_recfull_vertmom_20000']\n",
    "#print(roots[int(taskID-1)])\n",
    "#root = roots[int(taskID-1)]\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "#root = '/work/clas12/users/mencke/pyg_test_rec_traj_dataset_111'\n",
    "root = '/hpc/group/vossenlab/kam264/testt_40000'\n",
    "print(root)\n",
    "class MyOwnDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.load(self.processed_paths[0])\n",
    "        # For PyG<2.4:\n",
    "        # self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['some_file_1', 'some_file_2']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "    def process(self):\n",
    "        # Read data into huge `Data` list.\n",
    "        data_list = None\n",
    "\n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    "\n",
    "        self.save(data_list, self.processed_paths[0])\n",
    "        # For PyG<2.4:\n",
    "        # torch.save(self.collate(data_list), self.processed_paths[0])\n",
    "\n",
    "# Create PyG Dataset\n",
    "#root = '/hpc/group/vossenlab/kam264/pyg_test_rec_traj_dataset_5_28_24/' # 3_14_24 #OLD\n",
    "dataset = MyOwnDataset(\n",
    "            root,\n",
    "            transform=None, #T.Compose([T.ToUndirected(),T.KNNGraph(k=6)]),\n",
    "            pre_transform=None,\n",
    "            pre_filter=None\n",
    "        )\n",
    "dataset\n",
    "\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn.norm import GraphNorm, BatchNorm  \n",
    "\n",
    "batch_size = 8 \n",
    "LR =1e-4\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GCN, self).__init__()\n",
    "#         torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)#.jittable() #NOTE: NEEDED FOR DEPLOYMENT IN CMAKE\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)#.jittable()\n",
    "        #self.block2 = nn.DataParallel(self.block2)\n",
    "        #self.conv2 = torch.nn.DataParallel(self.conv2) #this was trying the parallization thing. \n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)#.jittable()\n",
    "        #self.conv3 = torch.nn.DataParallel(self.conv3)\n",
    "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin3 = Linear(hidden_channels, out_channels)\n",
    "        self.bn1 = torch_geometric.nn.norm.GraphNorm(hidden_channels)\n",
    "        self.bn2 = torch_geometric.nn.norm.GraphNorm(hidden_channels)\n",
    "        self.bn3 = torch_geometric.nn.norm.GraphNorm(hidden_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch): \n",
    "        # 1. Obtain node embeddings \n",
    "       # x = self.conv1(x, edge_index) #input layer\n",
    "      #  x = self.bn1(x) #normalize it\n",
    "     #   x = x.relu() #activation\n",
    "#         x = torch.nn.function.elu(x)\n",
    "#        x = self.conv2(x, edge_index)\n",
    "#        x = self.bn2(x)\n",
    "#        x = x.relu() \n",
    "#         print(\"x.relu() = \",x)\n",
    "#        x = self.conv3(x, edge_index)\n",
    "#        x = self.bn3(x)\n",
    "#         # 2. Readout layer\n",
    " #       x = global_mean_pool(x, batch) #what is this for.           # [batch_size, hidden_channels]\n",
    "  #      x = F.dropout(x, p=0.5, training=self.training) #for overfitting\n",
    "   #     x = self.lin3(x)\n",
    "        \n",
    "        x = self.conv1(x, edge_index) #input layer                             \n",
    "                                                      \n",
    "        x = self.bn1(x) #normalize it                                          \n",
    "\n",
    "        x = x.relu() #activation                                               \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = x.relu()\n",
    "#         print(\"x.relu() = \",x)                                               \n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = self.bn3(x)\n",
    "#         # 2. Readout layer                                                   \n",
    "        x = global_mean_pool(x, batch)\n",
    "        # 3. Apply a final classifier                                          \n",
    "        x = F.dropout(x, p=0.5, training=self.training) #for overfittin        \n",
    "        x = self.lin3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = GCN(dataset.num_node_features,64,2)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') \n",
    "#devicee = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu') \n",
    "print(\"Device = \",device)\n",
    "model = model.to(device)\n",
    "print(\"DEBUGGING: torch.cuda.is_available() = \",torch.cuda.is_available())\n",
    "\n",
    "from torch.utils.data import random_split #TODO: SEE IF YOU CAN USE THIS\n",
    "# torch.manual_seed(12345)\n",
    "# print('DEBUGGING: BEFORE: dataset.y.shape = ',dataset.y.shape)\n",
    "#dataset = dataset.shuffle() #shuffle (randmoize placement of it) not sure if this is needed. \n",
    "#print('DEBUGGING: AFTER:  dataset.y.shape = ',dataset.y.shape)\n",
    "\n",
    "#print(len(dataset)) \n",
    "\n",
    "fracs = [0.8, 0.1, 0.1] #percent of dataset used for training testing and validatoin 80%,10%,10% #NOTE: SHOULD CHECK np.sum(fracs) == 1 and len(fracs)==3\n",
    "fracs = [torch.sum(torch.tensor(fracs[:idx])) for idx in range(1,len(fracs)+1)] #get the indexes for training ... parts to use. \n",
    "#print(fracs)\n",
    "split1, split2 = [int(len(dataset)*frac) for frac in fracs[:-1]] \n",
    "train_dataset = dataset[:split1]\n",
    "val_dataset = dataset[split1:split2]\n",
    "test_dataset = dataset[split2:] \n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of validation graphs: {len(val_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}') \n",
    "\n",
    "from torch_geometric.loader import DataLoader \n",
    "#from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)#, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset,  batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False) \n",
    "\n",
    "model = GCN(dataset.num_node_features, dataset.num_classes, 1).to(device) #initiate the model, #2 is the number of outputs here is 2 as pion_z, proton_z \n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= LR) #get the optimizer\n",
    "\n",
    "data_labels = train_dataset.y \n",
    "#weight_signal = counts[1]/counts[0]#DEBUGGING MULTIPLY BY 2 ...\n",
    "#print(\"weight_signal = \",weight_signal) \n",
    "# weight = torch.FloatTensor([weight_signal, 1.0]).to(device) #NOTE: That labels are [sg?,bg?] so label 0 in this case is sg and label 1 is bg.\n",
    "\n",
    "\n",
    "#losss = torch.nn.MSELoss(reduction = 'mean').to(device)\n",
    "#losss = torch.sqrt(losss)\n",
    "\n",
    "#RMSE loss. \n",
    "def RMSELoss(out,y):\n",
    "    return torch.sqrt(torch.mean((out-y)**2))\n",
    "losss = RMSELoss \n",
    "\n",
    "#custom losss\n",
    "def pion_los(out,y):\n",
    "    mse_pi = 0\n",
    "    for j in range(len(out)):\n",
    "            #x_pi = out[j][0]; x_p =out[j][1]                                                                                                                                                                                                 \n",
    "\n",
    "        mse_pi += (out[j][0]-y[j][0].item() )**2\n",
    "    return torch.sqrt(mse_pi/len(out))\n",
    "\n",
    "#losss = pion_los\n",
    "\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train() #initailize the model                                                                                                                                                                                                       \n",
    "    #for i, data in tqdm(enumerate(train_loader)): #perhaps tqdm(enumerate(train_loader)), i is index, data jsut moves through all the dtaa in trainingg                                                                                      \n",
    "    for i,data in enumerate(train_loader):\n",
    "        data = data.to(device) #switch to GPU                                                                                                                                                                                                 \n",
    "        optimizer.zero_grad() #                                                                                                                                                                                                               \n",
    "        out = model(data.x, data.edge_index, data.batch).to(device)  # Perform a single forward pass                                                                                                                                          \n",
    "        yy = []\n",
    "        for j in range(0,len(out)):\n",
    "            yy+= [[data.y[j].item()]]\n",
    "\n",
    "        yy = torch.tensor(yy).to(device)\n",
    "        #print(out)\n",
    "        #print(yy)\n",
    "        loss = losss(out, yy).to(device) #compute the loss  \n",
    "        #print(loss)\n",
    "        loss.backward() #get the gradients.                                                                                                                                                                                                   \n",
    "        optimizer.step() #take a step.                                                                                                                                                                                                        \n",
    "\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "#from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def test(loader):\n",
    "    length = len(loader.dataset)\n",
    "    model.eval() #evaluate teh model.                                                                                                                                                                                                         \n",
    "\n",
    "    #mse_tot = []                                                                                                                                                                                                                             \n",
    "    mse_total = 0\n",
    "    mse_pi = 0\n",
    "    mse_p = 0\n",
    "    #r                                                                                                                                                                                                                                        \n",
    "    #for data in tqdm(loader):  # Iterate in batches over the training/test dataset.                                                                                                                                                          \n",
    "    for data in loader:\n",
    "        data = data.to(device) #put to GPU                                                                                                                                                                                                    \n",
    "        out = model(data.x, data.edge_index, data.batch).to(device) #evalueate                                                                                                                                                                \n",
    "        #this and the for loop is converting data.y to a tensor in the same shape as out rows and 2 columns first is y_pion second is y_proton                                                                                                \n",
    "        yy = []\n",
    "        for j in range(0,len(out)):\n",
    "            yy+= [[data.y[j].item()]]\n",
    "        yy = torch.tensor(yy).to(device) \n",
    "        loss = losss(out, yy).cpu() #getting teh loss function                                                                                                                                                                                \n",
    "        mse_total+=loss.item() #getting the mse (total)                                                                                                                                                                                       \n",
    "        #for j in range(len(out)):\n",
    "            #x_pi = out[j][0]; x_p =out[j][1]                                                                                                                                                                                                 \n",
    "\n",
    "         #   mse_pi += (out[j][0].item()-yy[j][0].item() )**2\n",
    "          #  mse_p +=(out[j][1].item()-yy[j][1].item())**2\n",
    "\n",
    "        #)                                                                                                                                                                                                                                    \n",
    "    return mse_total/length #, np.sqrt(mse_total/length), mse_pi/length, np.sqrt(mse_pi/length), mse_p/length, np.sqrt(mse_p/length)\n",
    "\n",
    "        \n",
    "def print_out():\n",
    "    model.eval() #initailize the model                                                                                                                                                                                                        \n",
    "    #for i, data in tqdm(enumerate(train_loader)): #perhaps tqdm(enumerate(train_loader)), i is index, data jsut moves through all the dtaa in trainingg                                                                                      \n",
    "    outt= []\n",
    "    for i,data in enumerate(test_loader):\n",
    "        data = data.to(device) #switch to GPU                                                                                                                                                                                                 \n",
    "        optimizer.zero_grad() #                                                                                                                                                                                                               \n",
    "        out = model(data.x, data.edge_index, data.batch).to(device)  # Perform a single forward pass                                                                                                                                          \n",
    "        out = out.cpu()\n",
    "        #yy = []                                                                                                                                                                                                                              \n",
    "        #for j in range(0,2*len(out),2):                                                                                                                                                                                                      \n",
    "        #    fuckk = data.y[j]; fuckj = data.y[j+1]                                                                                                                                                                                           \n",
    "        #    yy +=[[fuckk.item(),fuckj.item()]]                                                                                                                                                                                               \n",
    "        #yy = torch.tensor(yy).to(device)                                                                                                                                                                                                     \n",
    "        #loss = losss(out, yy).cpu() #compute the loss                                                                                                                                                                                        \n",
    "        outt+=[[out.detach().numpy()]]\n",
    "    return outt\n",
    "\n",
    "# for name, param in model.named_parameters():                                                                                                                                                                                                \n",
    "# for name, param in model.named_parameters():                                                                                                                                                                                                \n",
    "nepochs =  5\n",
    "train_metrics = {'mse':[], \"rmse\":[], 'mse_pi':[], 'rmse_pi':[], 'mse_p':[], 'rmse_p':[] }\n",
    "vall_metrics = {'mse':[], \"rmse\":[], 'mse_pi':[], 'rmse_pi':[], 'mse_p':[], 'rmse_p':[] }\n",
    "\n",
    "for epoch in range(nepochs):  \n",
    "    '''\n",
    "    if epoch ==(nepochs-1):\n",
    "        model.eval()\n",
    "        outt = []\n",
    "        for i, data in enumerate(train_loader):\n",
    "            data = data.to(device) \n",
    "            out = model(data.x, data.edge_index, data.batch).to(device)\n",
    "            out = out.cpu()\n",
    "            outt+=[[out.detach().numpy()]]\n",
    "        #print(outt)\n",
    "        #break'''\n",
    "    #print(\"BEFORE TRAIN()\")                                                                                                                                                                                                                  \n",
    "    train()\n",
    "    #print(\"BEFORE TEST(TRAIN_LOADER)\")                                                                                                                                                                                                       \n",
    "    #train_mse, train_rmse, train_mse_pi, train_rmse_pi, train_mse_p, train_rmse_p = test(train_loader)\n",
    "    train_mse = test(train_loader)\n",
    "\n",
    "    train_metrics['mse'].append(train_mse)\n",
    "    #train_metrics['rmse'].append(train_rmse)\n",
    "    #train_metrics['mse_pi'].append(train_mse_pi)\n",
    "    #train_metrics['rmse_pi'].append(train_rmse_pi)\n",
    "    #train_metrics['mse_p'].append(train_mse_p)\n",
    "    #train_metrics['rmse_p'].append(train_rmse_p)\n",
    "\n",
    "    #print(\"BEFORE TEST(VAL_LOADER)\")                                                                                                                                                                                                         \n",
    "    #vall_mse, vall_rmse, vall_mse_pi, vall_rmse_pi, vall_mse_p, vall_rmse_p = test(val_loader)\n",
    "    vall_mse =test(val_loader) \n",
    "    #if epoch==0 or val_roc_auc >np.max(val_metrics[\"roc_auc\"]) :                                                                                                                                                                             \n",
    "    #    model_best_auc = model                                                                                                                                                                                                               \n",
    "    #    PATH = '/work/clas12/users/mfmce/CLAS12_Lambda_resolution_REU_2023/model_best_auc.pt'                                                                                                                                                \n",
    "    #    torch.save({                                                                                                                                                                                                                         \n",
    "    #        'epoch': epoch,                                                                                                                                                                                                                  \n",
    "    #        'model_state_dict': model.state_dict(),                                                                                                                                                                                          \n",
    "    #        'optimizer_state_dict': optimizer.state_dict(),                                                                                                                                                                                  \n",
    " #             'loss': loss,                                                                                                                                                                                                                  \n",
    "    #        }, PATH)                                                                                                                                                                                                                         \n",
    "\n",
    "    vall_metrics['mse'].append(vall_mse)\n",
    "    #vall_metrics['rmse'].append(vall_rmse)\n",
    "    #vall_metrics['mse_pi'].append(vall_mse)\n",
    "    #vall_metrics['rmse_pi'].append(vall_mse)\n",
    "    #vall_metrics['mse_p'].append(vall_mse)\n",
    "    #all_metrics['rmse_p'].append(vall_mse)\n",
    "    if epoch%9==0:\n",
    "        print(\"Epoch \",epoch,\" Train mse: \",train_mse)\n",
    "        print(\"Epoch \",epoch,\" Validation mse: \",vall_mse)\n",
    "        #print(\"Epoch \",epoch,\" Train mse: \",train_mse,\" Train rmse: \",train_rmse,\" Train mse pion: \",train_mse_pi,\n",
    "         #     \" Train rmse pion: \",train_rmse_pi, \"Train mse proton:\",train_mse_p, \"Train rmse proton:\",train_rmse_p)\n",
    "        #print(\"Epoch \",epoch,\" Validation mse: \",vall_mse,\" Validation rmse: \",vall_rmse,\" Validation mse pion: \",vall_mse_pi,\n",
    "        #      \" Validation rmse pion: \",vall_rmse_pi, \"Validation mse proton:\",vall_mse_p, \"Validation rmse proton:\", vall_rmse_p)\n",
    "    if epoch==(nepochs-1):\n",
    "        a = print_out()\n",
    "        print(\"Epoch \",epoch,\" Train mse: \",train_mse)\n",
    "        print(\"Epoch \",epoch,\" Validation mse: \",vall_mse)\n",
    "        #print(\"Epoch \",epoch,\" Train mse: \",train_mse,\" Train rmse: \",train_rmse,\" Train mse pion: \",train_mse_pi,\n",
    "        #      \" Train rmse pion: \",train_rmse_pi, \"Train mse proton:\",train_mse_p, \"Train rmse proton:\",train_rmse_p)\n",
    "        #print(\"Epoch \",epoch,\" Validation mse: \",vall_mse,\" Validation rmse: \",vall_rmse,\" Validation mse pion: \",vall_mse_pi,\n",
    "        #      \" Validation rmse pion: \",vall_rmse_pi, \"Validation mse proton:\",vall_mse_p, \"Validation rmse proton:\", vall_rmse_p) \n",
    "\n",
    "        #print(a)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = [i for i in range(len(train_metrics[\"mse\"]))] \n",
    "#plt.figure()\n",
    "#plt.title('Training epoch vs. MSE') \n",
    "#print(plt.plot(epochs, train_metrics['mse']) )\n",
    "#plt.xlabel('epoch')\n",
    "#plt.ylabel('MSE')\n",
    "\n",
    "\n",
    "#plt.title('Validation epoch vs. MSE')  \n",
    "#print(plt.plot(epochs, vall_metrics['mse']))\n",
    "#plt.xlabel('epoch') \n",
    "#plt.ylabel('MSE') \n",
    "\n",
    "pi_x = []; p_x = [] \n",
    "for i in range(len(a)):\n",
    "    for j in range(len(a[i][0])): \n",
    "        pi_x.append(a[i][0][j][0].item())\n",
    "        #p_x.append(a[i][0][j][1].item()) \n",
    "\n",
    "pi_yy = [] \n",
    "#p_yy = []\n",
    "\n",
    "pi_rec_y = []\n",
    "#p_rec_y = []\n",
    "for i, data in enumerate(test_loader):\n",
    "    data = data#.to(device) #put to GPU\n",
    "    #break\n",
    "    for j in range(0,int(len(data.y))): \n",
    "        pi_yy.append(data.y[j].item())\n",
    " #       p_yy.append(data.y[j+1].item())\n",
    "\n",
    "        #pi_rec_y.append(data.rec[j].item()) \n",
    "       # p_rec_y.append(data.rec[j+1].item())\n",
    "    for j in range(len(data.rec)):\n",
    "        #pi_rec_y.append(data.rec[j][0][0])\n",
    "        pi_rec_y.append(data.rec[j][0])\n",
    "       # p_rec_y.append(data.rec[j][0][1])\n",
    "\n",
    "\n",
    "plott_pi = np.zeros((len(pi_x),3))\n",
    "#plott_p = np.zeros((len(p_x),2)) \n",
    "for i in range(len(pi_x)):\n",
    "    plott_pi[i][0] = pi_yy[i]\n",
    "    plott_pi[i][1] = pi_x[i]\n",
    "   # plott_pi[i][2] = pi_rec_y[i]\n",
    "   # plott_p[i][0] = p_yy[i]\n",
    "   # plott_p[i][1] = p_x[i]\n",
    "    #plott_p[i][2]  = p_rec_y[i] \n",
    "for i in range(len(pi_rec_y)): \n",
    "    plott_pi[i][2] = pi_rec_y[i]\n",
    "#plott_pi \n",
    "plt.figure()\n",
    "plt.title('Expected (orange) vs predicted (blue) of Pion momnetum ') \n",
    "plt.axvline(x = np.mean(pi_x), color = 'blue')\n",
    "plt.axvline(x = np.mean(pi_yy), color = 'orange')\n",
    "plt.hist(x = plott_pi, histtype ='step', color = ['orange', 'blue', 'green'], bins = 500)\n",
    "plt.xlim((-8,2))\n",
    "plt.imshow()\n",
    "#plt.savefig(\"hpc/volatile/group/vossenlab/dir_name_2/output.pdf\")\n",
    "from scipy.stats import kstest   \n",
    "print(kstest(pi_x, pi_yy)) \n",
    "print(kstest(pi_rec_y, pi_yy)) \n",
    "diff_x = []\n",
    "diff_rec = []\n",
    "for i in range(len(pi_x)):\n",
    "    diff_x.append((pi_x[i]-pi_yy[i])**2)\n",
    "    diff_rec.append((pi_rec_y[i]-pi_yy[i])**2) \n",
    "print(np.mean(diff_x)) \n",
    "print(np.mean(diff_rec))\"\"\"\n",
    " \n",
    "f = open(\"/hpc/group/vossenlab/kam264/dir_name_2/batch_filer.py\", \"w\")\n",
    "f.write(code) \n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10fa9ab2-be00-46aa-9fbc-667587aeafb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this onee\n",
    "code = \"\"\"import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import numpy.ma as ma \n",
    "import awkward as ak\n",
    "from tqdm import tqdm \n",
    "import torch \n",
    "import torch_geometric as tg \n",
    "import torch_geometric \n",
    "from torch_geometric.data import Data \n",
    "#import torch\n",
    "from torch_geometric.data import InMemoryDataset, download_url  \n",
    "import torch_geometric.transforms as T  \n",
    "\n",
    "#NOTE: NEW 2/20/23     \n",
    "from typing import List, Union  \n",
    "\n",
    "from torch_geometric.data import Data, HeteroData \n",
    "from torch_geometric.data.datapipes import functional_transform\n",
    "from torch_geometric.transforms import BaseTransform     \n",
    "##root = '/hpc/group/vossenlab/kam264/testt_30000' \n",
    "#root = '/hpc/group/vossenlab/kam264/testmrone_20000' \n",
    "#root = '/hpc/group/vossenlab/kam264/testrkin_20000' #really bad\n",
    "#root = '/hpc/group/vossenlab/kam264/test_momcut_20000' \n",
    "#root = '/hpc/group/vossenlab/kam264/ttest_momcut_30000'\n",
    "#root = '/hpc/group/vossenlab/kam264/ttest_momcut_theta_30000'\n",
    "root = '/hpc/group/vossenlab/kam264/ttest_momcut_phi_30000' \n",
    "print(root) \n",
    "\n",
    "class MyOwnDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.load(self.processed_paths[0])\n",
    "        # For PyG<2.4:\n",
    "        # self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['some_file_1', 'some_file_2']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "    def process(self):\n",
    "        # Read data into huge `Data` list.\n",
    "        data_list = None\n",
    "\n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    "\n",
    "        self.save(data_list, self.processed_paths[0])\n",
    "        # For PyG<2.4:\n",
    "        # torch.save(self.collate(data_list), self.processed_paths[0])\n",
    "\n",
    "# Create PyG Dataset\n",
    "#root = '/hpc/group/vossenlab/kam264/pyg_test_rec_traj_dataset_5_28_24/' # 3_14_24 #OLD\n",
    "dataset = MyOwnDataset(\n",
    "            root,\n",
    "            transform=None, #T.Compose([T.ToUndirected(),T.KNNGraph(k=6)]),\n",
    "            pre_transform=None,\n",
    "            pre_filter=None\n",
    "        )\n",
    "dataset\n",
    "\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn.norm import GraphNorm, BatchNorm \n",
    "\n",
    "batch_size = 64 \n",
    "LR =1e-3\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GCN, self).__init__()\n",
    "#         torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)#.jittable() #NOTE: NEEDED FOR DEPLOYMENT IN CMAKE\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)#.jittable()\n",
    "        #self.block2 = nn.DataParallel(self.block2)\n",
    "        #self.conv2 = torch.nn.DataParallel(self.conv2) #this was trying the parallization thing. \n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)#.jittable()\n",
    "        #self.conv3 = torch.nn.DataParallel(self.conv3)\n",
    "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin3 = Linear(hidden_channels, out_channels)\n",
    "        self.bn1 = torch_geometric.nn.norm.GraphNorm(hidden_channels)\n",
    "        self.bn2 = torch_geometric.nn.norm.GraphNorm(hidden_channels)\n",
    "        self.bn3 = torch_geometric.nn.norm.GraphNorm(hidden_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch): \n",
    "        # 1. Obtain node embeddings \n",
    "       # x = self.conv1(x, edge_index) #input layer\n",
    "      #  x = self.bn1(x) #normalize it\n",
    "     #   x = x.relu() #activation\n",
    "#         x = torch.nn.function.elu(x)\n",
    "#        x = self.conv2(x, edge_index)\n",
    "#        x = self.bn2(x)\n",
    "#        x = x.relu() \n",
    "#         print(\"x.relu() = \",x)\n",
    "#        x = self.conv3(x, edge_index)\n",
    "#        x = self.bn3(x)\n",
    "#         # 2. Readout layer\n",
    " #       x = global_mean_pool(x, batch) #what is this for.           # [batch_size, hidden_channels]\n",
    "  #      x = F.dropout(x, p=0.5, training=self.training) #for overfitting\n",
    "   #     x = self.lin3(x)\n",
    "        \n",
    "        x = self.conv1(x, edge_index) #input layer                             \n",
    "                                                      \n",
    "        x = self.bn1(x) #normalize it                                          \n",
    "\n",
    "        x = x.relu() #activation                                               \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = x.relu()\n",
    "#         print(\"x.relu() = \",x)  \n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = self.bn3(x)\n",
    "#         # 2. Readout layer                                                   \n",
    "        x = global_mean_pool(x, batch)\n",
    "        # 3. Apply a final classifier                                          \n",
    "        x = F.dropout(x, p=0.5, training=self.training) #for overfittin        \n",
    "        x = self.lin3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = GCN(dataset.num_node_features,64,2)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') \n",
    "#devicee = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu') \n",
    "print(\"Device = \",device)\n",
    "model = model.to(device)\n",
    "print(\"DEBUGGING: torch.cuda.is_available() = \",torch.cuda.is_available())\n",
    "\n",
    "from torch.utils.data import random_split #TODO: SEE IF YOU CAN USE THIS\n",
    "# torch.manual_seed(12345)\n",
    "# print('DEBUGGING: BEFORE: dataset.y.shape = ',dataset.y.shape)\n",
    "dataset = dataset.shuffle() #shuffle (randmoize placement of it) not sure if this is needed. \n",
    "#print('DEBUGGING: AFTER:  dataset.y.shape = ',dataset.y.shape)\n",
    "\n",
    "#print(len(dataset)) \n",
    "\n",
    "fracs = [0.8, 0.1, 0.1] #percent of dataset used for training testing and validatoin 80%,10%,10% #NOTE: SHOULD CHECK np.sum(fracs) == 1 and len(fracs)==3\n",
    "fracs = [torch.sum(torch.tensor(fracs[:idx])) for idx in range(1,len(fracs)+1)] #get the indexes for training ... parts to use. \n",
    "#print(fracs)\n",
    "split1, split2 = [int(len(dataset)*frac) for frac in fracs[:-1]] \n",
    "train_dataset = dataset[:split1]\n",
    "val_dataset = dataset[split1:split2]\n",
    "test_dataset = dataset[split2:] \n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of validation graphs: {len(val_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}') \n",
    "\n",
    "from torch_geometric.loader import DataLoader \n",
    "#from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)#, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset,  batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False) \n",
    "\n",
    "model = GCN(dataset.num_node_features, dataset.num_classes, 1).to(device) #initiate the model, #2 is the number of outputs here is 2 as pion_z, proton_z \n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= LR) #get the optimizer\n",
    "\n",
    "data_labels = train_dataset.y \n",
    "#weight_signal = counts[1]/counts[0]#DEBUGGING MULTIPLY BY 2 ...\n",
    "#print(\"weight_signal = \",weight_signal) \n",
    "# weight = torch.FloatTensor([weight_signal, 1.0]).to(device) #NOTE: That labels are [sg?,bg?] so label 0 in this case is sg and label 1 is bg.\n",
    "\n",
    "\n",
    "#losss = torch.nn.MSELoss(reduction = 'mean').to(device)\n",
    "#losss = torch.sqrt(losss)\n",
    "\n",
    "#RMSE loss. \n",
    "def RMSELoss(out,y):\n",
    "    return torch.sqrt(torch.mean((out-y)**2))\n",
    "losss = RMSELoss \n",
    "\n",
    "#custom losss\n",
    "def pion_los(out,y):\n",
    "    mse_pi = 0\n",
    "    for j in range(len(out)):\n",
    "            #x_pi = out[j][0]; x_p =out[j][1]                                                                                                                                                                                                 \n",
    "\n",
    "        mse_pi += (out[j][0]-y[j][0].item() )**2\n",
    "    return torch.sqrt(mse_pi/len(out))\n",
    "\n",
    "#losss = pion_los\n",
    "\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train() #initailize the model                                                                                                                                                                                                       \n",
    "    #for i, data in tqdm(enumerate(train_loader)): #perhaps tqdm(enumerate(train_loader)), i is index, data jsut moves through all the dtaa in trainingg                                                                                      \n",
    "    for i,data in enumerate(train_loader):\n",
    "        data = data.to(device) #switch to GPU                                                                                                                                                                                                 \n",
    "        optimizer.zero_grad() #                                                                                                                                                                                                               \n",
    "        out = model(data.x, data.edge_index, data.batch).to(device)  # Perform a single forward pass                                                                                                                                          \n",
    "        yy = []\n",
    "        for j in range(0,len(out)):\n",
    "            yy+= [[data.y[j].item()]]\n",
    "\n",
    "        yy = torch.tensor(yy).to(device)\n",
    "        #print(out)\n",
    "        #print(yy)\n",
    "        loss = losss(out, yy).to(device) #compute the loss  \n",
    "        #print(loss)\n",
    "        loss.backward() #get the gradients.                                                                                                                                                                                                   \n",
    "        optimizer.step() #take a step.                                                                                                                                                                                                        \n",
    "\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "#from sklearn.metrics import roc_auc_score \n",
    "\n",
    "def test(loader):\n",
    "    length = len(loader.dataset)\n",
    "    model.eval() #evaluate teh model.                                                                                                                                                                                                         \n",
    "\n",
    "    #mse_tot = []                                                                                                                                                                                                                             \n",
    "    mse_total = 0\n",
    "    mse_pi = 0\n",
    "    mse_p = 0\n",
    "    #r                                                                                                                                                                                                                                        \n",
    "    #for data in tqdm(loader):  # Iterate in batches over the training/test dataset.                                                                                                                                                          \n",
    "    for data in loader:\n",
    "        data = data.to(device) #put to GPU                                                                                                                                                                                                    \n",
    "        out = model(data.x, data.edge_index, data.batch).to(device) #evalueate                                                                                                                                                                \n",
    "        #this and the for loop is converting data.y to a tensor in the same shape as out rows and 2 columns first is y_pion second is y_proton                                                                                                \n",
    "        yy = []\n",
    "        for j in range(0,len(out)):\n",
    "            yy+= [[data.y[j].item()]]\n",
    "        yy = torch.tensor(yy).to(device) \n",
    "        loss = losss(out, yy).cpu() #getting teh loss function                                                                                                                                                                                \n",
    "        mse_total+=loss.item() #getting the mse (total)                                                                                                                                                                                       \n",
    "        #for j in range(len(out)):\n",
    "            #x_pi = out[j][0]; x_p =out[j][1]                                                                                                                                                                                                 \n",
    "\n",
    "         #   mse_pi += (out[j][0].item()-yy[j][0].item() )**2\n",
    "          #  mse_p +=(out[j][1].item()-yy[j][1].item())**2\n",
    "\n",
    "        #)                                                                                                                                                                                                                                    \n",
    "    return mse_total/length #, np.sqrt(mse_total/length), mse_pi/length, np.sqrt(mse_pi/length), mse_p/length, np.sqrt(mse_p/length) \n",
    "\n",
    "        \n",
    "def print_out():\n",
    "    model.eval() #initailize the model                                                                                                                                                                                                        \n",
    "    #for i, data in tqdm(enumerate(train_loader)): #perhaps tqdm(enumerate(train_loader)), i is index, data jsut moves through all the dtaa in trainingg                                                                                      \n",
    "    outt= []\n",
    "    for i,data in enumerate(test_loader):\n",
    "        data = data.to(device) #switch to GPU                                                                                                                                                                                                 \n",
    "        optimizer.zero_grad() #                                                                                                                                                                                                               \n",
    "        out = model(data.x, data.edge_index, data.batch).to(device)  # Perform a single forward pass                                                                                                                                          \n",
    "        out = out.cpu()\n",
    "        #yy = []                                                                                                                                                                                                                              \n",
    "        #for j in range(0,2*len(out),2):                                                                                                                                                                                                      \n",
    "        #    fuckk = data.y[j]; fuckj = data.y[j+1]                                                                                                                                                                                           \n",
    "        #    yy +=[[fuckk.item(),fuckj.item()]]                                                                                                                                                                                               \n",
    "        #yy = torch.tensor(yy).to(device)                                                                                                                                                                                                     \n",
    "        #loss = losss(out, yy).cpu() #compute the loss                                                                                                                                                                                        \n",
    "        outt+=[[out.detach().numpy()]]\n",
    "    return outt\n",
    "nepochs =  38\n",
    "train_metrics = {'mse':[], \"rmse\":[], 'mse_pi':[], 'rmse_pi':[], 'mse_p':[], 'rmse_p':[] } \n",
    "vall_metrics = {'mse':[], \"rmse\":[], 'mse_pi':[], 'rmse_pi':[], 'mse_p':[], 'rmse_p':[] }\n",
    "\n",
    "for epoch in range(nepochs):  \n",
    "    '''\n",
    "    if epoch ==(nepochs-1):\n",
    "        model.eval()\n",
    "        outt = []\n",
    "        for i, data in enumerate(train_loader):\n",
    "            data = data.to(device) \n",
    "            out = model(data.x, data.edge_index, data.batch).to(device)\n",
    "            out = out.cpu()\n",
    "            outt+=[[out.detach().numpy()]]\n",
    "        #print(outt)\n",
    "        #break'''\n",
    "    #print(\"BEFORE TRAIN()\")                                                                                                                                                                                                                  \n",
    "    train()\n",
    "    #print(\"BEFORE TEST(TRAIN_LOADER)\")                                                                                                                                                                                                       \n",
    "    #train_mse, train_rmse, train_mse_pi, train_rmse_pi, train_mse_p, train_rmse_p = test(train_loader)\n",
    "    train_mse = test(train_loader)\n",
    "\n",
    "    train_metrics['mse'].append(train_mse) \n",
    "    #train_metrics['rmse'].append(train_rmse)\n",
    "    #train_metrics['mse_pi'].append(train_mse_pi)\n",
    "    #train_metrics['rmse_pi'].append(train_rmse_pi)\n",
    "    #train_metrics['mse_p'].append(train_mse_p)\n",
    "    #train_metrics['rmse_p'].append(train_rmse_p)\n",
    "\n",
    "    #print(\"BEFORE TEST(VAL_LOADER)\")                                                                                                                                                                                                         \n",
    "    #vall_mse, vall_rmse, vall_mse_pi, vall_rmse_pi, vall_mse_p, vall_rmse_p = test(val_loader)\n",
    "    vall_mse =test(val_loader) \n",
    "    #if epoch==0 or val_roc_auc >np.max(val_metrics[\"roc_auc\"]) :                                                                                                                                                                             \n",
    "    #    model_best_auc = model                                                                                                                                                                                                               \n",
    "    #    PATH = '/work/clas12/users/mfmce/CLAS12_Lambda_resolution_REU_2023/model_best_auc.pt'                                                                                                                                                \n",
    "    #    torch.save({                                                                                                                                                                                                                         \n",
    "    #        'epoch': epoch,                                                                                                                                                                                                                  \n",
    "    #        'model_state_dict': model.state_dict(),                                                                                                                                                                                          \n",
    "    #        'optimizer_state_dict': optimizer.state_dict(),                                                                                                                                                                                  \n",
    " #             'loss': loss,                                                                                                                                                                                                                  \n",
    "    #        }, PATH)                                                                                                                                                                                                                         \n",
    "\n",
    "    vall_metrics['mse'].append(vall_mse)\n",
    "    #vall_metrics['rmse'].append(vall_rmse)\n",
    "    #vall_metrics['mse_pi'].append(vall_mse)\n",
    "    #vall_metrics['rmse_pi'].append(vall_mse)\n",
    "    #vall_metrics['mse_p'].append(vall_mse)\n",
    "    #all_metrics['rmse_p'].append(vall_mse)\n",
    "    if epoch%9==0:\n",
    "        print(\"Epoch \",epoch,\" Train mse: \",train_mse)\n",
    "        print(\"Epoch \",epoch,\" Validation mse: \",vall_mse)\n",
    "        #print(\"Epoch \",epoch,\" Train mse: \",train_mse,\" Train rmse: \",train_rmse,\" Train mse pion: \",train_mse_pi,\n",
    "         #     \" Train rmse pion: \",train_rmse_pi, \"Train mse proton:\",train_mse_p, \"Train rmse proton:\",train_rmse_p)\n",
    "        #print(\"Epoch \",epoch,\" Validation mse: \",vall_mse,\" Validation rmse: \",vall_rmse,\" Validation mse pion: \",vall_mse_pi,\n",
    "        #      \" Validation rmse pion: \",vall_rmse_pi, \"Validation mse proton:\",vall_mse_p, \"Validation rmse proton:\", vall_rmse_p)\n",
    "    if epoch==(nepochs-1):\n",
    "        a = print_out()\n",
    "        print(\"Epoch \",epoch,\" Train mse: \",train_mse)\n",
    "        print(\"Epoch \",epoch,\" Validation mse: \",vall_mse)\n",
    "        #print(\"Epoch \",epoch,\" Train mse: \",train_mse,\" Train rmse: \",train_rmse,\" Train mse pion: \",train_mse_pi,\n",
    "        #      \" Train rmse pion: \",train_rmse_pi, \"Train mse proton:\",train_mse_p, \"Train rmse proton:\",train_rmse_p)\n",
    "        #print(\"Epoch \",epoch,\" Validation mse: \",vall_mse,\" Validation rmse: \",vall_rmse,\" Validation mse pion: \",vall_mse_pi,\n",
    "        #      \" Validation rmse pion: \",vall_rmse_pi, \"Validation mse proton:\",vall_mse_p, \"Validation rmse proton:\", vall_rmse_p) \n",
    "\n",
    "        #print(a)\n",
    "        #b = print_outb()\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "epochs = [i for i in range(len(train_metrics[\"mse\"]))]  \n",
    "plt.figure()\n",
    "plt.title('Training epoch vs. MSE') \n",
    "plt.plot(epochs, train_metrics['mse']) \n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('MSE')\n",
    "print() \n",
    "print()\n",
    "\n",
    "plt.title('Validation epoch vs. MSE')   \n",
    "plt.plot(epochs, vall_metrics['mse']) \n",
    "plt.xlabel('epoch') \n",
    "plt.ylabel('MSE')  \n",
    "plt.savefig(\"output111pp.pdf\") ####################################################################\n",
    "pi_x = []; p_x = [] \n",
    "#pi_val = []\n",
    "for i in range(len(a)):\n",
    "    for j in range(len(a[i][0])): \n",
    "        pi_x.append(a[i][0][j][0].item())\n",
    "        \n",
    "        #p_x.append(a[i][0][j][1].item()) \n",
    "\n",
    "pi_yy = [] \n",
    "#p_yy = []\n",
    "\n",
    "pi_rec_y = []\n",
    "#p_rec_y = []\n",
    "for i, data in enumerate(test_loader):\n",
    "    data = data#.to(device) #put to GPU\n",
    "    #break\n",
    "    for j in range(0,int(len(data.y))): \n",
    "        pi_yy.append(data.y[j].item())\n",
    " #       p_yy.append(data.y[j+1].item())\n",
    "\n",
    "        #pi_rec_y.append(data.rec[j].item()) \n",
    "       # p_rec_y.append(data.rec[j+1].item())\n",
    "    for j in range(len(data.rec)):\n",
    "        #pi_rec_y.append(data.rec[j][0][0])\n",
    "        pi_rec_y.append(data.rec[j][0])\n",
    "       # p_rec_y.append(data.rec[j][0][1])\n",
    "\n",
    "\n",
    "#plott_pi = np.zeros((len(pi_x),4))\n",
    "#plott_p = np.zeros((len(p_x),2)) \n",
    "plott_pi = np.zeros((len(pi_x),3))\n",
    "for i in range(len(pi_x)):\n",
    "#for i in range(982):\n",
    "    plott_pi[i][0] = pi_yy[i]\n",
    "    plott_pi[i][1] = pi_x[i]\n",
    "    #plott_pi[i][3] = pi_val[i]\n",
    "   # plott_pi[i][2] = pi_rec_y[i] \n",
    "   # plott_p[i][0] = p_yy[i]\n",
    "   # plott_p[i][1] = p_x[i]\n",
    "    #plott_p[i][2]  = p_rec_y[i] \n",
    "for i in range(len(pi_rec_y)): \n",
    "    plott_pi[i][2] = pi_rec_y[i] \n",
    "#plott_pi \n",
    "plt.title('Distribution of Pion phi momentum') \n",
    "plt.axvline(x = np.mean(pi_x), color = 'blue')\n",
    "plt.axvline(x = np.mean(pi_yy), color = 'orange') \n",
    "plt.hist(x = plott_pi, histtype ='step', color = ['orange', 'blue', 'green'], bins = 50)\n",
    "plt.legend(['REC', 'True', 'Predicted']) \n",
    "plt.xlim((-4,4))  \n",
    "plt.ylabel('count')\n",
    "plt.xlabel('Pion momentum ')\n",
    "#plt.savefig(\"/hpc/volatile/group/vossenlab/dir_name_2/\")\n",
    "plt.savefig(\"output222pp.pdf\") #######################################################################\n",
    "from scipy.stats import kstest  \n",
    "print('kstate for predicted')\n",
    "print(kstest(pi_x, pi_yy)) \n",
    "print()\n",
    "print('kstate for rec')\n",
    "print(kstest(pi_rec_y, pi_yy))  \n",
    "print()\n",
    "diff_x = [] \n",
    "diff_rec = []\n",
    "for i in range(len(pi_x)):\n",
    "    diff_x.append(np.sqrt((pi_x[i]-pi_yy[i])**2))\n",
    "    diff_rec.append(np.sqrt((pi_rec_y[i]-pi_yy[i])**2))\n",
    "print('rmse for predicted')\n",
    "print(np.mean(diff_x)) \n",
    "print('rmse for rec')\n",
    "print(np.mean(diff_rec)) \n",
    "print()\n",
    "\n",
    "dif_x = [] \n",
    "dif_rec = []\n",
    "for i in range(len(pi_x)):\n",
    "    dif_x.append((pi_x[i]-pi_yy[i]))\n",
    "    dif_rec.append((pi_rec_y[i]-pi_yy[i]))\n",
    "\n",
    "plottt = np.zeros((len(dif_x), 2))\n",
    "for i in range(len(dif_x)):\n",
    "    plottt[i][0] = dif_x[i]\n",
    "    plottt[i][1] = dif_rec[i]\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Event by event error')\n",
    "n, bins = plt.hist(x = plottt, histtype = 'step', color = ['blue', 'orange'], bins = 50)[-1]\n",
    "plt.legend(['REC', 'Predicted'])\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('error')\n",
    "\n",
    "plt.xlim((-5, 5)) \n",
    "plt.savefig(\"output333pp.pdf\") #########################################################################\n",
    "\n",
    "from statistics import NormalDist\n",
    "norm = NormalDist.from_samples(dif_x)\n",
    "print('predicted mean')\n",
    "print(norm.mean)\n",
    "print()\n",
    "print('predicted stdev')\n",
    "print(norm.stdev) \n",
    "print()\n",
    "norm = NormalDist.from_samples(dif_rec) \n",
    "print('rec mean')\n",
    "print(norm.mean)\n",
    "print()\n",
    "print('rec stdev')\n",
    "print(norm.stdev) \n",
    "\n",
    "print()\n",
    "bb= plt.hist(x = plottt, histtype = 'step', color = ['blue', 'orange'], bins = 50) \n",
    "pred_values= np.zeros(len(bb[0][0]))\n",
    "xx = np.zeros(len(bb[1]))\n",
    "for i in range(len(bb[0][0])):\n",
    "    pred_values[i] = bb[0][0][i]\n",
    "    xx[i] = bb[1][i]\n",
    "\n",
    "from scipy.optimize import curve_fit \n",
    "def gauss(x,  A, x0, sigma): \n",
    "    return A * np.exp(-(x - x0) ** 2 / (2 * sigma ** 2))\n",
    "\n",
    "xx = np.delete(xx, 0)\n",
    "parameters, covariance = curve_fit(gauss, xx, pred_values )\n",
    "\n",
    "print(parameters) \n",
    "plt.figure()\n",
    "\n",
    "plt.title('Event by event error')\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('error')\n",
    "plt.hist(x = plottt, histtype = 'step', color = ['blue', 'orange'], bins = 50)\n",
    "\n",
    "def gauss(x ): \n",
    "    A = parameters[0]; x0 =parameters[1]; sigma = parameters[2]\n",
    "    return A * np.exp(-(x - x0) ** 2 / (2 * sigma ** 2))\n",
    "xx = np.delete(xx, -1) \n",
    "plt.plot( xx, gauss(xx), color = 'purple')  \n",
    "\n",
    "plt.xlim((-5, 5))   \n",
    "plt.ylabel('count')\n",
    "plt.xlabel('error')\n",
    "plt.savefig(\"output444pp.pdf\") #########################################################################\n",
    "\"\"\"\n",
    "f = open(\"/hpc/group/vossenlab/kam264/dir_name_2/batch_filer.py\", \"w\") \n",
    "f.write(code)  \n",
    "f.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad73974f-58a4-4fdb-a295-9d6c5dddce8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e724ea12-b353-478b-86eb-0c4dd11510fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this onee\n",
    "code = \"\"\"import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import numpy.ma as ma \n",
    "import awkward as ak\n",
    "from tqdm import tqdm \n",
    "import torch \n",
    "import torch_geometric as tg \n",
    "import torch_geometric \n",
    "from torch_geometric.data import Data \n",
    "#import torch\n",
    "from torch_geometric.data import InMemoryDataset, download_url  \n",
    "import torch_geometric.transforms as T  \n",
    "\n",
    "#NOTE: NEW 2/20/23     \n",
    "from typing import List, Union  \n",
    "\n",
    "from torch_geometric.data import Data, HeteroData \n",
    "from torch_geometric.data.datapipes import functional_transform\n",
    "from torch_geometric.transforms import BaseTransform     \n",
    "##root = '/hpc/group/vossenlab/kam264/testt_30000' \n",
    "#root = '/hpc/group/vossenlab/kam264/testmrone_20000' \n",
    "#root = '/hpc/group/vossenlab/kam264/testrkin_20000' #really bad\n",
    "#root = '/hpc/group/vossenlab/kam264/test_momcut_20000' \n",
    "#root = '/hpc/group/vossenlab/kam264/ttest_momcut_30000'\n",
    "#root = '/hpc/group/vossenlab/kam264/ttest_momcut_theta_30000'\n",
    "root = '/hpc/group/vossenlab/kam264/ttest_momcut_phi_30000' \n",
    "print(root) \n",
    "\n",
    "class MyOwnDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.load(self.processed_paths[0])\n",
    "        # For PyG<2.4:\n",
    "        # self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['some_file_1', 'some_file_2']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "    def process(self):\n",
    "        # Read data into huge `Data` list.\n",
    "        data_list = None\n",
    "\n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    "\n",
    "        self.save(data_list, self.processed_paths[0])\n",
    "        # For PyG<2.4:\n",
    "        # torch.save(self.collate(data_list), self.processed_paths[0])\n",
    "\n",
    "# Create PyG Dataset\n",
    "#root = '/hpc/group/vossenlab/kam264/pyg_test_rec_traj_dataset_5_28_24/' # 3_14_24 #OLD\n",
    "dataset = MyOwnDataset(\n",
    "            root,\n",
    "            transform=None, #T.Compose([T.ToUndirected(),T.KNNGraph(k=6)]),\n",
    "            pre_transform=None,\n",
    "            pre_filter=None\n",
    "        )\n",
    "dataset\n",
    "\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn.norm import GraphNorm, BatchNorm \n",
    "\n",
    "batch_size = 64 \n",
    "LR =1e-3\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GCN, self).__init__()\n",
    "#         torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)#.jittable() #NOTE: NEEDED FOR DEPLOYMENT IN CMAKE\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)#.jittable()\n",
    "        #self.block2 = nn.DataParallel(self.block2)\n",
    "        #self.conv2 = torch.nn.DataParallel(self.conv2) #this was trying the parallization thing. \n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)#.jittable()\n",
    "        #self.conv3 = torch.nn.DataParallel(self.conv3)\n",
    "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin3 = Linear(hidden_channels, out_channels)\n",
    "        self.bn1 = torch_geometric.nn.norm.GraphNorm(hidden_channels)\n",
    "        self.bn2 = torch_geometric.nn.norm.GraphNorm(hidden_channels)\n",
    "        self.bn3 = torch_geometric.nn.norm.GraphNorm(hidden_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch): \n",
    "        # 1. Obtain node embeddings \n",
    "       # x = self.conv1(x, edge_index) #input layer\n",
    "      #  x = self.bn1(x) #normalize it\n",
    "     #   x = x.relu() #activation\n",
    "#         x = torch.nn.function.elu(x)\n",
    "#        x = self.conv2(x, edge_index)\n",
    "#        x = self.bn2(x)\n",
    "#        x = x.relu() \n",
    "#         print(\"x.relu() = \",x)\n",
    "#        x = self.conv3(x, edge_index)\n",
    "#        x = self.bn3(x)\n",
    "#         # 2. Readout layer\n",
    " #       x = global_mean_pool(x, batch) #what is this for.           # [batch_size, hidden_channels]\n",
    "  #      x = F.dropout(x, p=0.5, training=self.training) #for overfitting\n",
    "   #     x = self.lin3(x)\n",
    "        \n",
    "        x = self.conv1(x, edge_index) #input layer                             \n",
    "                                                      \n",
    "        x = self.bn1(x) #normalize it                                          \n",
    "\n",
    "        x = x.relu() #activation                                               \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = x.relu()\n",
    "#         print(\"x.relu() = \",x)  \n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = self.bn3(x)\n",
    "#         # 2. Readout layer                                                   \n",
    "        x = global_mean_pool(x, batch)\n",
    "        # 3. Apply a final classifier                                          \n",
    "        x = F.dropout(x, p=0.5, training=self.training) #for overfittin        \n",
    "        x = self.lin3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = GCN(dataset.num_node_features,64,2)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') \n",
    "#devicee = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu') \n",
    "print(\"Device = \",device)\n",
    "model = model.to(device)\n",
    "print(\"DEBUGGING: torch.cuda.is_available() = \",torch.cuda.is_available())\n",
    "\n",
    "from torch.utils.data import random_split #TODO: SEE IF YOU CAN USE THIS\n",
    "# torch.manual_seed(12345)\n",
    "# print('DEBUGGING: BEFORE: dataset.y.shape = ',dataset.y.shape)\n",
    "dataset = dataset.shuffle() #shuffle (randmoize placement of it) not sure if this is needed. \n",
    "#print('DEBUGGING: AFTER:  dataset.y.shape = ',dataset.y.shape)\n",
    "\n",
    "#print(len(dataset)) \n",
    "\n",
    "fracs = [0.8, 0.1, 0.1] #percent of dataset used for training testing and validatoin 80%,10%,10% #NOTE: SHOULD CHECK np.sum(fracs) == 1 and len(fracs)==3\n",
    "fracs = [torch.sum(torch.tensor(fracs[:idx])) for idx in range(1,len(fracs)+1)] #get the indexes for training ... parts to use. \n",
    "#print(fracs)\n",
    "split1, split2 = [int(len(dataset)*frac) for frac in fracs[:-1]] \n",
    "train_dataset = dataset[:split1]\n",
    "val_dataset = dataset[split1:split2]\n",
    "test_dataset = dataset[split2:] \n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of validation graphs: {len(val_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}') \n",
    "\n",
    "from torch_geometric.loader import DataLoader \n",
    "#from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)#, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset,  batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False) \n",
    "\n",
    "model = GCN(dataset.num_node_features, dataset.num_classes, 1).to(device) #initiate the model, #2 is the number of outputs here is 2 as pion_z, proton_z \n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= LR) #get the optimizer\n",
    "\n",
    "data_labels = train_dataset.y \n",
    "#weight_signal = counts[1]/counts[0]#DEBUGGING MULTIPLY BY 2 ...\n",
    "#print(\"weight_signal = \",weight_signal) \n",
    "# weight = torch.FloatTensor([weight_signal, 1.0]).to(device) #NOTE: That labels are [sg?,bg?] so label 0 in this case is sg and label 1 is bg.\n",
    "\n",
    "\n",
    "#losss = torch.nn.MSELoss(reduction = 'mean').to(device)\n",
    "#losss = torch.sqrt(losss)\n",
    "\n",
    "#RMSE loss. \n",
    "def RMSELoss(out,y):\n",
    "    return torch.sqrt(torch.mean((out-y)**2))\n",
    "losss = RMSELoss \n",
    "\n",
    "#custom losss\n",
    "def pion_los(out,y):\n",
    "    mse_pi = 0\n",
    "    for j in range(len(out)):\n",
    "            #x_pi = out[j][0]; x_p =out[j][1]                                                                                                                                                                                                 \n",
    "\n",
    "        mse_pi += (out[j][0]-y[j][0].item() )**2\n",
    "    return torch.sqrt(mse_pi/len(out))\n",
    "\n",
    "#losss = pion_los\n",
    "\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train() #initailize the model                                                                                                                                                                                                       \n",
    "    #for i, data in tqdm(enumerate(train_loader)): #perhaps tqdm(enumerate(train_loader)), i is index, data jsut moves through all the dtaa in trainingg                                                                                      \n",
    "    for i,data in enumerate(train_loader):\n",
    "        data = data.to(device) #switch to GPU                                                                                                                                                                                                 \n",
    "        optimizer.zero_grad() #                                                                                                                                                                                                               \n",
    "        out = model(data.x, data.edge_index, data.batch).to(device)  # Perform a single forward pass                                                                                                                                          \n",
    "        yy = []\n",
    "        for j in range(0,len(out)):\n",
    "            yy+= [[data.y[j].item()]]\n",
    "\n",
    "        yy = torch.tensor(yy).to(device)\n",
    "        #print(out)\n",
    "        #print(yy)\n",
    "        loss = losss(out, yy).to(device) #compute the loss  \n",
    "        #print(loss)\n",
    "        loss.backward() #get the gradients.                                                                                                                                                                                                   \n",
    "        optimizer.step() #take a step.                                                                                                                                                                                                        \n",
    "\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "#from sklearn.metrics import roc_auc_score \n",
    "\n",
    "def test(loader):\n",
    "    length = len(loader.dataset)\n",
    "    model.eval() #evaluate teh model.                                                                                                                                                                                                         \n",
    "\n",
    "    #mse_tot = []                                                                                                                                                                                                                             \n",
    "    mse_total = 0\n",
    "    mse_pi = 0\n",
    "    mse_p = 0\n",
    "    #r                                                                                                                                                                                                                                        \n",
    "    #for data in tqdm(loader):  # Iterate in batches over the training/test dataset.                                                                                                                                                          \n",
    "    for data in loader:\n",
    "        data = data.to(device) #put to GPU                                                                                                                                                                                                    \n",
    "        out = model(data.x, data.edge_index, data.batch).to(device) #evalueate                                                                                                                                                                \n",
    "        #this and the for loop is converting data.y to a tensor in the same shape as out rows and 2 columns first is y_pion second is y_proton                                                                                                \n",
    "        yy = []\n",
    "        for j in range(0,len(out)):\n",
    "            yy+= [[data.y[j].item()]]\n",
    "        yy = torch.tensor(yy).to(device) \n",
    "        loss = losss(out, yy).cpu() #getting teh loss function                                                                                                                                                                                \n",
    "        mse_total+=loss.item() #getting the mse (total)                                                                                                                                                                                       \n",
    "        #for j in range(len(out)):\n",
    "            #x_pi = out[j][0]; x_p =out[j][1]                                                                                                                                                                                                 \n",
    "\n",
    "         #   mse_pi += (out[j][0].item()-yy[j][0].item() )**2\n",
    "          #  mse_p +=(out[j][1].item()-yy[j][1].item())**2\n",
    "\n",
    "        #)                                                                                                                                                                                                                                    \n",
    "    return mse_total/length #, np.sqrt(mse_total/length), mse_pi/length, np.sqrt(mse_pi/length), mse_p/length, np.sqrt(mse_p/length) \n",
    "\n",
    "        \n",
    "def print_out():\n",
    "    model.eval() #initailize the model                                                                                                                                                                                                        \n",
    "    #for i, data in tqdm(enumerate(train_loader)): #perhaps tqdm(enumerate(train_loader)), i is index, data jsut moves through all the dtaa in trainingg                                                                                      \n",
    "    outt= []\n",
    "    for i,data in enumerate(test_loader):\n",
    "        data = data.to(device) #switch to GPU                                                                                                                                                                                                 \n",
    "        optimizer.zero_grad() #                                                                                                                                                                                                               \n",
    "        out = model(data.x, data.edge_index, data.batch).to(device)  # Perform a single forward pass                                                                                                                                          \n",
    "        out = out.cpu()\n",
    "        #yy = []                                                                                                                                                                                                                              \n",
    "        #for j in range(0,2*len(out),2):                                                                                                                                                                                                      \n",
    "        #    fuckk = data.y[j]; fuckj = data.y[j+1]                                                                                                                                                                                           \n",
    "        #    yy +=[[fuckk.item(),fuckj.item()]]                                                                                                                                                                                               \n",
    "        #yy = torch.tensor(yy).to(device)                                                                                                                                                                                                     \n",
    "        #loss = losss(out, yy).cpu() #compute the loss                                                                                                                                                                                        \n",
    "        outt+=[[out.detach().numpy()]]\n",
    "    return outt\n",
    "nepochs =  38\n",
    "train_metrics = {'mse':[], \"rmse\":[], 'mse_pi':[], 'rmse_pi':[], 'mse_p':[], 'rmse_p':[] } \n",
    "vall_metrics = {'mse':[], \"rmse\":[], 'mse_pi':[], 'rmse_pi':[], 'mse_p':[], 'rmse_p':[] }\n",
    "\n",
    "for epoch in range(nepochs):  \n",
    "    '''\n",
    "    if epoch ==(nepochs-1):\n",
    "        model.eval()\n",
    "        outt = []\n",
    "        for i, data in enumerate(train_loader):\n",
    "            data = data.to(device) \n",
    "            out = model(data.x, data.edge_index, data.batch).to(device)\n",
    "            out = out.cpu()\n",
    "            outt+=[[out.detach().numpy()]]\n",
    "        #print(outt)\n",
    "        #break'''\n",
    "    #print(\"BEFORE TRAIN()\")                                                                                                                                                                                                                  \n",
    "    train()\n",
    "    #print(\"BEFORE TEST(TRAIN_LOADER)\")                                                                                                                                                                                                       \n",
    "    #train_mse, train_rmse, train_mse_pi, train_rmse_pi, train_mse_p, train_rmse_p = test(train_loader)\n",
    "    train_mse = test(train_loader)\n",
    "\n",
    "    train_metrics['mse'].append(train_mse) \n",
    "    #train_metrics['rmse'].append(train_rmse)\n",
    "    #train_metrics['mse_pi'].append(train_mse_pi)\n",
    "    #train_metrics['rmse_pi'].append(train_rmse_pi)\n",
    "    #train_metrics['mse_p'].append(train_mse_p)\n",
    "    #train_metrics['rmse_p'].append(train_rmse_p)\n",
    "\n",
    "    #print(\"BEFORE TEST(VAL_LOADER)\")                                                                                                                                                                                                         \n",
    "    #vall_mse, vall_rmse, vall_mse_pi, vall_rmse_pi, vall_mse_p, vall_rmse_p = test(val_loader)\n",
    "    vall_mse =test(val_loader) \n",
    "    #if epoch==0 or val_roc_auc >np.max(val_metrics[\"roc_auc\"]) :                                                                                                                                                                             \n",
    "    #    model_best_auc = model                                                                                                                                                                                                               \n",
    "    #    PATH = '/work/clas12/users/mfmce/CLAS12_Lambda_resolution_REU_2023/model_best_auc.pt'                                                                                                                                                \n",
    "    #    torch.save({                                                                                                                                                                                                                         \n",
    "    #        'epoch': epoch,                                                                                                                                                                                                                  \n",
    "    #        'model_state_dict': model.state_dict(),                                                                                                                                                                                          \n",
    "    #        'optimizer_state_dict': optimizer.state_dict(),                                                                                                                                                                                  \n",
    " #             'loss': loss,                                                                                                                                                                                                                  \n",
    "    #        }, PATH)                                                                                                                                                                                                                         \n",
    "\n",
    "    vall_metrics['mse'].append(vall_mse)\n",
    "    #vall_metrics['rmse'].append(vall_rmse)\n",
    "    #vall_metrics['mse_pi'].append(vall_mse)\n",
    "    #vall_metrics['rmse_pi'].append(vall_mse)\n",
    "    #vall_metrics['mse_p'].append(vall_mse)\n",
    "    #all_metrics['rmse_p'].append(vall_mse)\n",
    "    if epoch%9==0:\n",
    "        print(\"Epoch \",epoch,\" Train mse: \",train_mse)\n",
    "        print(\"Epoch \",epoch,\" Validation mse: \",vall_mse)\n",
    "        #print(\"Epoch \",epoch,\" Train mse: \",train_mse,\" Train rmse: \",train_rmse,\" Train mse pion: \",train_mse_pi,\n",
    "         #     \" Train rmse pion: \",train_rmse_pi, \"Train mse proton:\",train_mse_p, \"Train rmse proton:\",train_rmse_p)\n",
    "        #print(\"Epoch \",epoch,\" Validation mse: \",vall_mse,\" Validation rmse: \",vall_rmse,\" Validation mse pion: \",vall_mse_pi,\n",
    "        #      \" Validation rmse pion: \",vall_rmse_pi, \"Validation mse proton:\",vall_mse_p, \"Validation rmse proton:\", vall_rmse_p)\n",
    "    if epoch==(nepochs-1):\n",
    "        a = print_out()\n",
    "        print(\"Epoch \",epoch,\" Train mse: \",train_mse)\n",
    "        print(\"Epoch \",epoch,\" Validation mse: \",vall_mse)\n",
    "        #print(\"Epoch \",epoch,\" Train mse: \",train_mse,\" Train rmse: \",train_rmse,\" Train mse pion: \",train_mse_pi,\n",
    "        #      \" Train rmse pion: \",train_rmse_pi, \"Train mse proton:\",train_mse_p, \"Train rmse proton:\",train_rmse_p)\n",
    "        #print(\"Epoch \",epoch,\" Validation mse: \",vall_mse,\" Validation rmse: \",vall_rmse,\" Validation mse pion: \",vall_mse_pi,\n",
    "        #      \" Validation rmse pion: \",vall_rmse_pi, \"Validation mse proton:\",vall_mse_p, \"Validation rmse proton:\", vall_rmse_p) \n",
    "\n",
    "        #print(a)\n",
    "        #b = print_outb()\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "epochs = [i for i in range(len(train_metrics[\"mse\"]))]  \n",
    "plt.figure()\n",
    "plt.title('Training epoch vs. MSE') \n",
    "plt.plot(epochs, train_metrics['mse']) \n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('MSE')\n",
    "print() \n",
    "print()\n",
    "\n",
    "plt.title('Validation epoch vs. MSE')   \n",
    "plt.plot(epochs, vall_metrics['mse']) \n",
    "plt.xlabel('epoch') \n",
    "plt.ylabel('MSE')  \n",
    "plt.savefig(\"output111pp.pdf\") ####################################################################\n",
    "pi_x = []; p_x = [] \n",
    "#pi_val = []\n",
    "for i in range(len(a)):\n",
    "    for j in range(len(a[i][0])): \n",
    "        pi_x.append(a[i][0][j][0].item())\n",
    "        \n",
    "        #p_x.append(a[i][0][j][1].item()) \n",
    "\n",
    "pi_yy = [] \n",
    "#p_yy = []\n",
    "\n",
    "pi_rec_y = []\n",
    "#p_rec_y = []\n",
    "for i, data in enumerate(test_loader):\n",
    "    data = data#.to(device) #put to GPU\n",
    "    #break\n",
    "    for j in range(0,int(len(data.y))): \n",
    "        pi_yy.append(data.y[j].item())\n",
    " #       p_yy.append(data.y[j+1].item())\n",
    "\n",
    "        #pi_rec_y.append(data.rec[j].item()) \n",
    "       # p_rec_y.append(data.rec[j+1].item())\n",
    "    for j in range(len(data.rec)):\n",
    "        #pi_rec_y.append(data.rec[j][0][0])\n",
    "        pi_rec_y.append(data.rec[j][0])\n",
    "       # p_rec_y.append(data.rec[j][0][1])\n",
    "\n",
    "\n",
    "#plott_pi = np.zeros((len(pi_x),4))\n",
    "#plott_p = np.zeros((len(p_x),2)) \n",
    "plott_pi = np.zeros((len(pi_x),3))\n",
    "for i in range(len(pi_x)):\n",
    "#for i in range(982):\n",
    "    plott_pi[i][0] = pi_yy[i]\n",
    "    plott_pi[i][1] = pi_x[i]\n",
    "    #plott_pi[i][3] = pi_val[i]\n",
    "   # plott_pi[i][2] = pi_rec_y[i] \n",
    "   # plott_p[i][0] = p_yy[i]\n",
    "   # plott_p[i][1] = p_x[i]\n",
    "    #plott_p[i][2]  = p_rec_y[i] \n",
    "for i in range(len(pi_rec_y)): \n",
    "    plott_pi[i][2] = pi_rec_y[i] \n",
    "#plott_pi \n",
    "plt.title('Distribution of Pion phi momentum') \n",
    "plt.axvline(x = np.mean(pi_x), color = 'blue')\n",
    "plt.axvline(x = np.mean(pi_yy), color = 'orange') \n",
    "plt.hist(x = plott_pi, histtype ='step', color = ['orange', 'blue', 'green'], bins = 50)\n",
    "plt.legend(['REC', 'True', 'Predicted']) \n",
    "plt.xlim((-4,4))  \n",
    "plt.ylabel('count')\n",
    "plt.xlabel('Pion momentum ')\n",
    "#plt.savefig(\"/hpc/volatile/group/vossenlab/dir_name_2/\")\n",
    "plt.savefig(\"output222pp.pdf\") #######################################################################\n",
    "from scipy.stats import kstest  \n",
    "print('kstate for predicted')\n",
    "print(kstest(pi_x, pi_yy)) \n",
    "print()\n",
    "print('kstate for rec')\n",
    "print(kstest(pi_rec_y, pi_yy))  \n",
    "print()\n",
    "diff_x = [] \n",
    "diff_rec = []\n",
    "for i in range(len(pi_x)):\n",
    "    diff_x.append(np.sqrt((pi_x[i]-pi_yy[i])**2))\n",
    "    diff_rec.append(np.sqrt((pi_rec_y[i]-pi_yy[i])**2))\n",
    "print('rmse for predicted')\n",
    "print(np.mean(diff_x)) \n",
    "print('rmse for rec')\n",
    "print(np.mean(diff_rec)) \n",
    "print()\n",
    "\n",
    "dif_x = [] \n",
    "dif_rec = []\n",
    "for i in range(len(pi_x)):\n",
    "    dif_x.append((pi_x[i]-pi_yy[i]))\n",
    "    dif_rec.append((pi_rec_y[i]-pi_yy[i]))\n",
    "\n",
    "plottt = np.zeros((len(dif_x), 2))\n",
    "for i in range(len(dif_x)):\n",
    "    plottt[i][0] = dif_x[i]\n",
    "    plottt[i][1] = dif_rec[i]\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Event by event error')\n",
    "n, bins = plt.hist(x = plottt, histtype = 'step', color = ['blue', 'orange'], bins = 50)[-1]\n",
    "plt.legend(['REC', 'Predicted'])\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('error')\n",
    "\n",
    "plt.xlim((-5, 5)) \n",
    "plt.savefig(\"output333pp.pdf\") #########################################################################\n",
    "\n",
    "from statistics import NormalDist\n",
    "norm = NormalDist.from_samples(dif_x)\n",
    "print('predicted mean')\n",
    "print(norm.mean)\n",
    "print()\n",
    "print('predicted stdev')\n",
    "print(norm.stdev) \n",
    "print()\n",
    "norm = NormalDist.from_samples(dif_rec) \n",
    "print('rec mean')\n",
    "print(norm.mean)\n",
    "print()\n",
    "print('rec stdev')\n",
    "print(norm.stdev) \n",
    "\n",
    "print()\n",
    "bb= plt.hist(x = plottt, histtype = 'step', color = ['blue', 'orange'], bins = 50) \n",
    "pred_values= np.zeros(len(bb[0][0]))\n",
    "xx = np.zeros(len(bb[1]))\n",
    "for i in range(len(bb[0][0])):\n",
    "    pred_values[i] = bb[0][0][i]\n",
    "    xx[i] = bb[1][i]\n",
    "\n",
    "from scipy.optimize import curve_fit \n",
    "def gauss(x,  A, x0, sigma): \n",
    "    return A * np.exp(-(x - x0) ** 2 / (2 * sigma ** 2))\n",
    "\n",
    "xx = np.delete(xx, 0)\n",
    "parameters, covariance = curve_fit(gauss, xx, pred_values )\n",
    "\n",
    "print(parameters) \n",
    "plt.figure()\n",
    "\n",
    "plt.title('Event by event error')\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('error')\n",
    "plt.hist(x = plottt, histtype = 'step', color = ['blue', 'orange'], bins = 50)\n",
    "\n",
    "def gauss(x ): \n",
    "    A = parameters[0]; x0 =parameters[1]; sigma = parameters[2]\n",
    "    return A * np.exp(-(x - x0) ** 2 / (2 * sigma ** 2))\n",
    "xx = np.delete(xx, -1) \n",
    "plt.plot( xx, gauss(xx), color = 'purple')  \n",
    "\n",
    "plt.xlim((-5, 5))   \n",
    "plt.ylabel('count')\n",
    "plt.xlabel('error')\n",
    "plt.savefig(\"output444pp.pdf\") #########################################################################\n",
    "\"\"\"\n",
    "f = open(\"/hpc/group/vossenlab/kam264/dir_name_2/batch_filer.py\", \"w\") \n",
    "f.write(code)  \n",
    "f.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf3e67cd-673d-4a5b-bb5a-ca6a08b92d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = '''import pandas as pd\n",
    "import os \n",
    "import numpy as np\n",
    "import numpy.ma as ma \n",
    "import awkward as ak\n",
    "from tqdm import tqdm \n",
    "import torch \n",
    "import torch_geometric as tg \n",
    "import torch_geometric \n",
    "from torch_geometric.data import Data \n",
    "#import torch\n",
    "from torch_geometric.data import InMemoryDataset, download_url  \n",
    "import torch_geometric.transforms as T \n",
    "\n",
    "#NOTE: NEW 2/20/23     \n",
    "from typing import List, Union  \n",
    "\n",
    "from torch_geometric.data import Data, HeteroData  \n",
    "from torch_geometric.data.datapipes import functional_transform\n",
    "from torch_geometric.transforms import BaseTransform   \n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#root = '/hpc/group/vossenlab/kam264/mom_magnitude_pi_x_cx_recmomfull_20000' not horrible, ksate around .2\n",
    "#root = '/hpc/group/vossenlab/kam264/mom_magnitude_pi_x_cx_recfull_vertmom_20000' #much better, bad peak and dip though, so if can fix that, p value\n",
    "#.09, KstestResult(statistic=0.09777468706536856, pvalue=2.779826736292836e-60)\n",
    "#root = '/hpc/group/vossenlab/kam264/mom_magnitude_pi_onlymomentum_rec_20000' shit\n",
    "#root = '/hpc/group/vossenlab/kam264/mom_magnitude_pi_mommnetumvertx_rec_20000'\n",
    "#root = '/hpc/group/vossenlab/kam264/mom_magnitude_pi_all_rec_20000'#not bad, need to do rec stuff though \n",
    "#root = '/hpc/group/vossenlab/kam264/mom_magnitude_pi_all_protmom_20000' #same as above, but got it printed. but rec is jsut z part. \n",
    "#root = '/hpc/group/vossenlab/kam264/stupid_idea' #same as above, but got it printed. but rec is jsut z part. \n",
    "#root = '/hpc/group/vossenlab/kam264/pi_phi_polar'#, mse seems to be better\n",
    "#root = '/hpc/group/vossenlab/kam264/pi_theta_polar' #goodd\n",
    "#root = '/hpc/group/vossenlab/kam264/pi_r_polar' #see slides, week 6 update \n",
    "#root = '/hpc/group/vossenlab/kam264/everythingppi_pionvvv' #not what i ment, actully decent at pion zmomnetum.\n",
    "#root = '/hpc/group/vossenlab/kam264/everythingpi_randomgaus' #bad\n",
    "#root = '/hpc/group/vossenlab/kam264/everythingpi_rrrrandomgausss'\n",
    "root = '/hpc/group/vossenlab/kam264/testt_20000' \n",
    "#root = '/hpc/group/vossenlab/kam264/testmrone_20000' \n",
    "#root = '/hpc/group/vossenlab/kam264/testrkin_20000' #really bad\n",
    "#root = '/hpc/group/vossenlab/kam264/test_momcut_20000' \n",
    "#root = '/hpc/group/vossenlab/kam264/test_100'\n",
    "#root = '/hpc/group/vossenlab/kam264/ttest_wmom_15000_e' \n",
    "#root = '/hpc/group/vossenlab/kam264/L_imass8'\n",
    "\n",
    "\n",
    "class MyOwnDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.load(self.processed_paths[0])\n",
    "        # For PyG<2.4:\n",
    "        # self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['some_file_1', 'some_file_2']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "    def process(self):\n",
    "        # Read data into huge `Data` list.\n",
    "        data_list = None\n",
    "\n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    "\n",
    "        self.save(data_list, self.processed_paths[0])\n",
    "        # For PyG<2.4:\n",
    "        # torch.save(self.collate(data_list), self.processed_paths[0])\n",
    "\n",
    "# Create PyG Dataset\n",
    "#root = '/hpc/group/vossenlab/kam264/pyg_test_rec_traj_dataset_5_28_24/' # 3_14_24 #OLD\n",
    "dataset = MyOwnDataset(\n",
    "            root,\n",
    "            transform=None, #T.Compose([T.ToUndirected(),T.KNNGraph(k=6)]),\n",
    "            pre_transform=None,\n",
    "            pre_filter=None\n",
    "        )\n",
    "dataset \n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn.norm import GraphNorm, BatchNorm \n",
    "\n",
    "batch_size = 64\n",
    "LR =1e-3\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GCN, self).__init__()\n",
    "#         torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)#.jittable() #NOTE: NEEDED FOR DEPLOYMENT IN CMAKE\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)#.jittable()\n",
    "        #self.block2 = nn.DataParallel(self.block2)\n",
    "        #self.conv2 = torch.nn.DataParallel(self.conv2) #this was trying the parallization thing. \n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)#.jittable()\n",
    "        #self.conv3 = torch.nn.DataParallel(self.conv3)\n",
    "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin3 = Linear(hidden_channels, out_channels)\n",
    "        self.bn1 = torch_geometric.nn.norm.GraphNorm(hidden_channels)\n",
    "        self.bn2 = torch_geometric.nn.norm.GraphNorm(hidden_channels)\n",
    "        self.bn3 = torch_geometric.nn.norm.GraphNorm(hidden_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch): \n",
    "        # 1. Obtain node embeddings \n",
    "       # x = self.conv1(x, edge_index) #input layer\n",
    "      #  x = self.bn1(x) #normalize it\n",
    "     #   x = x.relu() #activation\n",
    "#         x = torch.nn.function.elu(x)\n",
    "#        x = self.conv2(x, edge_index)\n",
    "#        x = self.bn2(x)\n",
    "#        x = x.relu() \n",
    "#         print(\"x.relu() = \",x)\n",
    "#        x = self.conv3(x, edge_index)\n",
    "#        x = self.bn3(x)\n",
    "#         # 2. Readout layer\n",
    " #       x = global_mean_pool(x, batch) #what is this for.           # [batch_size, hidden_channels]\n",
    "  #      x = F.dropout(x, p=0.5, training=self.training) #for overfitting\n",
    "   #     x = self.lin3(x)\n",
    "        \n",
    "        x = self.conv1(x, edge_index) #input layer                             \n",
    "                                                      \n",
    "        x = self.bn1(x) #normalize it                                          \n",
    "\n",
    "        x = x.relu() #activation                                               \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = x.relu()\n",
    "#         print(\"x.relu() = \",x)  \n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = self.bn3(x)\n",
    "#         # 2. Readout layer                                                   \n",
    "        x = global_mean_pool(x, batch)\n",
    "        # 3. Apply a final classifier                                          \n",
    "        x = F.dropout(x, p=0.5, training=self.training) #for overfittin        \n",
    "        x = self.lin3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = GCN(dataset.num_node_features,64,2)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') \n",
    "#devicee = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu') \n",
    "print(\"Device = \",device)\n",
    "model = model.to(device)\n",
    "print(\"DEBUGGING: torch.cuda.is_available() = \",torch.cuda.is_available())\n",
    "\n",
    "from torch.utils.data import random_split #TODO: SEE IF YOU CAN USE THIS\n",
    "# torch.manual_seed(12345)\n",
    "# print('DEBUGGING: BEFORE: dataset.y.shape = ',dataset.y.shape)\n",
    "dataset = dataset.shuffle() #shuffle (randmoize placement of it) not sure if this is needed. \n",
    "#print('DEBUGGING: AFTER:  dataset.y.shape = ',dataset.y.shape)\n",
    "\n",
    "#print(len(dataset)) \n",
    "\n",
    "fracs = [0.8, 0.1, 0.1] #percent of dataset used for training testing and validatoin 80%,10%,10% #NOTE: SHOULD CHECK np.sum(fracs) == 1 and len(fracs)==3\n",
    "fracs = [torch.sum(torch.tensor(fracs[:idx])) for idx in range(1,len(fracs)+1)] #get the indexes for training ... parts to use. \n",
    "#print(fracs)\n",
    "split1, split2 = [int(len(dataset)*frac) for frac in fracs[:-1]] \n",
    "train_dataset = dataset[:split1]\n",
    "val_dataset = dataset[split1:split2]\n",
    "test_dataset = dataset[split2:] \n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of validation graphs: {len(val_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}') \n",
    "\n",
    "from torch_geometric.loader import DataLoader \n",
    "#from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)#, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset,  batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False) \n",
    "\n",
    "model = GCN(dataset.num_node_features, dataset.num_classes, 1).to(device) #initiate the model, #2 is the number of outputs here is 2 as pion_z, proton_z \n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= LR) #get the optimizer\n",
    "\n",
    "data_labels = train_dataset.y \n",
    "#weight_signal = counts[1]/counts[0]#DEBUGGING MULTIPLY BY 2 ...\n",
    "#print(\"weight_signal = \",weight_signal) \n",
    "# weight = torch.FloatTensor([weight_signal, 1.0]).to(device) #NOTE: That labels are [sg?,bg?] so label 0 in this case is sg and label 1 is bg.\n",
    "\n",
    "\n",
    "#losss = torch.nn.MSELoss(reduction = 'mean').to(device)\n",
    "#losss = torch.sqrt(losss)\n",
    "\n",
    "#RMSE loss. \n",
    "def RMSELoss(out,y):\n",
    "    return torch.sqrt(torch.mean((out-y)**2))\n",
    "losss = RMSELoss \n",
    "\n",
    "#custom losss\n",
    "def pion_los(out,y):\n",
    "    mse_pi = 0\n",
    "    for j in range(len(out)):\n",
    "            #x_pi = out[j][0]; x_p =out[j][1]                                                                                                                                                                                                 \n",
    "\n",
    "        mse_pi += (out[j][0]-y[j][0].item() )**2\n",
    "    return torch.sqrt(mse_pi/len(out))\n",
    "\n",
    "#losss = pion_los\n",
    "\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train() #initailize the model                                                                                                                                                                                                       \n",
    "    #for i, data in tqdm(enumerate(train_loader)): #perhaps tqdm(enumerate(train_loader)), i is index, data jsut moves through all the dtaa in trainingg                                                                                      \n",
    "    for i,data in enumerate(train_loader):\n",
    "        data = data.to(device) #switch to GPU                                                                                                                                                                                                 \n",
    "        optimizer.zero_grad() #                                                                                                                                                                                                               \n",
    "        out = model(data.x, data.edge_index, data.batch).to(device)  # Perform a single forward pass                                                                                                                                          \n",
    "        yy = []\n",
    "        for j in range(0,len(out)):\n",
    "            yy+= [[data.y[j].item()]]\n",
    "\n",
    "        yy = torch.tensor(yy).to(device)\n",
    "        #print(out)\n",
    "        #print(yy)\n",
    "        loss = losss(out, yy).to(device) #compute the loss  \n",
    "        #print(loss)\n",
    "        loss.backward() #get the gradients.                                                                                                                                                                                                   \n",
    "        optimizer.step() #take a step.                                                                                                                                                                                                        \n",
    "\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "#from sklearn.metrics import roc_auc_score \n",
    "\n",
    "def test(loader):\n",
    "    length = len(loader.dataset)\n",
    "    model.eval() #evaluate teh model.                                                                                                                                                                                                         \n",
    "\n",
    "    #mse_tot = []                                                                                                                                                                                                                             \n",
    "    mse_total = 0\n",
    "    mse_pi = 0\n",
    "    mse_p = 0\n",
    "    #r                                                                                                                                                                                                                                        \n",
    "    #for data in tqdm(loader):  # Iterate in batches over the training/test dataset.                                                                                                                                                          \n",
    "    for data in loader:\n",
    "        data = data.to(device) #put to GPU                                                                                                                                                                                                    \n",
    "        out = model(data.x, data.edge_index, data.batch).to(device) #evalueate                                                                                                                                                                \n",
    "        #this and the for loop is converting data.y to a tensor in the same shape as out rows and 2 columns first is y_pion second is y_proton                                                                                                \n",
    "        yy = []\n",
    "        for j in range(0,len(out)):\n",
    "            yy+= [[data.y[j].item()]]\n",
    "        yy = torch.tensor(yy).to(device) \n",
    "        loss = losss(out, yy).cpu() #getting teh loss function                                                                                                                                                                                \n",
    "        mse_total+=loss.item() #getting the mse (total)                                                                                                                                                                                       \n",
    "        #for j in range(len(out)):\n",
    "            #x_pi = out[j][0]; x_p =out[j][1]                                                                                                                                                                                                 \n",
    "\n",
    "         #   mse_pi += (out[j][0].item()-yy[j][0].item() )**2\n",
    "          #  mse_p +=(out[j][1].item()-yy[j][1].item())**2\n",
    "\n",
    "        #)                                                                                                                                                                                                                                    \n",
    "    return mse_total/length #, np.sqrt(mse_total/length), mse_pi/length, np.sqrt(mse_pi/length), mse_p/length, np.sqrt(mse_p/length) \n",
    "\n",
    "        \n",
    "def print_out():\n",
    "    model.eval() #initailize the model                                                                                                                                                                                                        \n",
    "    #for i, data in tqdm(enumerate(train_loader)): #perhaps tqdm(enumerate(train_loader)), i is index, data jsut moves through all the dtaa in trainingg                                                                                      \n",
    "    outt= []\n",
    "    for i,data in enumerate(test_loader):\n",
    "        data = data.to(device) #switch to GPU                                                                                                                                                                                                 \n",
    "        optimizer.zero_grad() #                                                                                                                                                                                                               \n",
    "        out = model(data.x, data.edge_index, data.batch).to(device)  # Perform a single forward pass                                                                                                                                          \n",
    "        out = out.cpu()\n",
    "        #yy = []                                                                                                                                                                                                                              \n",
    "        #for j in range(0,2*len(out),2):                                                                                                                                                                                                      \n",
    "        #    fuckk = data.y[j]; fuckj = data.y[j+1]                                                                                                                                                                                           \n",
    "        #    yy +=[[fuckk.item(),fuckj.item()]]                                                                                                                                                                                               \n",
    "        #yy = torch.tensor(yy).to(device)                                                                                                                                                                                                     \n",
    "        #loss = losss(out, yy).cpu() #compute the loss    \n",
    "        #neww = []\n",
    "        #for j in range(len(data.elc)):\n",
    "         #   neww.append(data.elc[j][0])\n",
    "        #outt+=[[out.detach().numpy()-neww]]\n",
    "        outt +=[[out.detach().numpy()]]\n",
    "    return outt\n",
    "\n",
    "def print_outb():\n",
    "    model.eval() #initailize the model  \n",
    "    root = '/hpc/group/vossenlab/kam264/tttest_wmom_eL' \n",
    "    dataset = MyOwnDataset(\n",
    "            root,\n",
    "            transform=None, #T.Compose([T.ToUndirected(),T.KNNGraph(k=6)]),\n",
    "            pre_transform=None,\n",
    "            pre_filter=None\n",
    "        )\n",
    "    #dataset\n",
    "    dataset = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    #for i, data in tqdm(enumerate(train_loader)): #perhaps tqdm(enumerate(train_loader)), i is index, data jsut moves through all the dtaa in trainingg                                                                                      \n",
    "    outt= []\n",
    "    for i,data in enumerate(dataset):\n",
    "        data = data.to(device) #switch to GPU                                                                                                                                                                                                 \n",
    "        optimizer.zero_grad() #                                                                                                                                                                                                               \n",
    "        out = model(data.x, data.edge_index, data.batch).to(device)  # Perform a single forward pass                                                                                                                                          \n",
    "        out = out.cpu()\n",
    "        #yy = []                                                                                                                                                                                                                              \n",
    "        #for j in range(0,2*len(out),2):                                                                                                                                                                                                      \n",
    "        #    fuckk = data.y[j]; fuckj = data.y[j+1]                                                                                                                                                                                           \n",
    "        #    yy +=[[fuckk.item(),fuckj.item()]]                                                                                                                                                                                               \n",
    "        #yy = torch.tensor(yy).to(device)                                                                                                                                                                                                     \n",
    "        #loss = losss(out, yy).cpu() #compute the loss                                                                                                                                                                                        \n",
    "        outt+=[[out.detach().numpy()]]\n",
    "    return outt\n",
    "nepochs =  4\n",
    "train_metrics = {'mse':[], \"rmse\":[], 'mse_pi':[], 'rmse_pi':[], 'mse_p':[], 'rmse_p':[] } \n",
    "vall_metrics = {'mse':[], \"rmse\":[], 'mse_pi':[], 'rmse_pi':[], 'mse_p':[], 'rmse_p':[] }\n",
    "\n",
    "for epoch in range(nepochs):  \n",
    "    #print(\"BEFORE TRAIN()\")                                                                                                                                                                                                                  \n",
    "    train()\n",
    "    #print(\"BEFORE TEST(TRAIN_LOADER)\")                                                                                                                                                                                                       \n",
    "    #train_mse, train_rmse, train_mse_pi, train_rmse_pi, train_mse_p, train_rmse_p = test(train_loader)\n",
    "    train_mse = test(train_loader)\n",
    "\n",
    "    train_metrics['mse'].append(train_mse) \n",
    "    #train_metrics['rmse'].append(train_rmse)\n",
    "    #train_metrics['mse_pi'].append(train_mse_pi)\n",
    "    #train_metrics['rmse_pi'].append(train_rmse_pi)\n",
    "    #train_metrics['mse_p'].append(train_mse_p)\n",
    "    #train_metrics['rmse_p'].append(train_rmse_p)\n",
    "\n",
    "    #print(\"BEFORE TEST(VAL_LOADER)\")                                                                                                                                                                                                         \n",
    "    #vall_mse, vall_rmse, vall_mse_pi, vall_rmse_pi, vall_mse_p, vall_rmse_p = test(val_loader)\n",
    "    vall_mse =test(val_loader) \n",
    "    #if epoch==0 or val_roc_auc >np.max(val_metrics[\"roc_auc\"]) :                                                                                                                                                                             \n",
    "    #    model_best_auc = model                                                                                                                                                                                                               \n",
    "    #    PATH = '/work/clas12/users/mfmce/CLAS12_Lambda_resolution_REU_2023/model_best_auc.pt'                                                                                                                                                \n",
    "    #    torch.save({                                                                                                                                                                                                                         \n",
    "    #        'epoch': epoch,                                                                                                                                                                                                                  \n",
    "    #        'model_state_dict': model.state_dict(),                                                                                                                                                                                          \n",
    "    #        'optimizer_state_dict': optimizer.state_dict(),                                                                                                                                                                                  \n",
    " #             'loss': loss,                                                                                                                                                                                                                  \n",
    "    #        }, PATH)                                                                                                                                                                                                                         \n",
    "\n",
    "    vall_metrics['mse'].append(vall_mse)\n",
    "    #vall_metrics['rmse'].append(vall_rmse)\n",
    "    #vall_metrics['mse_pi'].append(vall_mse)\n",
    "    #vall_metrics['rmse_pi'].append(vall_mse)\n",
    "    #vall_metrics['mse_p'].append(vall_mse)\n",
    "    #all_metrics['rmse_p'].append(vall_mse)\n",
    "    if epoch%9==0:\n",
    "        print(\"Epoch \",epoch,\" Train mse: \",train_mse)\n",
    "        print(\"Epoch \",epoch,\" Validation mse: \",vall_mse)\n",
    "        #print(\"Epoch \",epoch,\" Train mse: \",train_mse,\" Train rmse: \",train_rmse,\" Train mse pion: \",train_mse_pi,\n",
    "         #     \" Train rmse pion: \",train_rmse_pi, \"Train mse proton:\",train_mse_p, \"Train rmse proton:\",train_rmse_p)\n",
    "        #print(\"Epoch \",epoch,\" Validation mse: \",vall_mse,\" Validation rmse: \",vall_rmse,\" Validation mse pion: \",vall_mse_pi,\n",
    "        #      \" Validation rmse pion: \",vall_rmse_pi, \"Validation mse proton:\",vall_mse_p, \"Validation rmse proton:\", vall_rmse_p)\n",
    "    if epoch==(nepochs-1):\n",
    "        a = print_out()\n",
    "        print(\"Epoch \",epoch,\" Train mse: \",train_mse) \n",
    "        print(\"Epoch \",epoch,\" Validation mse: \",vall_mse)\n",
    "        #print(\"Epoch \",epoch,\" Train mse: \",train_mse,\" Train rmse: \",train_rmse,\" Train mse pion: \",train_mse_pi,\n",
    "        #      \" Train rmse pion: \",train_rmse_pi, \"Train mse proton:\",train_mse_p, \"Train rmse proton:\",train_rmse_p)\n",
    "        #print(\"Epoch \",epoch,\" Validation mse: \",vall_mse,\" Validation rmse: \",vall_rmse,\" Validation mse pion: \",vall_mse_pi,\n",
    "        #      \" Validation rmse pion: \",vall_rmse_pi, \"Validation mse proton:\",vall_mse_p, \"Validation rmse proton:\", vall_rmse_p) \n",
    "\n",
    "        #print(a)\n",
    "        #b = print_outb()\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "epochs = [i for i in range(len(train_metrics[\"mse\"]))]  \n",
    "plt.figure()\n",
    "plt.title('Training epoch vs. MSE') \n",
    "plt.plot(epochs, train_metrics['mse']) \n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('MSE')\n",
    "\n",
    "\n",
    "plt.title('Validation epoch vs. MSE')   \n",
    "plt.plot(epochs, vall_metrics['mse']) \n",
    "plt.xlabel('epoch') \n",
    "plt.ylabel('MSE')  \n",
    "plt.savefig(\"outputtttt11.pdf\") #########################################################################\n",
    "pi_x = []; p_x = [] \n",
    "#pi_val = []\n",
    "for i in range(len(a)):\n",
    "    for j in range(len(a[i][0])): \n",
    "        pi_x.append(a[i][0][j][0].item()) \n",
    "        \n",
    "        #p_x.append(a[i][0][j][1].item()) \n",
    "\n",
    "pi_yy = [] \n",
    "#p_yy = []\n",
    "\n",
    "pi_rec_y = []\n",
    "#p_rec_y = []\n",
    "for i, data in enumerate(test_loader):\n",
    "    data = data#.to(device) #put to GPU\n",
    "    #break \n",
    "    jj= 0\n",
    "    for j in range(0,int(len(data.y))): \n",
    "        #pi_yy.append(data.y[j].item() -data.elc[j][0] )\n",
    "        pi_yy.append(data.y[j].item())\n",
    "     \n",
    "    for j in range(len(data.rec)):\n",
    "        #pi_rec_y.append(data.rec[j][0]-data.elc[j][0])\n",
    "        pi_rec_y.append(data.rec[j][0][1])\n",
    "\n",
    "#plott_pi = np.zeros((len(pi_x),4))\n",
    "#plott_p = np.zeros((len(p_x),2)) \n",
    "plott_pi = np.zeros((len(pi_x),3))\n",
    "for i in range(len(pi_x)):\n",
    "#for i in range(982):\n",
    "    plott_pi[i][0] = pi_yy[i]\n",
    "    plott_pi[i][1] = pi_x[i]\n",
    "    #plott_pi[i][3] = pi_val[i]\n",
    "   # plott_pi[i][2] = pi_rec_y[i] \n",
    "   # plott_p[i][0] = p_yy[i]\n",
    "   # plott_p[i][1] = p_x[i]\n",
    "    #plott_p[i][2]  = p_rec_y[i] \n",
    "for i in range(len(pi_rec_y)): \n",
    "    plott_pi[i][2] = pi_rec_y[i] \n",
    "#plott_pi \n",
    "plt.figure\n",
    "plt.title('Location of Pion vertex', fontsize =20) \n",
    "#plt.axvline(x = np.mean(pi_x), color = 'blue')\n",
    "#plt.axvline(x = np.mean(pi_yy), color = 'orange') \n",
    "plt.hist(x = plott_pi, histtype ='step', color = ['orange', 'blue', 'green'], bins = 500 )\n",
    "#plt.hist(x = plott_pi, histtype ='step', bins = 500 )\n",
    "#plt.legend(['Predicted', 'True', 'REC'])  \n",
    "#plt.legend(['True', 'Predicted', 'REC']) \n",
    "plt.yticks(fontsize=10)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.xlim((-8,2)) \n",
    "plt.ylabel('count', fontsize = 15)\n",
    "plt.xlabel('Location of Pion vertex (cm)', fontsize = 15) \n",
    "plt.savefig(\"outputtttt22.pdf\") #########################################################################\n",
    "#plt.savefig(\"/hpc/volatile/group/vossenlab/dir_name_2/\")\n",
    "from scipy.stats import kstest   \n",
    "print('kstuff')\n",
    "print(kstest(pi_x, pi_yy)) \n",
    "print(kstest(pi_rec_y, pi_yy))   \n",
    "diff_x = [] \n",
    "diff_rec = []\n",
    "for i in range(len(pi_x)):\n",
    "    diff_x.append(np.sqrt((pi_x[i]-pi_yy[i])**2))\n",
    "    diff_rec.append(np.sqrt((pi_rec_y[i]-pi_yy[i])**2))\n",
    "print(np.mean(diff_x)) \n",
    "print(np.mean(diff_rec)) \n",
    "dif_x = [] \n",
    "dif_rec = []\n",
    "for i in range(len(pi_x)):\n",
    "    dif_x.append((pi_x[i]-pi_yy[i]))\n",
    "    dif_rec.append((pi_rec_y[i]-pi_yy[i]))\n",
    "\n",
    "plottt = np.zeros((len(dif_x), 2)) \n",
    "for i in range(len(dif_x)):\n",
    "    plottt[i][0] = dif_x[i]\n",
    "    plottt[i][1] = dif_rec[i]\n",
    "plt.figure\n",
    "plt.title('Event by Event Error', fontsize =20)\n",
    "#n, bins = plt.hist(x = plottt, histtype = 'step', color = ['blue', 'green'], bins = 500)[-1]\n",
    "#plt.legend(['REC', 'Predicted'])\n",
    "plt.hist(x = plottt, histtype = 'step', color = ['blue', 'green'], bins = 500)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.ylabel('Count', fontsize = 15)\n",
    "plt.xlabel('Error', fontsize = 15)\n",
    "\n",
    "plt.xlim((-20, 20)) \n",
    "plt.savefig(\"outputtttt33.pdf\") ######################################################################### \n",
    "from scipy.stats import norm as nm\n",
    "\n",
    "from statistics import NormalDist\n",
    "norm = NormalDist.from_samples(dif_x)\n",
    "print('predcition')\n",
    "print(norm.mean) \n",
    "print(norm.stdev)\n",
    "xxx = np.linspace(-20, 20, 100)\n",
    "from statistics import NormalDist\n",
    "norm = NormalDist.from_samples(dif_rec) \n",
    "print('rec')\n",
    "print(norm.mean) \n",
    "print(norm.stdev)\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "f = open(\"/hpc/group/vossenlab/kam264/dir_name_2/batch_filer.py\", \"w\")  \n",
    "f.write(code)  \n",
    "f.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8de803-cb4b-4530-a606-8a0b4a2f5750",
   "metadata": {},
   "outputs": [],
   "source": [
    "13096751, for normal better graphs. ##################################################################################\n",
    "13248096\n",
    "13248100\n",
    "13248102\n",
    "13250771 \n",
    "13254366 \n",
    "13254457"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0816a0-9edf-4654-8c1d-736fcd39e91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "13067918, this is good one for pion_v-electron vertex.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c40bd9d-883a-454e-8878-68d8ea6568e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = '''\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import numpy.ma as ma \n",
    "import awkward as ak\n",
    "from tqdm import tqdm \n",
    "import torch \n",
    "import torch_geometric as tg  \n",
    "import torch_geometric \n",
    "from torch_geometric.data import Data \n",
    "#import torch\n",
    "from torch_geometric.data import InMemoryDataset, download_url  \n",
    "import torch_geometric.transforms as T \n",
    "\n",
    "#NOTE: NEW 2/20/23      \n",
    "from typing import List, Union  \n",
    "\n",
    "from torch_geometric.data import Data, HeteroData \n",
    "from torch_geometric.data.datapipes import functional_transform\n",
    "from torch_geometric.transforms import BaseTransform     \n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#root = '/hpc/group/vossenlab/kam264/mom_magnitude_pi_x_cx_recmomfull_20000' not horrible, ksate around .2\n",
    "#root = '/hpc/group/vossenlab/kam264/mom_magnitude_pi_x_cx_recfull_vertmom_20000' #much better, bad peak and dip though, so if can fix that, p value\n",
    "#.09, KstestResult(statistic=0.09777468706536856, pvalue=2.779826736292836e-60)\n",
    "#root = '/hpc/group/vossenlab/kam264/mom_magnitude_pi_onlymomentum_rec_20000' shit\n",
    "#root = '/hpc/group/vossenlab/kam264/mom_magnitude_pi_mommnetumvertx_rec_20000'\n",
    "#root = '/hpc/group/vossenlab/kam264/mom_magnitude_pi_all_rec_20000'#not bad, need to do rec stuff though \n",
    "#root = '/hpc/group/vossenlab/kam264/mom_magnitude_pi_all_protmom_20000' #same as above, but got it printed. but rec is jsut z part. \n",
    "#root = '/hpc/group/vossenlab/kam264/stupid_idea' #same as above, but got it printed. but rec is jsut z part. \n",
    "#root = '/hpc/group/vossenlab/kam264/pi_phi_polar'#, mse seems to be better\n",
    "#root = '/hpc/group/vossenlab/kam264/pi_theta_polar' #goodd\n",
    "#root = '/hpc/group/vossenlab/kam264/pi_r_polar' #see slides, week 6 update \n",
    "#root = '/hpc/group/vossenlab/kam264/everythingppi_pionvvv' #not what i ment, actully decent at pion zmomnetum.\n",
    "#root = '/hpc/group/vossenlab/kam264/everythingpi_randomgaus' #bad\n",
    "#root = '/hpc/group/vossenlab/kam264/everythingpi_rrrrandomgausss'\n",
    "#root = '/hpc/group/vossenlab/kam264/testt_20000' \n",
    "#root = '/hpc/group/vossenlab/kam264/testmrone_20000' \n",
    "#root = '/hpc/group/vossenlab/kam264/testrkin_20000' #really bad\n",
    "#root = '/hpc/group/vossenlab/kam264/test_momcut_20000' \n",
    "#root = '/hpc/group/vossenlab/kam264/test_100'\n",
    "#root = '/hpc/group/vossenlab/kam264/ttest_wmom_eL'\n",
    "root = '/hpc/group/vossenlab/kam264/ttest_L_imass'\n",
    "root = '/hpc/group/vossenlab/kam264/test_L_imassss'\n",
    "\n",
    "\n",
    "class MyOwnDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.load(self.processed_paths[0])\n",
    "        # For PyG<2.4:\n",
    "        # self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['some_file_1', 'some_file_2']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "    def process(self):\n",
    "        # Read data into huge `Data` list.\n",
    "        data_list = None\n",
    "\n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    "\n",
    "        self.save(data_list, self.processed_paths[0])\n",
    "        # For PyG<2.4:\n",
    "        # torch.save(self.collate(data_list), self.processed_paths[0])\n",
    "\n",
    "# Create PyG Dataset\n",
    "#root = '/hpc/group/vossenlab/kam264/pyg_test_rec_traj_dataset_5_28_24/' # 3_14_24 #OLD\n",
    "dataset = MyOwnDataset(\n",
    "            root,\n",
    "            transform=None, #T.Compose([T.ToUndirected(),T.KNNGraph(k=6)]),\n",
    "            pre_transform=None,\n",
    "            pre_filter=None\n",
    "        ) \n",
    "dataset\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn.norm import GraphNorm, BatchNorm \n",
    "\n",
    "batch_size = 64\n",
    "LR =1e-3\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GCN, self).__init__()\n",
    "#         torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)#.jittable() #NOTE: NEEDED FOR DEPLOYMENT IN CMAKE\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)#.jittable()\n",
    "        #self.block2 = nn.DataParallel(self.block2)\n",
    "        #self.conv2 = torch.nn.DataParallel(self.conv2) #this was trying the parallization thing. \n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)#.jittable()\n",
    "        #self.conv3 = torch.nn.DataParallel(self.conv3)\n",
    "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin3 = Linear(hidden_channels, out_channels)\n",
    "        self.bn1 = torch_geometric.nn.norm.GraphNorm(hidden_channels)\n",
    "        self.bn2 = torch_geometric.nn.norm.GraphNorm(hidden_channels)\n",
    "        self.bn3 = torch_geometric.nn.norm.GraphNorm(hidden_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch): \n",
    "        # 1. Obtain node embeddings \n",
    "       # x = self.conv1(x, edge_index) #input layer\n",
    "      #  x = self.bn1(x) #normalize it\n",
    "     #   x = x.relu() #activation\n",
    "#         x = torch.nn.function.elu(x)\n",
    "#        x = self.conv2(x, edge_index)\n",
    "#        x = self.bn2(x)\n",
    "#        x = x.relu() \n",
    "#         print(\"x.relu() = \",x)\n",
    "#        x = self.conv3(x, edge_index)\n",
    "#        x = self.bn3(x)\n",
    "#         # 2. Readout layer\n",
    " #       x = global_mean_pool(x, batch) #what is this for.           # [batch_size, hidden_channels]\n",
    "  #      x = F.dropout(x, p=0.5, training=self.training) #for overfitting\n",
    "   #     x = self.lin3(x)\n",
    "        \n",
    "        x = self.conv1(x, edge_index) #input layer                             \n",
    "                                                      \n",
    "        x = self.bn1(x) #normalize it                                          \n",
    "\n",
    "        x = x.relu() #activation                                               \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = x.relu()\n",
    "#         print(\"x.relu() = \",x)  \n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = self.bn3(x)\n",
    "#         # 2. Readout layer                                                   \n",
    "        x = global_mean_pool(x, batch)\n",
    "        # 3. Apply a final classifier                                          \n",
    "        x = F.dropout(x, p=0.5, training=self.training) #for overfittin        \n",
    "        x = self.lin3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = GCN(dataset.num_node_features,64,2)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') \n",
    "#devicee = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu') \n",
    "print(\"Device = \",device)\n",
    "model = model.to(device)\n",
    "print(\"DEBUGGING: torch.cuda.is_available() = \",torch.cuda.is_available())\n",
    "\n",
    "from torch.utils.data import random_split #TODO: SEE IF YOU CAN USE THIS\n",
    "# torch.manual_seed(12345)\n",
    "# print('DEBUGGING: BEFORE: dataset.y.shape = ',dataset.y.shape)\n",
    "dataset = dataset.shuffle() #shuffle (randmoize placement of it) not sure if this is needed. \n",
    "#print('DEBUGGING: AFTER:  dataset.y.shape = ',dataset.y.shape)\n",
    "\n",
    "#print(len(dataset)) \n",
    "\n",
    "fracs = [0.8, 0.1, 0.1] #percent of dataset used for training testing and validatoin 80%,10%,10% #NOTE: SHOULD CHECK np.sum(fracs) == 1 and len(fracs)==3\n",
    "fracs = [torch.sum(torch.tensor(fracs[:idx])) for idx in range(1,len(fracs)+1)] #get the indexes for training ... parts to use. \n",
    "#print(fracs)\n",
    "split1, split2 = [int(len(dataset)*frac) for frac in fracs[:-1]] \n",
    "train_dataset = dataset[:split1]\n",
    "val_dataset = dataset[split1:split2]\n",
    "test_dataset = dataset[split2:] \n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of validation graphs: {len(val_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}') \n",
    "\n",
    "from torch_geometric.loader import DataLoader \n",
    "#from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)#, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset,  batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False) \n",
    "\n",
    "model = GCN(dataset.num_node_features, dataset.num_classes, 1).to(device) #initiate the model, #2 is the number of outputs here is 2 as pion_z, proton_z \n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= LR) #get the optimizer\n",
    "\n",
    "data_labels = train_dataset.y \n",
    "#weight_signal = counts[1]/counts[0]#DEBUGGING MULTIPLY BY 2 ...\n",
    "#print(\"weight_signal = \",weight_signal) \n",
    "# weight = torch.FloatTensor([weight_signal, 1.0]).to(device) #NOTE: That labels are [sg?,bg?] so label 0 in this case is sg and label 1 is bg.\n",
    "\n",
    "\n",
    "#losss = torch.nn.MSELoss(reduction = 'mean').to(device)\n",
    "#losss = torch.sqrt(losss)\n",
    "\n",
    "#RMSE loss. \n",
    "def RMSELoss(out,y):\n",
    "    return torch.sqrt(torch.mean((out-y)**2))\n",
    "losss = RMSELoss \n",
    "\n",
    "#custom losss\n",
    "def pion_los(out,y):\n",
    "    mse_pi = 0\n",
    "    for j in range(len(out)):\n",
    "            #x_pi = out[j][0]; x_p =out[j][1]                                                                                                                                                                                                 \n",
    "\n",
    "        mse_pi += (out[j][0]-y[j][0].item() )**2\n",
    "    return torch.sqrt(mse_pi/len(out))\n",
    "\n",
    "#losss = pion_los\n",
    "\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train() #initailize the model                                                                                                                                                                                                       \n",
    "    #for i, data in tqdm(enumerate(train_loader)): #perhaps tqdm(enumerate(train_loader)), i is index, data jsut moves through all the dtaa in trainingg                                                                                      \n",
    "    for i,data in enumerate(train_loader):\n",
    "        data = data.to(device) #switch to GPU                                                                                                                                                                                                 \n",
    "        optimizer.zero_grad() #                                                                                                                                                                                                               \n",
    "        out = model(data.x, data.edge_index, data.batch).to(device)  # Perform a single forward pass                                                                                                                                          \n",
    "        yy = []\n",
    "        for j in range(0,len(out)):\n",
    "            yy+= [[data.y[j].item()]]\n",
    "\n",
    "        yy = torch.tensor(yy).to(device)\n",
    "        #print(out)\n",
    "        #print(yy)\n",
    "        loss = losss(out, yy).to(device) #compute the loss  \n",
    "        #print(loss)\n",
    "        loss.backward() #get the gradients.                                                                                                                                                                                                   \n",
    "        optimizer.step() #take a step.                                                                                                                                                                                                        \n",
    "\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "#from sklearn.metrics import roc_auc_score \n",
    "\n",
    "def test(loader):\n",
    "    length = len(loader.dataset)\n",
    "    model.eval() #evaluate teh model.                                                                                                                                                                                                         \n",
    "\n",
    "    #mse_tot = []                                                                                                                                                                                                                             \n",
    "    mse_total = 0\n",
    "    mse_pi = 0\n",
    "    mse_p = 0\n",
    "    #r                                                                                                                                                                                                                                        \n",
    "    #for data in tqdm(loader):  # Iterate in batches over the training/test dataset.                                                                                                                                                          \n",
    "    for data in loader:\n",
    "        data = data.to(device) #put to GPU                                                                                                                                                                                                    \n",
    "        out = model(data.x, data.edge_index, data.batch).to(device) #evalueate                                                                                                                                                                \n",
    "        #this and the for loop is converting data.y to a tensor in the same shape as out rows and 2 columns first is y_pion second is y_proton                                                                                                \n",
    "        yy = []\n",
    "        for j in range(0,len(out)):\n",
    "            yy+= [[data.y[j].item()]]\n",
    "        yy = torch.tensor(yy).to(device) \n",
    "        loss = losss(out, yy).cpu() #getting teh loss function                                                                                                                                                                                \n",
    "        mse_total+=loss.item() #getting the mse (total)                                                                                                                                                                                       \n",
    "        #for j in range(len(out)):\n",
    "            #x_pi = out[j][0]; x_p =out[j][1]                                                                                                                                                                                                 \n",
    "\n",
    "         #   mse_pi += (out[j][0].item()-yy[j][0].item() )**2\n",
    "          #  mse_p +=(out[j][1].item()-yy[j][1].item())**2\n",
    "\n",
    "        #)                                                                                                                                                                                                                                    \n",
    "    return mse_total/length #, np.sqrt(mse_total/length), mse_pi/length, np.sqrt(mse_pi/length), mse_p/length, np.sqrt(mse_p/length) \n",
    "\n",
    "        \n",
    "def print_out():\n",
    "    model.eval() #initailize the model                                                                                                                                                                                                        \n",
    "    #for i, data in tqdm(enumerate(train_loader)): #perhaps tqdm(enumerate(train_loader)), i is index, data jsut moves through all the dtaa in trainingg                                                                                      \n",
    "    outt= []\n",
    "    for i,data in enumerate(test_loader):\n",
    "        data = data.to(device) #switch to GPU                                                                                                                                                                                                 \n",
    "        optimizer.zero_grad() #                                                                                                                                                                                                               \n",
    "        out = model(data.x, data.edge_index, data.batch).to(device)  # Perform a single forward pass                                                                                                                                          \n",
    "        out = out.cpu()\n",
    "        #yy = []                                                                                                                                                                                                                              \n",
    "        #for j in range(0,2*len(out),2):                                                                                                                                                                                                      \n",
    "        #    fuckk = data.y[j]; fuckj = data.y[j+1]                                                                                                                                                                                           \n",
    "        #    yy +=[[fuckk.item(),fuckj.item()]]                                                                                                                                                                                               \n",
    "        #yy = torch.tensor(yy).to(device)                                                                                                                                                                                                     \n",
    "        #loss = losss(out, yy).cpu() #compute the loss                                                                                                                                                                                        \n",
    "        outt+=[[out.detach().numpy()]]\n",
    "    return outt\n",
    "\n",
    "def print_outb():\n",
    "    model.eval() #initailize the model                                                                                                                                                                                                        \n",
    "    #for i, data in tqdm(enumerate(train_loader)): #perhaps tqdm(enumerate(train_loader)), i is index, data jsut moves through all the dtaa in trainingg                                                                                      \n",
    "    outt= []\n",
    "    for i,data in enumerate(test_loader):\n",
    "        data = data.to(device) #switch to GPU                                                                                                                                                                                                 \n",
    "        optimizer.zero_grad() #                                                                                                                                                                                                               \n",
    "        out = model(data.x, data.edge_index, data.batch).to(device)  # Perform a single forward pass                                                                                                                                          \n",
    "        out = out.cpu()\n",
    "        #yy = []                                                                                                                                                                                                                              \n",
    "        #for j in range(0,2*len(out),2):                                                                                                                                                                                                      \n",
    "        #    fuckk = data.y[j]; fuckj = data.y[j+1]                                                                                                                                                                                           \n",
    "        #    yy +=[[fuckk.item(),fuckj.item()]]                                                                                                                                                                                               \n",
    "        #yy = torch.tensor(yy).to(device)                                                                                                                                                                                                     \n",
    "        #loss = losss(out, yy).cpu() #compute the loss     \n",
    "        neww = []\n",
    "        for j in enumerate(test_loader):\n",
    "            neww.append(out[j]-data.elc)\n",
    "        outt+=[[new.detach().numpy()]]\n",
    "    return outt\n",
    "nepochs =  58\n",
    "train_metrics = {'mse':[], \"rmse\":[], 'mse_pi':[], 'rmse_pi':[], 'mse_p':[], 'rmse_p':[] } \n",
    "vall_metrics = {'mse':[], \"rmse\":[], 'mse_pi':[], 'rmse_pi':[], 'mse_p':[], 'rmse_p':[] }\n",
    "\n",
    "for epoch in range(nepochs):  \n",
    "\n",
    "    #print(\"BEFORE TRAIN()\")                                                                                                                                                                                                                  \n",
    "    train()\n",
    "    #print(\"BEFORE TEST(TRAIN_LOADER)\")                                                                                                                                                                                                       \n",
    "    #train_mse, train_rmse, train_mse_pi, train_rmse_pi, train_mse_p, train_rmse_p = test(train_loader)\n",
    "    train_mse = test(train_loader)\n",
    "\n",
    "    train_metrics['mse'].append(train_mse) \n",
    "    #train_metrics['rmse'].append(train_rmse)\n",
    "    #train_metrics['mse_pi'].append(train_mse_pi)\n",
    "    #train_metrics['rmse_pi'].append(train_rmse_pi)\n",
    "    #train_metrics['mse_p'].append(train_mse_p)\n",
    "    #train_metrics['rmse_p'].append(train_rmse_p)\n",
    "\n",
    "    #print(\"BEFORE TEST(VAL_LOADER)\")                                                                                                                                                                                                         \n",
    "    #vall_mse, vall_rmse, vall_mse_pi, vall_rmse_pi, vall_mse_p, vall_rmse_p = test(val_loader)\n",
    "    vall_mse =test(val_loader) \n",
    "    #if epoch==0 or val_roc_auc >np.max(val_metrics[\"roc_auc\"]) :                                                                                                                                                                             \n",
    "    #    model_best_auc = model                                                                                                                                                                                                               \n",
    "    #    PATH = '/work/clas12/users/mfmce/CLAS12_Lambda_resolution_REU_2023/model_best_auc.pt'                                                                                                                                                \n",
    "    #    torch.save({                                                                                                                                                                                                                         \n",
    "    #        'epoch': epoch,                                                                                                                                                                                                                  \n",
    "    #        'model_state_dict': model.state_dict(),                                                                                                                                                                                          \n",
    "    #        'optimizer_state_dict': optimizer.state_dict(),                                                                                                                                                                                  \n",
    " #             'loss': loss,                                                                                                                                                                                                                  \n",
    "    #        }, PATH)                                                                                                                                                                                                                         \n",
    "\n",
    "    vall_metrics['mse'].append(vall_mse)\n",
    "    #vall_metrics['rmse'].append(vall_rmse)\n",
    "    #vall_metrics['mse_pi'].append(vall_mse)\n",
    "    #vall_metrics['rmse_pi'].append(vall_mse)\n",
    "    #vall_metrics['mse_p'].append(vall_mse)\n",
    "    #all_metrics['rmse_p'].append(vall_mse)\n",
    "    if epoch%9==0:\n",
    "        print(\"Epoch \",epoch,\" Train mse: \",train_mse)\n",
    "        print(\"Epoch \",epoch,\" Validation mse: \",vall_mse)\n",
    "        #print(\"Epoch \",epoch,\" Train mse: \",train_mse,\" Train rmse: \",train_rmse,\" Train mse pion: \",train_mse_pi,\n",
    "         #     \" Train rmse pion: \",train_rmse_pi, \"Train mse proton:\",train_mse_p, \"Train rmse proton:\",train_rmse_p)\n",
    "        #print(\"Epoch \",epoch,\" Validation mse: \",vall_mse,\" Validation rmse: \",vall_rmse,\" Validation mse pion: \",vall_mse_pi,\n",
    "        #      \" Validation rmse pion: \",vall_rmse_pi, \"Validation mse proton:\",vall_mse_p, \"Validation rmse proton:\", vall_rmse_p)\n",
    "    if epoch==(nepochs-1):\n",
    "        a = print_out()\n",
    "        #b=  print_outb()\n",
    "        print(\"Epoch \",epoch,\" Train mse: \",train_mse)\n",
    "        print(\"Epoch \",epoch,\" Validation mse: \",vall_mse)\n",
    "        #print(\"Epoch \",epoch,\" Train mse: \",train_mse,\" Train rmse: \",train_rmse,\" Train mse pion: \",train_mse_pi,\n",
    "        #      \" Train rmse pion: \",train_rmse_pi, \"Train mse proton:\",train_mse_p, \"Train rmse proton:\",train_rmse_p)\n",
    "        #print(\"Epoch \",epoch,\" Validation mse: \",vall_mse,\" Validation rmse: \",vall_rmse,\" Validation mse pion: \",vall_mse_pi,\n",
    "        #      \" Validation rmse pion: \",vall_rmse_pi, \"Validation mse proton:\",vall_mse_p, \"Validation rmse proton:\", vall_rmse_p) \n",
    "\n",
    "        #print(a)\n",
    "        #b = print_outb()\n",
    "import matplotlib.pyplot as plt \n",
    "plt.figure()\n",
    "epochs = [i for i in range(len(train_metrics[\"mse\"]))]  \n",
    "plt.figure()\n",
    "plt.title('Training epoch vs. MSE') \n",
    "plt.plot(epochs, train_metrics['mse']) \n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('MSE')\n",
    "\n",
    "\n",
    "plt.title('Validation epoch vs. MSE')   \n",
    "plt.plot(epochs, vall_metrics['mse'])  \n",
    "plt.xlabel('epoch') \n",
    "plt.ylabel('MSE')  \n",
    "plt.savefig(\"outputlambda1.pdf\") #########################################################################\n",
    "pi_x = []; p_x = [] \n",
    "#pi_val = []\n",
    "for i in range(len(a)):\n",
    "    for j in range(len(a[i][0])): \n",
    "        pi_x.append(a[i][0][j][0].item())\n",
    "        \n",
    "        #p_x.append(a[i][0][j][1].item()) \n",
    "\n",
    "pi_yy = [] \n",
    "#p_yy = []\n",
    "\n",
    "pi_rec_y = []\n",
    "#p_rec_y = []\n",
    "for i, data in enumerate(test_loader):\n",
    "    data = data#.to(device) #put to GPU\n",
    "    #break\n",
    "    for j in range(0,int(len(data.y))): \n",
    "        pi_yy.append(data.y[j].item())\n",
    " #       p_yy.append(data.y[j+1].item())\n",
    "\n",
    "        #pi_rec_y.append(data.rec[j].item()) \n",
    "       # p_rec_y.append(data.rec[j+1].item())\n",
    "    for j in range(len(data.rec)):\n",
    "        #pi_rec_y.append(data.rec[j][0][0])\n",
    "        pi_rec_y.append(data.rec[j][0])\n",
    "       # p_rec_y.append(data.rec[j][0][1])\n",
    "\n",
    "\n",
    "#plott_pi = np.zeros((len(pi_x),4))\n",
    "#plott_p = np.zeros((len(p_x),2)) \n",
    "plott_pi = np.zeros((len(pi_x),3))\n",
    "for i in range(len(pi_x)):\n",
    "#for i in range(982):\n",
    "    plott_pi[i][0] = pi_yy[i]\n",
    "    plott_pi[i][1] = pi_x[i]\n",
    "    #plott_pi[i][3] = pi_val[i]\n",
    "   # plott_pi[i][2] = pi_rec_y[i] \n",
    "   # plott_p[i][0] = p_yy[i]\n",
    "   # plott_p[i][1] = p_x[i]\n",
    "    #plott_p[i][2]  = p_rec_y[i]  \n",
    "for i in range(len(pi_rec_y)): \n",
    "    plott_pi[i][2] = pi_rec_y[i] \n",
    "#plott_pi \n",
    "plt.figure\n",
    "plt.title('location of Pion vertex') \n",
    "#plt.axvline(x = np.mean(pi_x), color = 'blue')\n",
    "#plt.axvline(x = np.mean(pi_yy), color = 'orange') \n",
    "plt.hist(x = plott_pi, histtype ='step', color = ['orange', 'blue', 'green'], bins = 100 )\n",
    "#plt.hist(x = plott_pi, histtype ='step',  bins = 100 )\n",
    "#plt.legend(['Predicted', 'True', 'REC']) \n",
    "plt.xlim((-20,20)) \n",
    "plt.ylabel('count')\n",
    "plt.xlabel('Location of Pion vertex (cm)')\n",
    "plt.savefig(\"outputlambda22.pdf\") #########################################################################\n",
    "plt.figure()\n",
    "from scipy.stats import kstest   \n",
    "print(kstest(pi_x, pi_yy)) \n",
    "print(kstest(pi_rec_y, pi_yy))   \n",
    "diff_x = []  \n",
    "diff_rec = []\n",
    "for i in range(len(pi_x)):\n",
    "    diff_x.append(np.sqrt((pi_x[i]-pi_yy[i])**2))\n",
    "    diff_rec.append(np.sqrt((pi_rec_y[i]-pi_yy[i])**2))\n",
    "print(np.mean(diff_x)) \n",
    "print(np.mean(diff_rec)) \n",
    "dif_x = [] \n",
    "dif_rec = []\n",
    "for i in range(len(pi_x)):\n",
    "    dif_x.append((pi_x[i]-pi_yy[i]))\n",
    "    dif_rec.append((pi_rec_y[i]-pi_yy[i]))\n",
    "\n",
    "plottt = np.zeros((len(dif_x), 2)) \n",
    "for i in range(len(dif_x)): \n",
    "    plottt[i][0] = dif_x[i]\n",
    "    plottt[i][1] = dif_rec[i]\n",
    "\n",
    "plt.title('Event by event error')\n",
    "n, bins = plt.hist(x = plottt, histtype = 'step', color = ['blue', 'orange'], bins = 100)[-1]\n",
    "plt.legend(['REC', 'Predicted'])\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('error')\n",
    "\n",
    "plt.xlim((-20, 20)) \n",
    "from scipy.stats import norm as nm\n",
    "\n",
    "from statistics import NormalDist\n",
    "norm = NormalDist.from_samples(dif_x) \n",
    "print(norm.mean) \n",
    "print(norm.stdev)\n",
    "xxx = np.linspace(-20, 20, 100)\n",
    "from statistics import NormalDist \n",
    "norm = NormalDist.from_samples(dif_rec)\n",
    "print(norm.mean) \n",
    "print(norm.stdev)\n",
    "xxx = np.linspace(-20, 20, 100) \n",
    "plt.savefig(\"outputlambda3.pdf\") #########################################################################'''\n",
    "\n",
    "f = open(\"/hpc/group/vossenlab/kam264/dir_name_2/batch_filer.py\", \"w\") \n",
    "f.write(code)  \n",
    "f.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f345e4ba-58fa-4ba7-9757-87d360639239",
   "metadata": {},
   "outputs": [],
   "source": [
    "13074079, labmda 50 epochs. 8 \n",
    "Device =  cuda:0\n",
    "DEBUGGING: torch.cuda.is_available() =  True\n",
    "Number of training graphs: 13323\n",
    "Number of validation graphs: 1665\n",
    "Number of test graphs: 1666\n",
    "Epoch  0  Train mse:  0.11418712324345603\n",
    "Epoch  0  Validation mse:  0.12300028070673212\n",
    "Epoch  9  Train mse:  0.10811281300858049\n",
    "Epoch  9  Validation mse:  0.11824354251941761\n",
    "Epoch  18  Train mse:  0.10617317992681892\n",
    "Epoch  18  Validation mse:  0.11660980906214442\n",
    "Epoch  27  Train mse:  0.10476982794858078\n",
    "Epoch  27  Validation mse:  0.1159476076876437\n",
    "Epoch  36  Train mse:  0.10392544483052246\n",
    "Epoch  36  Validation mse:  0.11599628001719982\n",
    "Epoch  45  Train mse:  0.10280925995110124\n",
    "Epoch  45  Validation mse:  0.11516106236088383\n",
    "Epoch  54  Train mse:  0.10186095160066495\n",
    "Epoch  54  Validation mse:  0.11490584536715671\n",
    "Epoch  57  Train mse:  0.10166625948645891\n",
    "Epoch  57  Validation mse:  0.11475377211699615\n",
    "KstestResult(statistic=0.22448979591836735, pvalue=3.4406253902989497e-37)\n",
    "KstestResult(statistic=0.49159663865546216, pvalue=4.773546502099893e-183)\n",
    "4.724261289604438 \n",
    "8.734397769901678\n",
    "-0.6083568734422076\n",
    "6.563282006342264\n",
    "-7.349712741561758\n",
    "11.031449617693259 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235d598a-0cfb-4ef7-b924-eda0e962aa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = '''import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import numpy.ma as ma \n",
    "import awkward as ak\n",
    "from tqdm import tqdm \n",
    "import torch \n",
    "import torch_geometric as tg \n",
    "import torch_geometric \n",
    "from torch_geometric.data import Data \n",
    "#import torch\n",
    "from torch_geometric.data import InMemoryDataset, download_url  \n",
    "import torch_geometric.transforms as T \n",
    "\n",
    "#NOTE: NEW 2/20/23     \n",
    "from typing import List, Union   \n",
    "\n",
    "from torch_geometric.data import Data, HeteroData  \n",
    "from torch_geometric.data.datapipes import functional_transform \n",
    "from torch_geometric.transforms import BaseTransform \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f908126-1475-4820-ab05-978b4caf6248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4a6cf6-9269-4afa-ab29-5d6dc89a85bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4d91d4-6141-4e83-ab8f-d2b4aaec04f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58285a05-032c-455b-9fa5-361262fd6620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c895f60-0718-4292-bd44-ddd5cf3c3d83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
