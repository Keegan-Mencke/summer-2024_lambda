{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ee30794-fd3b-4c7f-9332-5fbf12f32670",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = '''import os   \n",
    "import numpy as np\n",
    "import numpy.ma as ma   \n",
    "#import awkward as ak \n",
    "from tqdm import tqdm \n",
    "import torch \n",
    "import torch_geometric as tg  \n",
    "import torch_geometric \n",
    "from torch_geometric.data import Data \n",
    "#import torch \n",
    "from torch_geometric.data import InMemoryDataset, download_url  \n",
    "import torch_geometric.transforms as T \n",
    "\n",
    "#NOTE: NEW 2/20/23      \n",
    "from typing import List, Union     \n",
    "\n",
    "from torch_geometric.data import Data, HeteroData \n",
    "from torch_geometric.data.datapipes import functional_transform\n",
    "from torch_geometric.transforms import BaseTransform       \n",
    "torch.cuda.empty_cache()  \n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn.norm import GraphNorm, BatchNorm \n",
    "from torch.utils.data import random_split \n",
    "from torch_geometric.loader import DataLoader \n",
    "\n",
    "class MyOwnDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.load(self.processed_paths[0])\n",
    "        # For PyG<2.4:\n",
    "        # self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['some_file_1', 'some_file_2']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "    def process(self):\n",
    "        # Read data into huge `Data` list.\n",
    "        data_list = None\n",
    "\n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    "\n",
    "        self.save(data_list, self.processed_paths[0]) \n",
    "\n",
    "#root = '/hpc/group/vossenlab/kam264/L_imass3'\n",
    "#root = '/hpc/group/vossenlab/kam264/onlylambda_25k' \n",
    "#root = '/hpc/group/vossenlab/kam264/Lambda_13_16'\n",
    "#root = '/hpc/group/vossenlab/kam264/Lambda_1all' \n",
    "\n",
    "batch_size = 16 \n",
    "LR =1e-2\n",
    "torch.cuda.empty_cache()   \n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels): \n",
    "        super(GCN, self).__init__()\n",
    "#         torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)#.jittable() #NOTE: NEEDED FOR DEPLOYMENT IN CMAKE\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)#.jittable()\n",
    "        #self.block2 = nn.DataParallel(self.block2)\n",
    "        #self.conv2 = torch.nn.DataParallel(self.conv2) #this was trying the parallization thing. \n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)#.jittable()\n",
    "        #self.conv3 = torch.nn.DataParallel(self.conv3)\n",
    "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin3 = Linear(hidden_channels, out_channels)\n",
    "        self.bn1 = torch_geometric.nn.norm.GraphNorm(hidden_channels)\n",
    "        self.bn2 = torch_geometric.nn.norm.GraphNorm(hidden_channels)\n",
    "        self.bn3 = torch_geometric.nn.norm.GraphNorm(hidden_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):  \n",
    "        \n",
    "        x = self.conv1(x, edge_index) #input layer                             \n",
    "                                                      \n",
    "        x = self.bn1(x) #normalize it                                          \n",
    "\n",
    "        x = x.relu() #activation                                               \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = x.relu()\n",
    "#         print(\"x.relu() = \",x)  \n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = self.bn3(x)\n",
    "#         # 2. Readout layer                                                   \n",
    "        x = global_mean_pool(x, batch)\n",
    "        # 3. Apply a final classifier                                          \n",
    "        x = F.dropout(x, p=0.5, training=self.training) #for overfittin        \n",
    "        x = self.lin3(x) \n",
    " \n",
    "        return x\n",
    "def RMSELoss(out,y):\n",
    "    return torch.sqrt(torch.mean((out-y)**2))\n",
    "def train():\n",
    "    model.train() #initailize the model                                                                                                                                                                                                        \n",
    "    #for i, data in tqdm(enumerate(train_loader)): #perhaps tqdm(enumerate(train_loader)), i is index, data jsut moves through all the dtaa in trainingg                                                                                      \n",
    "    for i,data in enumerate(train_loader):\n",
    "        data = data.to(device) #switch to GPU                                                                                                                                                                                                 \n",
    "        optimizer.zero_grad() #                                                                                                                                                                                                               \n",
    "        out = model(data.x, data.edge_index, data.batch).to(device)  # Perform a single forward pass                                                                                                                                          \n",
    "        yy = []\n",
    "        for j in range(0,len(out)):\n",
    "            yy+= [[data.y[j].item()]]\n",
    "\n",
    "        yy = torch.tensor(yy).to(device) \n",
    "        #print(out)\n",
    "        #print(yy)\n",
    "        loss = losss(out, yy).to(device) #compute the loss  \n",
    "        #print(loss)\n",
    "        loss.backward() #get the gradients.                                                                                                                                                                                                   \n",
    "        optimizer.step() \n",
    "def test(loader): \n",
    "    length = len(loader.dataset)\n",
    "    model.eval() #evaluate teh model.                                                                                                                                                                                                         \n",
    "\n",
    "    #mse_tot = []                                                                                                                                                                                                                             \n",
    "    mse_total = 0\n",
    "    mse_pi = 0\n",
    "    mse_p = 0\n",
    "    #r                                                                                                                                                                                                                                        \n",
    "    #for data in tqdm(loader):  # Iterate in batches over the training/test dataset.                                                                                                                                                          \n",
    "    for data in loader:\n",
    "        data = data.to(device) #put to GPU                                                                                                                                                                                                    \n",
    "        out = model(data.x, data.edge_index, data.batch).to(device) #evalueate                                                                                                                                                                \n",
    "        #this and the for loop is converting data.y to a tensor in the same shape as out rows and 2 columns first is y_pion second is y_proton                                                                                                \n",
    "        yy = []\n",
    "        for j in range(0,len(out)):\n",
    "            yy+= [[data.y[j].item()]]\n",
    "        yy = torch.tensor(yy).to(device) \n",
    "        loss = losss(out, yy).cpu() #getting teh loss function                                                                                                                                                                                \n",
    "        mse_total+=loss.item() #getting the mse (total)                                                                                                                                                                                       \n",
    "\n",
    "    return mse_total/length \n",
    "def print_out():\n",
    "    model.eval() #initailize the model                                                                                                                                                                                                        \n",
    "    #for i, data in tqdm(enumerate(train_loader)): #perhaps tqdm(enumerate(train_loader)), i is index, data jsut moves through all the dtaa in trainingg                                                                                      \n",
    "    outt= []\n",
    "    for i,data in enumerate(test_loader):\n",
    "        data = data.to(device) #switch to GPU                                                                                                                                                                                                 \n",
    "        optimizer.zero_grad() #                                                                                                                                                                                                               \n",
    "        out = model(data.x, data.edge_index, data.batch).to(device)  # Perform a single forward pass                                                                                                                                          \n",
    "        out = out.cpu()\n",
    "\n",
    "        outt+=[[out.detach().numpy()]]\n",
    "    return outt\n",
    "\n",
    "def print_outb():\n",
    "    model.eval() #initailize the model                                                                                                                                                                                                        \n",
    "    #for i, data in tqdm(enumerate(train_loader)): #perhaps tqdm(enumerate(train_loader)), i is index, data jsut moves through all the dtaa in trainingg      \n",
    "\n",
    "    outt= [] \n",
    "    #for i,data in enumerate(dataset):\n",
    "    for i,data in enumerate(test_loader):\n",
    "        data = data.to(device) #switch to GPU                                                                                                                                                                                                 \n",
    "        optimizer.zero_grad() #                                                                                                                                                                                                               \n",
    "        out = model(data.x, data.edge_index, data.batch).to(device)  # Perform a single forward pass                                                                                                                                          \n",
    "        out = out.cpu()\n",
    "\n",
    "        neww = []\n",
    "        for j in range(len(data)):\n",
    "            neww.append(out[j].item() -data.elc[j][0]) \n",
    "        outt+=[[neww]]\n",
    "    return outt  \n",
    "#model = GCN(dataset.num_node_features,64,2)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') \n",
    "#devicee = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu') \n",
    "print(\"Device = \",device) \n",
    "\n",
    "print(\"DEBUGGING: torch.cuda.is_available() = \",torch.cuda.is_available())\n",
    "train_metrics = {'mse':[], \"rmse\":[] } \n",
    "vall_metrics = {'mse':[], \"rmse\":[] }\n",
    "\n",
    "for iii in range(4):   \n",
    "    if iii>0: \n",
    "        #del dataset\n",
    "        del train_loader\n",
    "        del val_loader \n",
    "    if iii==0:\n",
    "        root = '/hpc/group/vossenlab/kam264/Hnorec_first'\n",
    "    if iii==1:\n",
    "        root = '/hpc/group/vossenlab/kam264/Hnorec_second'\n",
    "    if iii==2:\n",
    "        root = '/hpc/group/vossenlab/kam264/Hnorec_third'\n",
    "    if iii==3:\n",
    "        root = '/hpc/group/vossenlab/kam264/Hnorec_fourth'   \n",
    "    dataset = MyOwnDataset(\n",
    "            root,\n",
    "            transform=None, #T.Compose([T.ToUndirected(),T.KNNGraph(k=6)]),\n",
    "            pre_transform=None,\n",
    "            pre_filter=None\n",
    "        ) \n",
    "    dataset\n",
    "    model = GCN(dataset.num_node_features,64,2)\n",
    "    fracs = [0.9, 0.08, 0.02] #percent of dataset used for training testing and validatoin 80%,10%,10% #NOTE: SHOULD CHECK np.sum(fracs) == 1 and len(fracs)==3\n",
    "    fracs = [torch.sum(torch.tensor(fracs[:idx])) for idx in range(1,len(fracs)+1)] #get the indexes for training ... parts to use. \n",
    "    #print(fracs)\n",
    "    split1, split2 = [int(len(dataset)*frac) for frac in fracs[:-1]] \n",
    "    train_dataset = dataset[:split1]\n",
    "    val_dataset = dataset[split1:split2]\n",
    "    test_dataset = dataset[split2:]   \n",
    "     \n",
    "    print(f'Number of training graphs: {len(train_dataset)}')\n",
    "    print(f'Number of validation graphs: {len(val_dataset)}') \n",
    "\n",
    "    from torch_geometric.loader import DataLoader \n",
    "    #from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=8, shuffle=True)#, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset,  batch_size=batch_size,num_workers=8,  shuffle=True)\n",
    "    model = GCN(dataset.num_node_features, dataset.num_classes, 1).to(device) #initiate the model, #2 is the number of outputs here is 2 as pion_z, proton_z \n",
    "    model = model.to(device)  \n",
    "    del dataset\n",
    "    del train_dataset\n",
    "    del val_dataset\n",
    "    del test_dataset \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr= LR) \n",
    "    \n",
    "    losss = RMSELoss  \n",
    "    nepochs =  18  \n",
    "    \n",
    "    for epoch in range(nepochs):  \n",
    "  \n",
    "        #print(\"BEFORE TRAIN()\")                                                                                                                                                                                                                  \n",
    "        train() \n",
    "        #print(\"BEFORE TEST(TRAIN_LOADER)\")                                                                                                                                                                                                       \n",
    "        #train_mse, train_rmse, train_mse_pi, train_rmse_pi, train_mse_p, train_rmse_p = test(train_loader)\n",
    "        train_mse = test(train_loader)\n",
    "\n",
    "        train_metrics['mse'].append(train_mse) \n",
    "\n",
    "        vall_mse =test(val_loader) \n",
    "\n",
    "\n",
    "        vall_metrics['mse'].append(vall_mse) \n",
    "\n",
    "        if epoch%9==0:\n",
    "            print(\"Epoch \",epoch,\" Train mse: \",train_mse)\n",
    "            print(\"Epoch \",epoch,\" Validation mse: \",vall_mse)\n",
    "        if epoch==(nepochs-1):\n",
    "            #PATH = '/work/clas12/users/mfmce/CLAS12_Lambda_resolution_REU_2023/model_best_auc.pt' \n",
    "\n",
    "            #a = print_out()\n",
    "            #b=  print_outb() \n",
    "            print(\"Epoch \",epoch,\" Train mse: \",train_mse)\n",
    "            print(\"Epoch \",epoch,\" Validation mse: \",vall_mse)\n",
    "\n",
    "root = '/hpc/group/vossenlab/kam264/Hnorec_Last'     \n",
    "dataset = MyOwnDataset(\n",
    "        root,\n",
    "        transform=None, #T.Compose([T.ToUndirected(),T.KNNGraph(k=6)]),\n",
    "        pre_transform=None,\n",
    "        pre_filter=None\n",
    "    )    \n",
    "\n",
    "model.eval() #initailize the model                                                                                                                                                                                                        \n",
    "#for i, data in tqdm(enumerate(train_loader)): #perhaps tqdm(enumerate(train_loader)), i is index, data jsut moves through all the dtaa in trainingg      \n",
    "\n",
    "outt= [] \n",
    "pi_mas = []\n",
    "#for i,data in enumerate(dataset): \n",
    "for i,data in enumerate(dataset):\n",
    "    data = data.to(device) #switch to GPU                                                                                                                                                                                                 \n",
    "    #optimizer.zero_grad() #                                                                                                                                                                                                               \n",
    "    out = model(data.x, data.edge_index, data.batch).to(device)  # Perform a single forward pass                                                                                                                                          \n",
    "    out = out.cpu()\n",
    "    #print(out)\n",
    "    #print(data.elc) \n",
    "\n",
    "   # neww = []\n",
    "    #for j in range(len(data)):\n",
    "        #neww.append(out[j].item() -data.elc[j][0])\n",
    "     #   neww.append(out[j].item() -data.elc[j])\n",
    "    #outt+=[[neww]]\n",
    "    #outt+=[[out[0][0].item()-data.elc[0]]] \n",
    "    outt+=[[out[0][0].item()]]\n",
    "    for j in range(0,int(len(data.imass))): \n",
    "        pi_mas.append(data.imass[j] ) \n",
    "#return outt  \n",
    "#b= print_outbb() \n",
    "b=outt \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "pi_x = np.zeros(len(outt))\n",
    "for i in range(len(outt)):\n",
    "    pi_x[i] = outt[i][0]\n",
    "pi_yy = [] \n",
    "#p_yy = [] \n",
    "#pi_x = outt\n",
    "pi_rec_y = []\n",
    "#p_rec_y = []\n",
    "for i, data in enumerate(dataset):\n",
    "    data = data#.to(device) #put to GPU\n",
    "    #break\n",
    "    jj= 0\n",
    "    for j in range(0,int(len(data.y))): \n",
    "        pi_yy.append(data.y[j].item() -data.elc[j] ) \n",
    "      \n",
    "    for j in range(len(data.rec)):\n",
    "        pi_rec_y.append(data.rec[j]-data.elc[j])\n",
    "       # p_rec_y.append(data.rec[j][0][1])\n",
    "\n",
    "#plott_pi = np.zeros((len(pi_x),4))\n",
    "#plott_p = np.zeros((len(p_x),2)) \n",
    "plott_pi = np.zeros((len(pi_x),3))\n",
    "for i in range(len(pi_x)):\n",
    "#for i in range(982):\n",
    "    plott_pi[i][0] = pi_yy[i]\n",
    "    plott_pi[i][1] = pi_x[i]# [0]\n",
    "    #plott_pi[i][3] = pi_val[i]\n",
    "   # plott_pi[i][2] = pi_rec_y[i] \n",
    "   # plott_p[i][0] = p_yy[i]\n",
    "   # plott_p[i][1] = p_x[i]\n",
    "    #plott_p[i][2]  = p_rec_y[i] \n",
    "for i in range(len(pi_x)): \n",
    "    plott_pi[i][2] = pi_rec_y[i] \n",
    "#plott_pi \n",
    "plt.figure(0) ##################\n",
    "plt.title('location of Pion vertex') \n",
    "#plt.axvline(x = np.mean(pi_x), color = 'blue') \n",
    "#plt.axvline(x = np.mean(pi_yy), color = 'orange') \n",
    "plt.hist(x = plott_pi, histtype ='step', color = ['orange', 'blue', 'green'], bins = 500 ) \n",
    "#plt.hist(x = plott_pi, histtype ='step', bins = 500 )\n",
    "#plt.legend(['Predicted', 'True', 'REC']) \n",
    "#plt.legend(['True', 'Predicted', 'REC']) \n",
    "plt.xlim((-10,15)) \n",
    "plt.ylabel('count') \n",
    "plt.xlabel('Location of Pion vertex (cm)') \n",
    " #############################################################\n",
    "\n",
    "plt.figure(1) #######################\n",
    "pi_yy = np.array(pi_yy)\n",
    "pi_x = np.array(pi_x)\n",
    "pi_rec_y = np.array(pi_rec_y)#np.logical_and\n",
    "indices_two = np.where(np.logical_and(pi_yy < -1.5, pi_yy > -2) ) \n",
    "#indices_two = np.where(pi_yy <-1.5 and pi_yy>-2 )\n",
    "#print(indices_two)\n",
    "#print(pi_x[indices_two])  \n",
    "\n",
    "diff_x = [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
    "diff_rec  = [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
    "for j in range(1,24):\n",
    "    indices = np.where(np.logical_and(pi_yy < -2+j/2, pi_yy > -2+j/2-.5) ) \n",
    "    dif_x =[ abs(pi_x[indices] - pi_yy[indices])]\n",
    "    dif_rec = [abs(pi_rec_y[indices] - pi_yy[indices])]\n",
    "    diff_x[j-1].append(dif_x)\n",
    "    diff_rec[j-1].append(dif_rec)\n",
    "mean_x = np.zeros(24)\n",
    "mean_rec = np.zeros(24)\n",
    "error_x = np.zeros(24)\n",
    "error_rec = np.zeros(24)\n",
    "for i in range(24): \n",
    "    mean_x[i] = np.nanmean(diff_x[i])\n",
    "    mean_rec[i] = np.nanmean(diff_rec[i]) \n",
    "    error_x[i] = np.nanstd(diff_x[i])\n",
    "    error_rec[i] = np.nanstd(diff_rec[i])\n",
    "xxx = np.linspace(-2,10,24)\n",
    "plt.scatter(xxx, mean_x)\n",
    "plt.scatter(xxx, mean_rec, alpha = .5 )  \n",
    "plt.errorbar(xxx, mean_x,  yerr=error_x, fmt=\"o\")   \n",
    "plt.errorbar(xxx, mean_rec,  yerr=error_rec, fmt=\"o\", alpha = .5)\n",
    "plt.title('absolute error based on pion vertex, orange is rec, blue is GNN')\n",
    "plt.xlabel('Pion vertex (cm)') \n",
    "plt.ylabel('abolute error from True (cm)')\n",
    " #############################################################\n",
    "plt.figure(2) #######################\n",
    "diff_x = [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
    "diff_rec  = [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
    "for j in range(1,24):\n",
    "    indices = np.where(np.logical_and(pi_yy < -2+j/2, pi_yy > -2+j/2-.5) ) \n",
    "    dif_x =[ (pi_x[indices] - pi_yy[indices])]\n",
    "    dif_rec = [(pi_rec_y[indices] - pi_yy[indices])]\n",
    "    diff_x[j-1].append(dif_x)\n",
    "    diff_rec[j-1].append(dif_rec)\n",
    "mean_x = np.zeros(24)\n",
    "mean_rec = np.zeros(24)\n",
    "error_x = np.zeros(24)\n",
    "error_rec = np.zeros(24)\n",
    "for i in range(24): \n",
    "    mean_x[i] = np.nanmean(diff_x[i])\n",
    "    mean_rec[i] = np.nanmean(diff_rec[i]) \n",
    "    error_x[i] = np.nanstd(diff_x[i])\n",
    "    error_rec[i] = np.nanstd(diff_rec[i])\n",
    "xxx = np.linspace(-2,10,24)\n",
    "plt.scatter(xxx, mean_x)\n",
    "plt.scatter(xxx, mean_rec, alpha = .5 )  \n",
    "plt.errorbar(xxx, mean_x,  yerr=error_x, fmt=\"o\")   \n",
    "plt.errorbar(xxx, mean_rec,  yerr=error_rec, fmt=\"o\", alpha = .5)\n",
    "plt.title(' error based on pion vertex, orange is rec, blue is GNN')\n",
    "plt.xlabel('Pion vertex (cm)') \n",
    "plt.ylabel(' error from True (cm)')\n",
    " #############################################################\n",
    "\n",
    "pi_mas = []\n",
    "pi_LamT = []\n",
    "#p_rec_y = []\n",
    "for i, data in enumerate(dataset):\n",
    "    data = data#.to(device) #put to GPU\n",
    "    #break\n",
    "    #print(data.LamT)\n",
    "    for j in range(0,int(len(data.imass))): \n",
    "        pi_mas.append(data.imass[j]) \n",
    "        pi_LamT.append(data.LamT[j]) \n",
    "Total_signal = pi_LamT.count(1)\n",
    "print('S/B')\n",
    "print('Purity') \n",
    "print('Efficienty')\n",
    "s_to_b_values = []\n",
    "purity_values = []\n",
    "efficiency_values = []\n",
    "for j in range(0, 20):\n",
    "    pi_LamTt = []\n",
    "    #imass\n",
    "    print('cut ',j/2)\n",
    "    for i in range(len(pi_x)): \n",
    "        if pi_x[i]>j/2:\n",
    "            #imass.append(pi_mas[i])\n",
    "            pi_LamTt.append(pi_LamT[i])\n",
    "    print(pi_LamTt.count(1))\n",
    "    #print(pi_LamT.count(0))\n",
    "    print(pi_LamTt.count(1)/pi_LamTt.count(0))\n",
    "    print(pi_LamTt.count(1)/(len(pi_LamTt)))\n",
    "    print(pi_LamTt.count(1)/(Total_signal))\n",
    "    #print(pi_LamTt.count(1)/len(pi_LamTt))\n",
    "    s_to_b = pi_LamTt.count(1)/pi_LamTt.count(0)\n",
    "    purity = pi_LamTt.count(1)/(len(pi_LamTt))\n",
    "    efficiency = pi_LamTt.count(1)/(Total_signal)\n",
    "    s_to_b_values.append(s_to_b)\n",
    "    purity_values.append(purity)\n",
    "    efficiency_values.append(efficiency)\n",
    "    print()\n",
    "    plt.figure(3)\n",
    "    plt.scatter(j/2, pi_LamTt.count(1)/pi_LamTt.count(0), color = 'blue')\n",
    "    plt.xlabel('pion vetex cut')\n",
    "    plt.ylabel('S/B') \n",
    "    plt.title('Graph of S/B as pion vertex cuts')\n",
    "    \n",
    "    plt.figure(4)\n",
    "    plt.scatter(j/2, pi_LamTt.count(1)/(len(pi_LamTt)), color = 'red')\n",
    "    plt.xlabel('pion vetex cut')\n",
    "    plt.ylabel('Purity') \n",
    "    plt.title('Graph of Purity as pion vertex cuts')\n",
    "                \n",
    "    plt.figure(5)\n",
    "    plt.scatter(j/2, pi_LamTt.count(1)/(Total_signal), color = 'purple') \n",
    "    plt.xlabel('pion vetex cut')\n",
    "    plt.ylabel('Efficiency ') \n",
    "    plt.title('Graph of Efficiency  as pion vertex cuts')\n",
    "    #print(len(pi_LamT))\n",
    "    #plt.figure()\n",
    "    #plt.title('cut'+str( j/2))  \n",
    "    #plt.hist(imass, bins = 25)\n",
    "plt.figure(0)\n",
    "plt.savefig('LHnrec1T.png') \n",
    "plt.figure(1)\n",
    "plt.savefig('LHnrec2T.png') \n",
    "plt.figure(2)\n",
    "plt.savefig('LHnrec3T.png')\n",
    "plt.figure(3)\n",
    "plt.savefig('LHnrec4T.png') \n",
    "plt.figure(4)\n",
    "plt.savefig('LHnrec5T.png')\n",
    "plt.figure(5)\n",
    "plt.savefig('LHnrec6T.png')   \n",
    "np.save('s_to_b_valuesHnrec2.npy', s_to_b_values)\n",
    "np.save('purity_valuesHnrec2.npy', purity_values) \n",
    "np.save('efficiency_valuesHnrec2.npy', efficiency_values)\n",
    "for j in range(0, 20):\n",
    "    imass = []\n",
    "    print('cut ',j/2)\n",
    "    for i in range(len(pi_x)):\n",
    "        if pi_x[i]>j/2:\n",
    "            imass.append(pi_mas[i])\n",
    "    plt.figure(j+5)\n",
    "    plt.title('cut'+str( j/2)) \n",
    "    plt.hist(imass, bins = 25)\n",
    "    plt.savefig('cut_trying_no_electron'+str(j/2)+'.png')\n",
    "'''    \n",
    "\n",
    "f = open(\"/hpc/group/vossenlab/kam264/dir_name_2/batch_filer.py\", \"w\") \n",
    "f.write(code)             \n",
    "f.close()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69af0ee1-daf6-40ce-8b68-6876c2ce12c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e64a9ce-e655-4f6f-8ce0-25e3c30f5507",
   "metadata": {},
   "outputs": [],
   "source": [
    "14254553, \n",
    "14310550, longer one \n",
    "14822104  \n",
    "14824602 , LHnrec1F, 'cut_f'+str(j/2)+'.png', normal try. \n",
    "14860833"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e37d47c-fbe5-4242-b9dc-bb050a596087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import numpy as np\n",
    "import numpy.ma as ma   \n",
    "#import awkward as ak \n",
    "from tqdm import tqdm \n",
    "import torch \n",
    "import torch_geometric as tg  \n",
    "import torch_geometric \n",
    "from torch_geometric.data import Data \n",
    "#import torch \n",
    "from torch_geometric.data import InMemoryDataset, download_url  \n",
    "import torch_geometric.transforms as T \n",
    "\n",
    "#NOTE: NEW 2/20/23      \n",
    "from typing import List, Union    \n",
    "\n",
    "from torch_geometric.data import Data, HeteroData \n",
    "from torch_geometric.data.datapipes import functional_transform\n",
    "from torch_geometric.transforms import BaseTransform       \n",
    "torch.cuda.empty_cache()  \n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn.norm import GraphNorm, BatchNorm \n",
    "from torch.utils.data import random_split \n",
    "from torch_geometric.loader import DataLoader \n",
    "\n",
    "class MyOwnDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.load(self.processed_paths[0])\n",
    "        # For PyG<2.4:\n",
    "        # self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['some_file_1', 'some_file_2']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "    def process(self):\n",
    "        # Read data into huge `Data` list.\n",
    "        data_list = None\n",
    "\n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    "\n",
    "        self.save(data_list, self.processed_paths[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5692a682-de29-4a3b-8944-21b386a0950d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyOwnDataset(18500)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = '/hpc/group/vossenlab/kam264/QuatorL_Last'    \n",
    "dataset = MyOwnDataset(\n",
    "        root,\n",
    "        transform=None, #T.Compose([T.ToUndirected(),T.KNNGraph(k=6)]),\n",
    "        pre_transform=None,\n",
    "        pre_filter=None\n",
    "    )     \n",
    "dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "370e5bee-7ef7-434f-a9e3-09f5fd7c542e",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = '''import os   \n",
    "import numpy as np\n",
    "import numpy.ma as ma   \n",
    "#import awkward as ak \n",
    "from tqdm import tqdm \n",
    "import torch \n",
    "import torch_geometric as tg  \n",
    "import torch_geometric \n",
    "from torch_geometric.data import Data \n",
    "#import torch \n",
    "from torch_geometric.data import InMemoryDataset, download_url  \n",
    "import torch_geometric.transforms as T \n",
    "\n",
    "#NOTE: NEW 2/20/23      \n",
    "from typing import List, Union     \n",
    "\n",
    "from torch_geometric.data import Data, HeteroData \n",
    "from torch_geometric.data.datapipes import functional_transform\n",
    "from torch_geometric.transforms import BaseTransform       \n",
    "torch.cuda.empty_cache()  \n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn.norm import GraphNorm, BatchNorm \n",
    "from torch.utils.data import random_split \n",
    "from torch_geometric.loader import DataLoader \n",
    "\n",
    "class MyOwnDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.load(self.processed_paths[0])\n",
    "        # For PyG<2.4:\n",
    "        # self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['some_file_1', 'some_file_2']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "    def process(self):\n",
    "        # Read data into huge `Data` list.\n",
    "        data_list = None\n",
    "\n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    "\n",
    "        self.save(data_list, self.processed_paths[0]) \n",
    "\n",
    "#root = '/hpc/group/vossenlab/kam264/L_imass3'\n",
    "#root = '/hpc/group/vossenlab/kam264/onlylambda_25k' \n",
    "#root = '/hpc/group/vossenlab/kam264/Lambda_13_16'\n",
    "#root = '/hpc/group/vossenlab/kam264/Lambda_1all' \n",
    "\n",
    "batch_size = 16 \n",
    "LR =1e-2\n",
    "torch.cuda.empty_cache()   \n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels): \n",
    "        super(GCN, self).__init__()\n",
    "#         torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)#.jittable() #NOTE: NEEDED FOR DEPLOYMENT IN CMAKE\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)#.jittable()\n",
    "        #self.block2 = nn.DataParallel(self.block2)\n",
    "        #self.conv2 = torch.nn.DataParallel(self.conv2) #this was trying the parallization thing. \n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)#.jittable()\n",
    "        #self.conv3 = torch.nn.DataParallel(self.conv3)\n",
    "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin3 = Linear(hidden_channels, out_channels)\n",
    "        self.bn1 = torch_geometric.nn.norm.GraphNorm(hidden_channels)\n",
    "        self.bn2 = torch_geometric.nn.norm.GraphNorm(hidden_channels)\n",
    "        self.bn3 = torch_geometric.nn.norm.GraphNorm(hidden_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):  \n",
    "        \n",
    "        x = self.conv1(x, edge_index) #input layer                             \n",
    "                                                      \n",
    "        x = self.bn1(x) #normalize it                                          \n",
    "\n",
    "        x = x.relu() #activation                                               \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = x.relu()\n",
    "#         print(\"x.relu() = \",x)  \n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = self.bn3(x)\n",
    "#         # 2. Readout layer                                                   \n",
    "        x = global_mean_pool(x, batch)\n",
    "        # 3. Apply a final classifier                                          \n",
    "        x = F.dropout(x, p=0.5, training=self.training) #for overfittin        \n",
    "        x = self.lin3(x) \n",
    " \n",
    "        return x\n",
    "def RMSELoss(out,y):\n",
    "    return torch.sqrt(torch.mean((out-y)**2))\n",
    "def train():\n",
    "    model.train() #initailize the model                                                                                                                                                                                                        \n",
    "    #for i, data in tqdm(enumerate(train_loader)): #perhaps tqdm(enumerate(train_loader)), i is index, data jsut moves through all the dtaa in trainingg                                                                                      \n",
    "    for i,data in enumerate(train_loader):\n",
    "        data = data.to(device) #switch to GPU                                                                                                                                                                                                 \n",
    "        optimizer.zero_grad() #                                                                                                                                                                                                               \n",
    "        out = model(data.x, data.edge_index, data.batch).to(device)  # Perform a single forward pass                                                                                                                                          \n",
    "        yy = []\n",
    "        for j in range(0,len(out)):\n",
    "            yy+= [[data.y[j].item()]]\n",
    "\n",
    "        yy = torch.tensor(yy).to(device) \n",
    "        #print(out)\n",
    "        #print(yy)\n",
    "        loss = losss(out, yy).to(device) #compute the loss  \n",
    "        #print(loss)\n",
    "        loss.backward() #get the gradients.                                                                                                                                                                                                   \n",
    "        optimizer.step() \n",
    "def test(loader): \n",
    "    length = len(loader.dataset)\n",
    "    model.eval() #evaluate teh model.                                                                                                                                                                                                         \n",
    "\n",
    "    #mse_tot = []                                                                                                                                                                                                                             \n",
    "    mse_total = 0\n",
    "    mse_pi = 0\n",
    "    mse_p = 0\n",
    "    #r                                                                                                                                                                                                                                        \n",
    "    #for data in tqdm(loader):  # Iterate in batches over the training/test dataset.                                                                                                                                                          \n",
    "    for data in loader:\n",
    "        data = data.to(device) #put to GPU                                                                                                                                                                                                    \n",
    "        out = model(data.x, data.edge_index, data.batch).to(device) #evalueate                                                                                                                                                                \n",
    "        #this and the for loop is converting data.y to a tensor in the same shape as out rows and 2 columns first is y_pion second is y_proton                                                                                                \n",
    "        yy = []\n",
    "        for j in range(0,len(out)):\n",
    "            yy+= [[data.y[j].item()]]\n",
    "        yy = torch.tensor(yy).to(device) \n",
    "        loss = losss(out, yy).cpu() #getting teh loss function                                                                                                                                                                                \n",
    "        mse_total+=loss.item() #getting the mse (total)                                                                                                                                                                                       \n",
    "\n",
    "    return mse_total/length \n",
    "def print_out():\n",
    "    model.eval() #initailize the model                                                                                                                                                                                                        \n",
    "    #for i, data in tqdm(enumerate(train_loader)): #perhaps tqdm(enumerate(train_loader)), i is index, data jsut moves through all the dtaa in trainingg                                                                                      \n",
    "    outt= []\n",
    "    for i,data in enumerate(test_loader):\n",
    "        data = data.to(device) #switch to GPU                                                                                                                                                                                                 \n",
    "        optimizer.zero_grad() #                                                                                                                                                                                                               \n",
    "        out = model(data.x, data.edge_index, data.batch).to(device)  # Perform a single forward pass                                                                                                                                          \n",
    "        out = out.cpu()\n",
    "\n",
    "        outt+=[[out.detach().numpy()]]\n",
    "    return outt\n",
    "\n",
    "def print_outb():\n",
    "    model.eval() #initailize the model                                                                                                                                                                                                        \n",
    "    #for i, data in tqdm(enumerate(train_loader)): #perhaps tqdm(enumerate(train_loader)), i is index, data jsut moves through all the dtaa in trainingg      \n",
    "\n",
    "    outt= [] \n",
    "    #for i,data in enumerate(dataset):\n",
    "    for i,data in enumerate(test_loader):\n",
    "        data = data.to(device) #switch to GPU                                                                                                                                                                                                 \n",
    "        optimizer.zero_grad() #                                                                                                                                                                                                               \n",
    "        out = model(data.x, data.edge_index, data.batch).to(device)  # Perform a single forward pass                                                                                                                                          \n",
    "        out = out.cpu()\n",
    "\n",
    "        neww = []\n",
    "        for j in range(len(data)):\n",
    "            neww.append(out[j].item() -data.elc[j][0]) \n",
    "        outt+=[[neww]]\n",
    "    return outt  \n",
    "#model = GCN(dataset.num_node_features,64,2)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') \n",
    "#devicee = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu') \n",
    "print(\"Device = \",device) \n",
    "\n",
    "print(\"DEBUGGING: torch.cuda.is_available() = \",torch.cuda.is_available())\n",
    "train_metrics = {'mse':[], \"rmse\":[] } \n",
    "vall_metrics = {'mse':[], \"rmse\":[] }\n",
    "\n",
    "for iii in range(4):   \n",
    "    if iii>0: \n",
    "        #del dataset\n",
    "        del train_loader  \n",
    "        del val_loader \n",
    "    if iii==0:\n",
    "        root = '/hpc/group/vossenlab/kam264/QuatorL_first'\n",
    "    if iii==1:\n",
    "        root = '/hpc/group/vossenlab/kam264/QuatorL_second'\n",
    "    if iii==2:\n",
    "        root = '/hpc/group/vossenlab/kam264/QuatorL_third'\n",
    "    if iii==3:\n",
    "        root = '/hpc/group/vossenlab/kam264/QuatorL_Last'   \n",
    "    dataset = MyOwnDataset(\n",
    "            root,\n",
    "            transform=None, #T.Compose([T.ToUndirected(),T.KNNGraph(k=6)]),\n",
    "            pre_transform=None,\n",
    "            pre_filter=None\n",
    "        ) \n",
    "    dataset\n",
    "    model = GCN(dataset.num_node_features,64,2)\n",
    "    fracs = [0.9, 0.08, 0.02] #percent of dataset used for training testing and validatoin 80%,10%,10% #NOTE: SHOULD CHECK np.sum(fracs) == 1 and len(fracs)==3\n",
    "    fracs = [torch.sum(torch.tensor(fracs[:idx])) for idx in range(1,len(fracs)+1)] #get the indexes for training ... parts to use. \n",
    "    #print(fracs)\n",
    "    split1, split2 = [int(len(dataset)*frac) for frac in fracs[:-1]] \n",
    "    train_dataset = dataset[:split1]\n",
    "    val_dataset = dataset[split1:split2]\n",
    "    test_dataset = dataset[split2:]   \n",
    "     \n",
    "    print(f'Number of training graphs: {len(train_dataset)}')\n",
    "    print(f'Number of validation graphs: {len(val_dataset)}') \n",
    "\n",
    "    from torch_geometric.loader import DataLoader \n",
    "    #from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=8, shuffle=True)#, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset,  batch_size=batch_size,num_workers=8,  shuffle=True)\n",
    "    model = GCN(dataset.num_node_features, dataset.num_classes, 1).to(device) #initiate the model, #2 is the number of outputs here is 2 as pion_z, proton_z \n",
    "    model = model.to(device)  \n",
    "    del dataset\n",
    "    del train_dataset\n",
    "    del val_dataset\n",
    "    del test_dataset \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr= LR) \n",
    "    \n",
    "    losss = RMSELoss  \n",
    "    nepochs = 68  \n",
    "    \n",
    "    for epoch in range(nepochs):  \n",
    "  \n",
    "        #print(\"BEFORE TRAIN()\")                                                                                                                                                                                                                  \n",
    "        train() \n",
    "        #print(\"BEFORE TEST(TRAIN_LOADER)\")                                                                                                                                                                                                       \n",
    "        #train_mse, train_rmse, train_mse_pi, train_rmse_pi, train_mse_p, train_rmse_p = test(train_loader)\n",
    "        train_mse = test(train_loader)\n",
    "\n",
    "        train_metrics['mse'].append(train_mse) \n",
    "\n",
    "        vall_mse =test(val_loader) \n",
    "\n",
    "\n",
    "        vall_metrics['mse'].append(vall_mse) \n",
    "\n",
    "        if epoch%9==0:\n",
    "            print(\"Epoch \",epoch,\" Train mse: \",train_mse)\n",
    "            print(\"Epoch \",epoch,\" Validation mse: \",vall_mse)\n",
    "        if epoch==(nepochs-1):\n",
    "            #PATH = '/work/clas12/users/mfmce/CLAS12_Lambda_resolution_REU_2023/model_best_auc.pt' \n",
    "\n",
    "            #a = print_out()\n",
    "            #b=  print_outb() \n",
    "            print(\"Epoch \",epoch,\" Train mse: \",train_mse)\n",
    "            print(\"Epoch \",epoch,\" Validation mse: \",vall_mse)\n",
    "\n",
    "root = '/hpc/group/vossenlab/kam264/HalfL_first'     \n",
    "dataset = MyOwnDataset(\n",
    "        root,\n",
    "        transform=None, #T.Compose([T.ToUndirected(),T.KNNGraph(k=6)]),\n",
    "        pre_transform=None,\n",
    "        pre_filter=None\n",
    "    )    \n",
    "\n",
    "model.eval() #initailize the model                                                                                                                                                                                                        \n",
    "#for i, data in tqdm(enumerate(train_loader)): #perhaps tqdm(enumerate(train_loader)), i is index, data jsut moves through all the dtaa in trainingg      \n",
    "\n",
    "outt= [] \n",
    "pi_mas = []\n",
    "#for i,data in enumerate(dataset): \n",
    "for i,data in enumerate(dataset):\n",
    "    data = data.to(device) #switch to GPU                                                                                                                                                                                                 \n",
    "    #optimizer.zero_grad() #                                                                                                                                                                                                               \n",
    "    out = model(data.x, data.edge_index, data.batch).to(device)  # Perform a single forward pass                                                                                                                                          \n",
    "    out = out.cpu()\n",
    "    #print(out)\n",
    "    #print(data.elc) \n",
    "\n",
    "   # neww = []\n",
    "    #for j in range(len(data)):\n",
    "        #neww.append(out[j].item() -data.elc[j][0])\n",
    "     #   neww.append(out[j].item() -data.elc[j])\n",
    "    #outt+=[[neww]]\n",
    "    #outt+=[[out[0][0].item()-data.elc[0]]] \n",
    "    outt+=[[out[0][0].item()-data.elc[0]]]\n",
    "    for j in range(0,int(len(data.imass))): \n",
    "        pi_mas.append(data.imass[j] ) \n",
    "#return outt  \n",
    "#b= print_outbb() \n",
    "b=outt \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "pi_x = np.zeros(len(outt))\n",
    "for i in range(len(outt)):\n",
    "    pi_x[i] = outt[i][0]\n",
    "pi_yy = [] \n",
    "#p_yy = [] \n",
    "#pi_x = outt\n",
    "pi_rec_y = []\n",
    "#p_rec_y = []\n",
    "for i, data in enumerate(dataset):\n",
    "    data = data#.to(device) #put to GPU\n",
    "    #break\n",
    "    jj= 0\n",
    "    for j in range(0,int(len(data.y))): \n",
    "        pi_yy.append(data.y[j].item() -data.elc[j] ) \n",
    "      \n",
    "    for j in range(len(data.rec)):\n",
    "        pi_rec_y.append(data.rec[j]-data.elc[j])\n",
    "       # p_rec_y.append(data.rec[j][0][1])\n",
    "\n",
    "#plott_pi = np.zeros((len(pi_x),4))\n",
    "#plott_p = np.zeros((len(p_x),2)) \n",
    "plott_pi = np.zeros((len(pi_x),3))\n",
    "for i in range(len(pi_x)):\n",
    "#for i in range(982):\n",
    "    plott_pi[i][0] = pi_yy[i]\n",
    "    plott_pi[i][1] = pi_x[i]# [0]\n",
    "    #plott_pi[i][3] = pi_val[i]\n",
    "   # plott_pi[i][2] = pi_rec_y[i] \n",
    "   # plott_p[i][0] = p_yy[i]\n",
    "   # plott_p[i][1] = p_x[i]\n",
    "    #plott_p[i][2]  = p_rec_y[i] \n",
    "for i in range(len(pi_x)): \n",
    "    plott_pi[i][2] = pi_rec_y[i] \n",
    "#plott_pi \n",
    "plt.figure(0) ##################\n",
    "plt.title('location of Pion vertex') \n",
    "#plt.axvline(x = np.mean(pi_x), color = 'blue') \n",
    "#plt.axvline(x = np.mean(pi_yy), color = 'orange') \n",
    "plt.hist(x = plott_pi, histtype ='step', color = ['orange', 'blue', 'green'], bins = 500 ) \n",
    "#plt.hist(x = plott_pi, histtype ='step', bins = 500 )\n",
    "#plt.legend(['Predicted', 'True', 'REC']) \n",
    "#plt.legend(['True', 'Predicted', 'REC']) \n",
    "plt.xlim((-10,15)) \n",
    "plt.ylabel('count') \n",
    "plt.xlabel('Location of Pion vertex (cm)') \n",
    " #############################################################\n",
    "\n",
    "plt.figure(1) #######################\n",
    "pi_yy = np.array(pi_yy)\n",
    "pi_x = np.array(pi_x)\n",
    "pi_rec_y = np.array(pi_rec_y)#np.logical_and\n",
    "indices_two = np.where(np.logical_and(pi_yy < -1.5, pi_yy > -2) ) \n",
    "#indices_two = np.where(pi_yy <-1.5 and pi_yy>-2 )\n",
    "#print(indices_two)\n",
    "#print(pi_x[indices_two])  \n",
    "\n",
    "diff_x = [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
    "diff_rec  = [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
    "for j in range(1,24):\n",
    "    indices = np.where(np.logical_and(pi_yy < -2+j/2, pi_yy > -2+j/2-.5) ) \n",
    "    dif_x =[ abs(pi_x[indices] - pi_yy[indices])]\n",
    "    dif_rec = [abs(pi_rec_y[indices] - pi_yy[indices])]\n",
    "    diff_x[j-1].append(dif_x)\n",
    "    diff_rec[j-1].append(dif_rec)\n",
    "mean_x = np.zeros(24)\n",
    "mean_rec = np.zeros(24)\n",
    "error_x = np.zeros(24)\n",
    "error_rec = np.zeros(24)\n",
    "for i in range(24): \n",
    "    mean_x[i] = np.nanmean(diff_x[i])\n",
    "    mean_rec[i] = np.nanmean(diff_rec[i]) \n",
    "    error_x[i] = np.nanstd(diff_x[i])\n",
    "    error_rec[i] = np.nanstd(diff_rec[i])\n",
    "xxx = np.linspace(-2,10,24)\n",
    "plt.scatter(xxx, mean_x)\n",
    "plt.scatter(xxx, mean_rec, alpha = .5 )  \n",
    "plt.errorbar(xxx, mean_x,  yerr=error_x, fmt=\"o\")   \n",
    "plt.errorbar(xxx, mean_rec,  yerr=error_rec, fmt=\"o\", alpha = .5)\n",
    "plt.title('absolute error based on pion vertex, orange is rec, blue is GNN')\n",
    "plt.xlabel('Pion vertex (cm)') \n",
    "plt.ylabel('abolute error from True (cm)')\n",
    " #############################################################\n",
    "plt.figure(2) #######################\n",
    "diff_x = [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
    "diff_rec  = [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
    "for j in range(1,24):\n",
    "    indices = np.where(np.logical_and(pi_yy < -2+j/2, pi_yy > -2+j/2-.5) ) \n",
    "    dif_x =[ (pi_x[indices] - pi_yy[indices])]\n",
    "    dif_rec = [(pi_rec_y[indices] - pi_yy[indices])]\n",
    "    diff_x[j-1].append(dif_x)\n",
    "    diff_rec[j-1].append(dif_rec)\n",
    "mean_x = np.zeros(24)\n",
    "mean_rec = np.zeros(24)\n",
    "error_x = np.zeros(24)\n",
    "error_rec = np.zeros(24)\n",
    "for i in range(24): \n",
    "    mean_x[i] = np.nanmean(diff_x[i])\n",
    "    mean_rec[i] = np.nanmean(diff_rec[i]) \n",
    "    error_x[i] = np.nanstd(diff_x[i])\n",
    "    error_rec[i] = np.nanstd(diff_rec[i])\n",
    "xxx = np.linspace(-2,10,24)\n",
    "plt.scatter(xxx, mean_x)\n",
    "plt.scatter(xxx, mean_rec, alpha = .5 )  \n",
    "plt.errorbar(xxx, mean_x,  yerr=error_x, fmt=\"o\")   \n",
    "plt.errorbar(xxx, mean_rec,  yerr=error_rec, fmt=\"o\", alpha = .5)\n",
    "plt.title(' error based on pion vertex, orange is rec, blue is GNN')\n",
    "plt.xlabel('Pion vertex (cm)') \n",
    "plt.ylabel(' error from True (cm)')\n",
    " #############################################################\n",
    "\n",
    "pi_mas = []\n",
    "pi_LamT = []\n",
    "#p_rec_y = []\n",
    "for i, data in enumerate(dataset):\n",
    "    data = data#.to(device) #put to GPU\n",
    "    #break\n",
    "    #print(data.LamT)\n",
    "    for j in range(0,int(len(data.imass))): \n",
    "        pi_mas.append(data.imass[j]) \n",
    "        pi_LamT.append(data.LamT[j]) \n",
    "Total_signal = pi_LamT.count(1)\n",
    "print('S/B')\n",
    "print('Purity') \n",
    "print('Efficienty')\n",
    "s_to_b_values = []\n",
    "purity_values = []\n",
    "efficiency_values = []\n",
    "for j in range(0, 20):\n",
    "    pi_LamTt = []\n",
    "    #imass\n",
    "    print('cut ',j/2)\n",
    "    for i in range(len(pi_x)): \n",
    "        if pi_x[i]>j/2:\n",
    "            #imass.append(pi_mas[i])\n",
    "            pi_LamTt.append(pi_LamT[i])\n",
    "    print(pi_LamTt.count(1))\n",
    "    #print(pi_LamT.count(0))\n",
    "    print(pi_LamTt.count(1)/pi_LamTt.count(0))\n",
    "    print(pi_LamTt.count(1)/(len(pi_LamTt)))\n",
    "    print(pi_LamTt.count(1)/(Total_signal))\n",
    "    #print(pi_LamTt.count(1)/len(pi_LamTt))\n",
    "    s_to_b = pi_LamTt.count(1)/pi_LamTt.count(0)\n",
    "    purity = pi_LamTt.count(1)/(len(pi_LamTt))\n",
    "    efficiency = pi_LamTt.count(1)/(Total_signal)\n",
    "    s_to_b_values.append(s_to_b)\n",
    "    purity_values.append(purity)\n",
    "    efficiency_values.append(efficiency)\n",
    "    print()\n",
    "    plt.figure(3)\n",
    "    plt.scatter(j/2, pi_LamTt.count(1)/pi_LamTt.count(0), color = 'blue')\n",
    "    plt.xlabel('pion vetex cut')\n",
    "    plt.ylabel('S/B') \n",
    "    plt.title('Graph of S/B as pion vertex cuts')\n",
    "    \n",
    "    plt.figure(4)\n",
    "    plt.scatter(j/2, pi_LamTt.count(1)/(len(pi_LamTt)), color = 'red')\n",
    "    plt.xlabel('pion vetex cut')\n",
    "    plt.ylabel('Purity') \n",
    "    plt.title('Graph of Purity as pion vertex cuts')\n",
    "                \n",
    "    plt.figure(5)\n",
    "    plt.scatter(j/2, pi_LamTt.count(1)/(Total_signal), color = 'purple') \n",
    "    plt.xlabel('pion vetex cut')\n",
    "    plt.ylabel('Efficiency ') \n",
    "    plt.title('Graph of Efficiency  as pion vertex cuts')\n",
    "    #print(len(pi_LamT))\n",
    "    #plt.figure()\n",
    "    #plt.title('cut'+str( j/2))  \n",
    "    #plt.hist(imass, bins = 25)\n",
    "plt.figure(0)\n",
    "plt.savefig('extralongg1.png') \n",
    "plt.figure(1)\n",
    "plt.savefig('extralongg2.png') \n",
    "plt.figure(2)\n",
    "plt.savefig('extralongg3.png')\n",
    "plt.figure(3)\n",
    "plt.savefig('extralongg4.png') \n",
    "plt.figure(4)\n",
    "plt.savefig('extralongg5.png') \n",
    "plt.figure(5)\n",
    "plt.savefig('extralongg6.png')   \n",
    "np.save('s_to_b_valuesHnrec2Qextralongg1.npy', s_to_b_values)\n",
    "np.save('purity_valuesHnrec2Qextralongg1.npy', purity_values) \n",
    "np.save('efficiency_valuesHnrec2Qextralongg1.npy', efficiency_values)'''    \n",
    "\n",
    "f = open(\"/hpc/group/vossenlab/kam264/dir_name_2/batch_filer.py\", \"w\") \n",
    "f.write(code)             \n",
    "f.close()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60bade44-660a-457d-ab15-d6b3f3e18bae",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-d6028de1621e>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-d6028de1621e>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    To do\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "To do\n",
    "\n",
    "run same thing as above, but for longer say 80 epochs. \n",
    "same as bove, but change a few of the graphs. plot2,3 for based on rec or pion vertex instead of true\n",
    " perhaps some momentum stuff. \n",
    "histogram binned on momentum  \n",
    "no invarient mass\n",
    " maybe ture the thinggs.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587c79df-5836-4c01-8277-fed124bede15",
   "metadata": {},
   "outputs": [],
   "source": [
    "longer one. 15129533, 68 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b813f2f1-c27d-464a-a7f6-48121c0236d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "14860834 \n",
    "\n",
    "shorter run, 14904687 \n",
    "plt.figure(0)\n",
    "plt.savefig('LHnrec1QL.png') \n",
    "plt.figure(1)\n",
    "plt.savefig('LHnrec2QL.png') \n",
    "plt.figure(2)\n",
    "plt.savefig('LHnrec3QL.png')\n",
    "plt.figure(3)\n",
    "plt.savefig('LHnrec4QL.png') \n",
    "plt.figure(4)\n",
    "plt.savefig('LHnrec5QL.png')\n",
    "plt.figure(5)\n",
    "plt.savefig('LHnrec6QL.png')   \n",
    "np.save('s_to_b_valuesHnrec2Q.npy', s_to_b_values)\n",
    "np.save('purity_valuesHnrec2Q.npy', purity_values) \n",
    "np.save('efficiency_valuesHnrec2Q.npy', efficiency_values)\n",
    "for j in range(0, 20):\n",
    "    imass = []\n",
    "    print('cut ',j/2)\n",
    "    for i in range(len(pi_x)):\n",
    "        if pi_x[i]>j/2:\n",
    "            imass.append(pi_mas[i])\n",
    "    plt.figure(j+5)\n",
    "    plt.title('cut'+str( j/2)) \n",
    "    plt.hist(imass, bins = 25)\n",
    "    plt.savefig('cut_QL'+str(j/2)+'.png')\n",
    "    \n",
    "logner run, 14905492 \n",
    "plt.savefig('LHnrec1QLl.png') \n",
    "plt.figure(1)\n",
    "plt.savefig('LHnrec2QLl.png') \n",
    "plt.figure(2)\n",
    "plt.savefig('LHnrec3QLl.png')\n",
    "plt.figure(3)\n",
    "plt.savefig('LHnrec4QLl.png') \n",
    "plt.figure(4)\n",
    "plt.savefig('LHnrec5QLl.png')\n",
    "plt.figure(5)\n",
    "plt.savefig('LHnrec6QL.png')   \n",
    "np.save('s_to_b_valuesHnrec2Q.npy', s_to_b_values)\n",
    "np.save('purity_valuesHnrec2Q.npy', purity_values) \n",
    "np.save('efficiency_valuesHnrec2Q.npy', efficiency_values)\n",
    "for j in range(0, 20):\n",
    "    imass = []\n",
    "    print('cut ',j/2)\n",
    "    for i in range(len(pi_x)):\n",
    "        if pi_x[i]>j/2:\n",
    "            imass.append(pi_mas[i])\n",
    "    plt.figure(j+5)\n",
    "    plt.title('cut'+str( j/2)) \n",
    "    plt.hist(imass, bins = 25)\n",
    "    plt.savefig('cut_QLl'+str(j/2)+'.png') \n",
    "\n",
    "logner one 21 epochs. 14970276 LHnrec1QLll.png'\n",
    "14971034 longer one 30 epch. LHnrec1QLllL.png'\n",
    "cut_QLHH'+str(j/2)+'.png') 14971172, 21 epch, evaluate on half epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a29d72e-35b6-40ab-bbda-7c50fd54cca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "15067910 \n",
    "plt.figure(0)\n",
    "plt.savefig('extralong1.png') \n",
    "plt.figure(1)\n",
    "plt.savefig('extralong2.png') \n",
    "plt.figure(2)\n",
    "plt.savefig('extralong3.png')\n",
    "plt.figure(3)\n",
    "plt.savefig('extralong4.png') \n",
    "plt.figure(4)\n",
    "plt.savefig('extralong5.png')\n",
    "plt.figure(5)\n",
    "plt.savefig('extralong6.png')   \n",
    "np.save('s_to_b_valuesHnrec2Qextralong1.npy', s_to_b_values)\n",
    "np.save('purity_valuesHnrec2Qextralong1.npy', purity_values) \n",
    "np.save('efficiency_valuesHnrec2Qextralong1.npy', efficiency_values)\n",
    "for j in range(0, 20):\n",
    "    imass = []\n",
    "    print('cut ',j/2)\n",
    "    for i in range(len(pi_x)):\n",
    "        if pi_x[i]>j/2:\n",
    "            imass.append(pi_mas[i])\n",
    "    plt.figure(j+5)\n",
    "    plt.title('cut'+str( j/2)) \n",
    "    plt.hist(imass, bins = 25)\n",
    "    plt.savefig('cut_extralong'+str(j/2)+'.png')\n",
    "    \n",
    "for j in range(0, 20):\n",
    "    imass = []\n",
    "    print('cut ',j/2)\n",
    "    for i in range(len(pi_x)):\n",
    "        if pi_x[i]>j/2:\n",
    "            imass.append(pi_mas[i])\n",
    "    plt.figure(j+5)\n",
    "    plt.title('cut'+str( j/2)) \n",
    "    plt.hist(imass, bins = 25) \n",
    "    plt.savefig('cut_extralong'+str(j/2)+'.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d1d97c-8db4-4c82-bd68-6416b04066a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacfdd76-1ca5-4ba9-ba30-73715a75d134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56db57ff-138f-4f3b-b446-d5e77fb53a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = '''import os  \n",
    "import numpy as np\n",
    "import numpy.ma as ma    \n",
    "#import awkward as ak \n",
    "from tqdm import tqdm \n",
    "import torch \n",
    "import torch_geometric as tg  \n",
    "import torch_geometric \n",
    "from torch_geometric.data import Data \n",
    "#import torch \n",
    "from torch_geometric.data import InMemoryDataset, download_url  \n",
    "import torch_geometric.transforms as T \n",
    "\n",
    "#NOTE: NEW 2/20/23      \n",
    "from typing import List, Union     \n",
    "\n",
    "from torch_geometric.data import Data, HeteroData \n",
    "from torch_geometric.data.datapipes import functional_transform\n",
    "from torch_geometric.transforms import BaseTransform       \n",
    "torch.cuda.empty_cache()  \n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn.norm import GraphNorm, BatchNorm \n",
    "from torch.utils.data import random_split \n",
    "from torch_geometric.loader import DataLoader \n",
    "\n",
    "class MyOwnDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.load(self.processed_paths[0])\n",
    "        # For PyG<2.4:\n",
    "        # self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['some_file_1', 'some_file_2']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "    def process(self):\n",
    "        # Read data into huge `Data` list.\n",
    "        data_list = None\n",
    "\n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    "\n",
    "        self.save(data_list, self.processed_paths[0]) \n",
    "\n",
    "#root = '/hpc/group/vossenlab/kam264/L_imass3'\n",
    "#root = '/hpc/group/vossenlab/kam264/onlylambda_25k' \n",
    "#root = '/hpc/group/vossenlab/kam264/Lambda_13_16'\n",
    "#root = '/hpc/group/vossenlab/kam264/Lambda_1all' \n",
    "\n",
    "batch_size = 16 \n",
    "LR =1e-2\n",
    "torch.cuda.empty_cache()   \n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels): \n",
    "        super(GCN, self).__init__()\n",
    "#         torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)#.jittable() #NOTE: NEEDED FOR DEPLOYMENT IN CMAKE\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)#.jittable()\n",
    "        #self.block2 = nn.DataParallel(self.block2)\n",
    "        #self.conv2 = torch.nn.DataParallel(self.conv2) #this was trying the parallization thing. \n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)#.jittable()\n",
    "        #self.conv3 = torch.nn.DataParallel(self.conv3)\n",
    "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin3 = Linear(hidden_channels, out_channels)\n",
    "        self.bn1 = torch_geometric.nn.norm.GraphNorm(hidden_channels)\n",
    "        self.bn2 = torch_geometric.nn.norm.GraphNorm(hidden_channels)\n",
    "        self.bn3 = torch_geometric.nn.norm.GraphNorm(hidden_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):  \n",
    "        \n",
    "        x = self.conv1(x, edge_index) #input layer                             \n",
    "                                                      \n",
    "        x = self.bn1(x) #normalize it                                          \n",
    "\n",
    "        x = x.relu() #activation                                               \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = x.relu()\n",
    "#         print(\"x.relu() = \",x)  \n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = self.bn3(x)\n",
    "#         # 2. Readout layer                                                   \n",
    "        x = global_mean_pool(x, batch)\n",
    "        # 3. Apply a final classifier                                          \n",
    "        x = F.dropout(x, p=0.5, training=self.training) #for overfittin        \n",
    "        x = self.lin3(x) \n",
    " \n",
    "        return x\n",
    "def RMSELoss(out,y):\n",
    "    return torch.sqrt(torch.mean((out-y)**2))\n",
    "def train():\n",
    "    model.train() #initailize the model                                                                                                                                                                                                        \n",
    "    #for i, data in tqdm(enumerate(train_loader)): #perhaps tqdm(enumerate(train_loader)), i is index, data jsut moves through all the dtaa in trainingg                                                                                      \n",
    "    for i,data in enumerate(train_loader):\n",
    "        data = data.to(device) #switch to GPU                                                                                                                                                                                                 \n",
    "        optimizer.zero_grad() #                                                                                                                                                                                                               \n",
    "        out = model(data.x, data.edge_index, data.batch).to(device)  # Perform a single forward pass                                                                                                                                          \n",
    "        yy = []\n",
    "        for j in range(0,len(out)):\n",
    "            yy+= [[data.y[j].item()]]\n",
    "\n",
    "        yy = torch.tensor(yy).to(device) \n",
    "        #print(out)\n",
    "        #print(yy)\n",
    "        loss = losss(out, yy).to(device) #compute the loss  \n",
    "        #print(loss)\n",
    "        loss.backward() #get the gradients.                                                                                                                                                                                                   \n",
    "        optimizer.step() \n",
    "def test(loader): \n",
    "    length = len(loader.dataset)\n",
    "    model.eval() #evaluate teh model.                                                                                                                                                                                                         \n",
    "\n",
    "    #mse_tot = []                                                                                                                                                                                                                             \n",
    "    mse_total = 0\n",
    "    mse_pi = 0\n",
    "    mse_p = 0\n",
    "    #r                                                                                                                                                                                                                                        \n",
    "    #for data in tqdm(loader):  # Iterate in batches over the training/test dataset.                                                                                                                                                          \n",
    "    for data in loader:\n",
    "        data = data.to(device) #put to GPU                                                                                                                                                                                                    \n",
    "        out = model(data.x, data.edge_index, data.batch).to(device) #evalueate                                                                                                                                                                \n",
    "        #this and the for loop is converting data.y to a tensor in the same shape as out rows and 2 columns first is y_pion second is y_proton                                                                                                \n",
    "        yy = []\n",
    "        for j in range(0,len(out)):\n",
    "            yy+= [[data.y[j].item()]]\n",
    "        yy = torch.tensor(yy).to(device) \n",
    "        loss = losss(out, yy).cpu() #getting teh loss function                                                                                                                                                                                \n",
    "        mse_total+=loss.item() #getting the mse (total)                                                                                                                                                                                       \n",
    "\n",
    "    return mse_total/length \n",
    "def print_out():\n",
    "    model.eval() #initailize the model                                                                                                                                                                                                        \n",
    "    #for i, data in tqdm(enumerate(train_loader)): #perhaps tqdm(enumerate(train_loader)), i is index, data jsut moves through all the dtaa in trainingg                                                                                      \n",
    "    outt= []\n",
    "    for i,data in enumerate(test_loader):\n",
    "        data = data.to(device) #switch to GPU                                                                                                                                                                                                 \n",
    "        optimizer.zero_grad() #                                                                                                                                                                                                               \n",
    "        out = model(data.x, data.edge_index, data.batch).to(device)  # Perform a single forward pass                                                                                                                                          \n",
    "        out = out.cpu()\n",
    "\n",
    "        outt+=[[out.detach().numpy()]]\n",
    "    return outt\n",
    "\n",
    "def print_outb():\n",
    "    model.eval() #initailize the model                                                                                                                                                                                                        \n",
    "    #for i, data in tqdm(enumerate(train_loader)): #perhaps tqdm(enumerate(train_loader)), i is index, data jsut moves through all the dtaa in trainingg      \n",
    "\n",
    "    outt= [] \n",
    "    #for i,data in enumerate(dataset):\n",
    "    for i,data in enumerate(test_loader):\n",
    "        data = data.to(device) #switch to GPU                                                                                                                                                                                                 \n",
    "        optimizer.zero_grad() #                                                                                                                                                                                                               \n",
    "        out = model(data.x, data.edge_index, data.batch).to(device)  # Perform a single forward pass                                                                                                                                          \n",
    "        out = out.cpu()\n",
    "\n",
    "        neww = []\n",
    "        for j in range(len(data)):\n",
    "            neww.append(out[j].item() -data.elc[j][0]) \n",
    "        outt+=[[neww]]\n",
    "    return outt  \n",
    "#model = GCN(dataset.num_node_features,64,2)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') \n",
    "#devicee = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu') \n",
    "print(\"Device = \",device) \n",
    "\n",
    "print(\"DEBUGGING: torch.cuda.is_available() = \",torch.cuda.is_available())\n",
    "train_metrics = {'mse':[], \"rmse\":[] } \n",
    "vall_metrics = {'mse':[], \"rmse\":[] }\n",
    "\n",
    "for iii in range(4):   \n",
    "    if iii>0: \n",
    "        #del dataset\n",
    "        del train_loader\n",
    "        del val_loader \n",
    "    if iii==0:\n",
    "        root = '/hpc/group/vossenlab/kam264/HalfL_first'\n",
    "    if iii==1:\n",
    "        root = '/hpc/group/vossenlab/kam264/HalfL_second'\n",
    "    if iii==2:\n",
    "        root = '/hpc/group/vossenlab/kam264/HalfL_third'\n",
    "    if iii==3:\n",
    "        root = '/hpc/group/vossenlab/kam264/HalfL_fourth'   \n",
    "    dataset = MyOwnDataset(\n",
    "            root,\n",
    "            transform=None, #T.Compose([T.ToUndirected(),T.KNNGraph(k=6)]),\n",
    "            pre_transform=None,\n",
    "            pre_filter=None\n",
    "        ) \n",
    "    dataset\n",
    "    model = GCN(dataset.num_node_features,64,2)\n",
    "    fracs = [0.9, 0.08, 0.02] #percent of dataset used for training testing and validatoin 80%,10%,10% #NOTE: SHOULD CHECK np.sum(fracs) == 1 and len(fracs)==3\n",
    "    fracs = [torch.sum(torch.tensor(fracs[:idx])) for idx in range(1,len(fracs)+1)] #get the indexes for training ... parts to use. \n",
    "    #print(fracs)\n",
    "    split1, split2 = [int(len(dataset)*frac) for frac in fracs[:-1]] \n",
    "    train_dataset = dataset[:split1]\n",
    "    val_dataset = dataset[split1:split2]\n",
    "    test_dataset = dataset[split2:]   \n",
    "     \n",
    "    print(f'Number of training graphs: {len(train_dataset)}')\n",
    "    print(f'Number of validation graphs: {len(val_dataset)}') \n",
    "\n",
    "    from torch_geometric.loader import DataLoader \n",
    "    #from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=8, shuffle=True)#, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset,  batch_size=batch_size,num_workers=8,  shuffle=True)\n",
    "    model = GCN(dataset.num_node_features, dataset.num_classes, 1).to(device) #initiate the model, #2 is the number of outputs here is 2 as pion_z, proton_z \n",
    "    model = model.to(device)  \n",
    "    del dataset\n",
    "    del train_dataset\n",
    "    del val_dataset\n",
    "    del test_dataset \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr= LR) \n",
    "    \n",
    "    losss = RMSELoss  \n",
    "    nepochs =  21 \n",
    "    \n",
    "    for epoch in range(nepochs):  \n",
    "  \n",
    "        #print(\"BEFORE TRAIN()\")                                                                                                                                                                                                                  \n",
    "        train() \n",
    "        #print(\"BEFORE TEST(TRAIN_LOADER)\")                                                                                                                                                                                                       \n",
    "        #train_mse, train_rmse, train_mse_pi, train_rmse_pi, train_mse_p, train_rmse_p = test(train_loader)\n",
    "        train_mse = test(train_loader)\n",
    "\n",
    "        train_metrics['mse'].append(train_mse) \n",
    "\n",
    "        vall_mse =test(val_loader) \n",
    "\n",
    "\n",
    "        vall_metrics['mse'].append(vall_mse) \n",
    "\n",
    "        if epoch%9==0:\n",
    "            print(\"Epoch \",epoch,\" Train mse: \",train_mse)\n",
    "            print(\"Epoch \",epoch,\" Validation mse: \",vall_mse)\n",
    "        if epoch==(nepochs-1):\n",
    "            #PATH = '/work/clas12/users/mfmce/CLAS12_Lambda_resolution_REU_2023/model_best_auc.pt' \n",
    "\n",
    "            #a = print_out()\n",
    "            #b=  print_outb() \n",
    "            print(\"Epoch \",epoch,\" Train mse: \",train_mse)\n",
    "            print(\"Epoch \",epoch,\" Validation mse: \",vall_mse)\n",
    "\n",
    "root = '/hpc/group/vossenlab/kam264/HalfL_fifth'     \n",
    "dataset = MyOwnDataset(\n",
    "        root,\n",
    "        transform=None, #T.Compose([T.ToUndirected(),T.KNNGraph(k=6)]),\n",
    "        pre_transform=None,\n",
    "        pre_filter=None\n",
    "    )    \n",
    "\n",
    "model.eval() #initailize the model                                                                                                                                                                                                        \n",
    "#for i, data in tqdm(enumerate(train_loader)): #perhaps tqdm(enumerate(train_loader)), i is index, data jsut moves through all the dtaa in trainingg      \n",
    "\n",
    "outt= [] \n",
    "#for i,data in enumerate(dataset): \n",
    "for i,data in enumerate(dataset):\n",
    "    data = data.to(device) #switch to GPU                                                                                                                                                                                                 \n",
    "    #optimizer.zero_grad() #                                                                                                                                                                                                               \n",
    "    out = model(data.x, data.edge_index, data.batch).to(device)  # Perform a single forward pass                                                                                                                                          \n",
    "    out = out.cpu()\n",
    "    #print(out)\n",
    "    #print(data.elc) \n",
    "\n",
    "   # neww = []\n",
    "    #for j in range(len(data)):\n",
    "        #neww.append(out[j].item() -data.elc[j][0])\n",
    "     #   neww.append(out[j].item() -data.elc[j])\n",
    "    #outt+=[[neww]]\n",
    "    outt+=[[out[0][0].item()-data.elc[0]]] \n",
    "#return outt  \n",
    "#b= print_outbb() \n",
    "b=outt \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "pi_x = np.zeros(len(outt))\n",
    "for i in range(len(outt)):\n",
    "    pi_x[i] = outt[i][0]\n",
    "pi_yy = [] \n",
    "#p_yy = [] \n",
    "#pi_x = outt\n",
    "pi_rec_y = []\n",
    "#p_rec_y = []\n",
    "for i, data in enumerate(dataset):\n",
    "    data = data#.to(device) #put to GPU\n",
    "    #break\n",
    "    jj= 0\n",
    "    for j in range(0,int(len(data.y))): \n",
    "        pi_yy.append(data.y[j].item() -data.elc[j] ) \n",
    "      \n",
    "    for j in range(len(data.rec)):\n",
    "        pi_rec_y.append(data.rec[j]-data.elc[j])\n",
    "       # p_rec_y.append(data.rec[j][0][1])\n",
    "\n",
    "#plott_pi = np.zeros((len(pi_x),4))\n",
    "#plott_p = np.zeros((len(p_x),2)) \n",
    "plott_pi = np.zeros((len(pi_x),3))\n",
    "for i in range(len(pi_x)):\n",
    "#for i in range(982):\n",
    "    plott_pi[i][0] = pi_yy[i]\n",
    "    plott_pi[i][1] = pi_x[i]# [0]\n",
    "    #plott_pi[i][3] = pi_val[i]\n",
    "   # plott_pi[i][2] = pi_rec_y[i] \n",
    "   # plott_p[i][0] = p_yy[i]\n",
    "   # plott_p[i][1] = p_x[i]\n",
    "    #plott_p[i][2]  = p_rec_y[i] \n",
    "for i in range(len(pi_x)): \n",
    "    plott_pi[i][2] = pi_rec_y[i] \n",
    "#plott_pi \n",
    "plt.figure(0) ##################\n",
    "plt.title('location of Pion vertex') \n",
    "#plt.axvline(x = np.mean(pi_x), color = 'blue') \n",
    "#plt.axvline(x = np.mean(pi_yy), color = 'orange') \n",
    "plt.hist(x = plott_pi, histtype ='step', color = ['orange', 'blue', 'green'], bins = 500 ) \n",
    "#plt.hist(x = plott_pi, histtype ='step', bins = 500 )\n",
    "#plt.legend(['Predicted', 'True', 'REC']) \n",
    "#plt.legend(['True', 'Predicted', 'REC']) \n",
    "plt.xlim((-10,15)) \n",
    "plt.ylabel('count') \n",
    "plt.xlabel('Location of Pion vertex (cm)') \n",
    " #############################################################\n",
    "\n",
    "plt.figure(1) #######################\n",
    "pi_yy = np.array(pi_yy)\n",
    "pi_x = np.array(pi_x)\n",
    "pi_rec_y = np.array(pi_rec_y)#np.logical_and\n",
    "indices_two = np.where(np.logical_and(pi_yy < -1.5, pi_yy > -2) ) \n",
    "#indices_two = np.where(pi_yy <-1.5 and pi_yy>-2 )\n",
    "#print(indices_two)\n",
    "#print(pi_x[indices_two])  \n",
    "\n",
    "diff_x = [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
    "diff_rec  = [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
    "for j in range(1,24):\n",
    "    indices = np.where(np.logical_and(pi_yy < -2+j/2, pi_yy > -2+j/2-.5) ) \n",
    "    dif_x =[ abs(pi_x[indices] - pi_yy[indices])]\n",
    "    dif_rec = [abs(pi_rec_y[indices] - pi_yy[indices])]\n",
    "    diff_x[j-1].append(dif_x)\n",
    "    diff_rec[j-1].append(dif_rec)\n",
    "mean_x = np.zeros(24)\n",
    "mean_rec = np.zeros(24)\n",
    "error_x = np.zeros(24)\n",
    "error_rec = np.zeros(24)\n",
    "for i in range(24): \n",
    "    mean_x[i] = np.nanmean(diff_x[i])\n",
    "    mean_rec[i] = np.nanmean(diff_rec[i]) \n",
    "    error_x[i] = np.nanstd(diff_x[i])\n",
    "    error_rec[i] = np.nanstd(diff_rec[i])\n",
    "xxx = np.linspace(-2,10,24)\n",
    "plt.scatter(xxx, mean_x)\n",
    "plt.scatter(xxx, mean_rec, alpha = .5 )  \n",
    "plt.errorbar(xxx, mean_x,  yerr=error_x, fmt=\"o\")   \n",
    "plt.errorbar(xxx, mean_rec,  yerr=error_rec, fmt=\"o\", alpha = .5)\n",
    "plt.title('absolute error based on pion vertex, orange is rec, blue is GNN')\n",
    "plt.xlabel('Pion vertex (cm)') \n",
    "plt.ylabel('abolute error from True (cm)')\n",
    " #############################################################\n",
    "plt.figure(2) #######################\n",
    "diff_x = [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
    "diff_rec  = [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
    "for j in range(1,24):\n",
    "    indices = np.where(np.logical_and(pi_yy < -2+j/2, pi_yy > -2+j/2-.5) ) \n",
    "    dif_x =[ (pi_x[indices] - pi_yy[indices])]\n",
    "    dif_rec = [(pi_rec_y[indices] - pi_yy[indices])]\n",
    "    diff_x[j-1].append(dif_x)\n",
    "    diff_rec[j-1].append(dif_rec)\n",
    "mean_x = np.zeros(24)\n",
    "mean_rec = np.zeros(24)\n",
    "error_x = np.zeros(24)\n",
    "error_rec = np.zeros(24)\n",
    "for i in range(24): \n",
    "    mean_x[i] = np.nanmean(diff_x[i])\n",
    "    mean_rec[i] = np.nanmean(diff_rec[i]) \n",
    "    error_x[i] = np.nanstd(diff_x[i])\n",
    "    error_rec[i] = np.nanstd(diff_rec[i])\n",
    "xxx = np.linspace(-2,10,24)\n",
    "plt.scatter(xxx, mean_x)\n",
    "plt.scatter(xxx, mean_rec, alpha = .5 )  \n",
    "plt.errorbar(xxx, mean_x,  yerr=error_x, fmt=\"o\")   \n",
    "plt.errorbar(xxx, mean_rec,  yerr=error_rec, fmt=\"o\", alpha = .5)\n",
    "plt.title(' error based on pion vertex, orange is rec, blue is GNN')\n",
    "plt.xlabel('Pion vertex (cm)') \n",
    "plt.ylabel(' error from True (cm)')\n",
    " #############################################################\n",
    "\n",
    "pi_mas = []\n",
    "pi_LamT = []\n",
    "#p_rec_y = []\n",
    "for i, data in enumerate(dataset):\n",
    "    data = data#.to(device) #put to GPU\n",
    "    #break\n",
    "    #print(data.LamT)\n",
    "    for j in range(0,int(len(data.imass))): \n",
    "        pi_mas.append(data.imass[j]) \n",
    "        pi_LamT.append(data.LamT[j]) \n",
    "Total_signal = pi_LamT.count(1)\n",
    "print('S/B')\n",
    "print('Purity') \n",
    "print('Efficienty')\n",
    "s_to_b_values = []\n",
    "purity_values = []\n",
    "efficiency_values = []\n",
    "for j in range(0, 20):\n",
    "    pi_LamTt = []\n",
    "    #imass\n",
    "    print('cut ',j/2)\n",
    "    for i in range(len(pi_x)): \n",
    "        if pi_x[i]>j/2:\n",
    "            #imass.append(pi_mas[i])\n",
    "            pi_LamTt.append(pi_LamT[i])\n",
    "    print(pi_LamTt.count(1))\n",
    "    #print(pi_LamT.count(0))\n",
    "    print(pi_LamTt.count(1)/pi_LamTt.count(0))\n",
    "    print(pi_LamTt.count(1)/(len(pi_LamTt)))\n",
    "    print(pi_LamTt.count(1)/(Total_signal))\n",
    "    #print(pi_LamTt.count(1)/len(pi_LamTt))\n",
    "    s_to_b = pi_LamTt.count(1)/pi_LamTt.count(0)\n",
    "    purity = pi_LamTt.count(1)/(len(pi_LamTt))\n",
    "    efficiency = pi_LamTt.count(1)/(Total_signal)\n",
    "    s_to_b_values.append(s_to_b)\n",
    "    purity_values.append(purity)\n",
    "    efficiency_values.append(efficiency)\n",
    "    print()\n",
    "    plt.figure(3)\n",
    "    plt.scatter(j/2, pi_LamTt.count(1)/pi_LamTt.count(0), color = 'blue')\n",
    "    plt.xlabel('pion vetex cut')\n",
    "    plt.ylabel('S/B') \n",
    "    plt.title('Graph of S/B as pion vertex cuts')\n",
    "    \n",
    "    plt.figure(4)\n",
    "    plt.scatter(j/2, pi_LamTt.count(1)/(len(pi_LamTt)), color = 'red')\n",
    "    plt.xlabel('pion vetex cut')\n",
    "    plt.ylabel('Purity') \n",
    "    plt.title('Graph of Purity as pion vertex cuts')\n",
    "                \n",
    "    plt.figure(5)\n",
    "    plt.scatter(j/2, pi_LamTt.count(1)/(Total_signal), color = 'purple') \n",
    "    plt.xlabel('pion vetex cut')\n",
    "    plt.ylabel('Efficiency ') \n",
    "    plt.title('Graph of Efficiency  as pion vertex cuts')\n",
    "    #print(len(pi_LamT))\n",
    "    #plt.figure()\n",
    "    #plt.title('cut'+str( j/2))  \n",
    "    #plt.hist(imass, bins = 25)\n",
    "plt.figure(0)\n",
    "plt.savefig('Long1.png') \n",
    "plt.figure(1)\n",
    "plt.savefig('Long2.png') \n",
    "plt.figure(2)\n",
    "plt.savefig('Long3.png')\n",
    "plt.figure(3)\n",
    "plt.savefig('Long4.png') \n",
    "plt.figure(4)\n",
    "plt.savefig('Long5.png')\n",
    "plt.figure(5)\n",
    "plt.savefig('Long5.png')   \n",
    "np.save('long1.npy', s_to_b_values)\n",
    "np.save('long2.npy', purity_values) \n",
    "np.save('long3.npy', efficiency_values)'''    \n",
    "\n",
    "f = open(\"/hpc/group/vossenlab/kam264/dir_name_2/batch_filer.py\", \"w\") \n",
    "f.write(code)              \n",
    "f.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1135b7-ff52-4b73-b1c0-f74793dfe640",
   "metadata": {},
   "outputs": [],
   "source": [
    "longer, one. noraml, half L. 14311050 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b2932b-c13b-4407-ba21-ca4539ad9c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
