{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fa5ff32-0fc4-4d68-ae54-cfa676a5482f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basecode what I showed on Monday.  , go to end for new code. \n",
    "code = '''import os    \n",
    "import numpy as np\n",
    "import numpy.ma as ma   \n",
    "#import awkward as ak \n",
    "from tqdm import tqdm \n",
    "import torch \n",
    "import torch_geometric as tg  \n",
    "import torch_geometric \n",
    "from torch_geometric.data import Data \n",
    "#import torch \n",
    "from torch_geometric.data import InMemoryDataset, download_url  \n",
    "import torch_geometric.transforms as T \n",
    "\n",
    "#NOTE: NEW 2/20/23      \n",
    "from typing import List, Union     \n",
    "\n",
    "from torch_geometric.data import Data, HeteroData \n",
    "from torch_geometric.data.datapipes import functional_transform\n",
    "from torch_geometric.transforms import BaseTransform       \n",
    "torch.cuda.empty_cache()  \n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn.norm import GraphNorm, BatchNorm \n",
    "from torch.utils.data import random_split \n",
    "from torch_geometric.loader import DataLoader \n",
    "\n",
    "class MyOwnDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.load(self.processed_paths[0])\n",
    "        # For PyG<2.4:\n",
    "        # self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['some_file_1', 'some_file_2']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "    def process(self):\n",
    "        # Read data into huge `Data` list.\n",
    "        data_list = None\n",
    "\n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    "\n",
    "        self.save(data_list, self.processed_paths[0]) \n",
    "\n",
    "#root = '/hpc/group/vossenlab/kam264/L_imass3'\n",
    "#root = '/hpc/group/vossenlab/kam264/onlylambda_25k' \n",
    "#root = '/hpc/group/vossenlab/kam264/Lambda_13_16'\n",
    "#root = '/hpc/group/vossenlab/kam264/Lambda_1all' \n",
    "\n",
    "batch_size = 16 \n",
    "LR =1e-2\n",
    "torch.cuda.empty_cache()   \n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels): \n",
    "        super(GCN, self).__init__()\n",
    "#         torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)#.jittable() #NOTE: NEEDED FOR DEPLOYMENT IN CMAKE\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)#.jittable()\n",
    "        #self.block2 = nn.DataParallel(self.block2)\n",
    "        #self.conv2 = torch.nn.DataParallel(self.conv2) #this was trying the parallization thing. \n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)#.jittable()\n",
    "        #self.conv3 = torch.nn.DataParallel(self.conv3)\n",
    "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin3 = Linear(hidden_channels, out_channels)\n",
    "        self.bn1 = torch_geometric.nn.norm.GraphNorm(hidden_channels)\n",
    "        self.bn2 = torch_geometric.nn.norm.GraphNorm(hidden_channels)\n",
    "        self.bn3 = torch_geometric.nn.norm.GraphNorm(hidden_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):  \n",
    "        \n",
    "        x = self.conv1(x, edge_index) #input layer                             \n",
    "                                                      \n",
    "        x = self.bn1(x) #normalize it                                          \n",
    "\n",
    "        x = x.relu() #activation                                               \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = x.relu()\n",
    "#         print(\"x.relu() = \",x)  \n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = self.bn3(x)\n",
    "#         # 2. Readout layer                                                   \n",
    "        x = global_mean_pool(x, batch)\n",
    "        # 3. Apply a final classifier                                          \n",
    "        x = F.dropout(x, p=0.5, training=self.training) #for overfittin        \n",
    "        x = self.lin3(x) \n",
    " \n",
    "        return x\n",
    "def RMSELoss(out,y):\n",
    "    return torch.sqrt(torch.mean((out-y)**2))\n",
    "def train():\n",
    "    model.train() #initailize the model                                                                                                                                                                                                        \n",
    "    #for i, data in tqdm(enumerate(train_loader)): #perhaps tqdm(enumerate(train_loader)), i is index, data jsut moves through all the dtaa in trainingg                                                                                      \n",
    "    for i,data in enumerate(train_loader):\n",
    "        data = data.to(device) #switch to GPU                                                                                                                                                                                                 \n",
    "        optimizer.zero_grad() #                                                                                                                                                                                                               \n",
    "        out = model(data.x, data.edge_index, data.batch).to(device)  # Perform a single forward pass                                                                                                                                          \n",
    "        yy = []\n",
    "        for j in range(0,len(out)):\n",
    "            yy+= [[data.y[j].item()]]\n",
    "\n",
    "        yy = torch.tensor(yy).to(device) \n",
    "        #print(out)\n",
    "        #print(yy)\n",
    "        loss = losss(out, yy).to(device) #compute the loss  \n",
    "        #print(loss)\n",
    "        loss.backward() #get the gradients.                                                                                                                                                                                                   \n",
    "        optimizer.step() \n",
    "def test(loader): \n",
    "    length = len(loader.dataset)\n",
    "    model.eval() #evaluate teh model.                                                                                                                                                                                                         \n",
    "\n",
    "    #mse_tot = []                                                                                                                                                                                                                             \n",
    "    mse_total = 0\n",
    "    mse_pi = 0\n",
    "    mse_p = 0\n",
    "    #r                                                                                                                                                                                                                                        \n",
    "    #for data in tqdm(loader):  # Iterate in batches over the training/test dataset.                                                                                                                                                          \n",
    "    for data in loader:\n",
    "        data = data.to(device) #put to GPU                                                                                                                                                                                                    \n",
    "        out = model(data.x, data.edge_index, data.batch).to(device) #evalueate                                                                                                                                                                \n",
    "        #this and the for loop is converting data.y to a tensor in the same shape as out rows and 2 columns first is y_pion second is y_proton                                                                                                \n",
    "        yy = []\n",
    "        for j in range(0,len(out)):\n",
    "            yy+= [[data.y[j].item()]]\n",
    "        yy = torch.tensor(yy).to(device) \n",
    "        loss = losss(out, yy).cpu() #getting teh loss function                                                                                                                                                                                \n",
    "        mse_total+=loss.item() #getting the mse (total)                                                                                                                                                                                       \n",
    "\n",
    "    return mse_total/length \n",
    "def print_out():\n",
    "    model.eval() #initailize the model                                                                                                                                                                                                        \n",
    "    #for i, data in tqdm(enumerate(train_loader)): #perhaps tqdm(enumerate(train_loader)), i is index, data jsut moves through all the dtaa in trainingg                                                                                      \n",
    "    outt= []\n",
    "    for i,data in enumerate(test_loader):\n",
    "        data = data.to(device) #switch to GPU                                                                                                                                                                                                 \n",
    "        optimizer.zero_grad() #                                                                                                                                                                                                               \n",
    "        out = model(data.x, data.edge_index, data.batch).to(device)  # Perform a single forward pass                                                                                                                                          \n",
    "        out = out.cpu()\n",
    "\n",
    "        outt+=[[out.detach().numpy()]]\n",
    "    return outt\n",
    "\n",
    "def print_outb():\n",
    "    model.eval() #initailize the model                                                                                                                                                                                                        \n",
    "    #for i, data in tqdm(enumerate(train_loader)): #perhaps tqdm(enumerate(train_loader)), i is index, data jsut moves through all the dtaa in trainingg      \n",
    "\n",
    "    outt= [] \n",
    "    #for i,data in enumerate(dataset):\n",
    "    for i,data in enumerate(test_loader):\n",
    "        data = data.to(device) #switch to GPU                                                                                                                                                                                                 \n",
    "        optimizer.zero_grad() #                                                                                                                                                                                                               \n",
    "        out = model(data.x, data.edge_index, data.batch).to(device)  # Perform a single forward pass                                                                                                                                          \n",
    "        out = out.cpu()\n",
    "\n",
    "        neww = []\n",
    "        for j in range(len(data)):\n",
    "            neww.append(out[j].item() -data.elc[j][0]) \n",
    "        outt+=[[neww]]\n",
    "    return outt  \n",
    "#model = GCN(dataset.num_node_features,64,2)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') \n",
    "#devicee = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu') \n",
    "print(\"Device = \",device) \n",
    "\n",
    "print(\"DEBUGGING: torch.cuda.is_available() = \",torch.cuda.is_available())\n",
    "train_metrics = {'mse':[], \"rmse\":[] } \n",
    "vall_metrics = {'mse':[], \"rmse\":[] }\n",
    "\n",
    "for iii in range(1):    \n",
    "    if iii>0: \n",
    "        #del dataset\n",
    "        del train_loader  \n",
    "        del val_loader \n",
    "    if iii==0:\n",
    "        root = '/hpc/group/vossenlab/kam264/Qmom0'\n",
    "    if iii==1:\n",
    "        root = '/hpc/group/vossenlab/kam264/Qmom1'\n",
    "    if iii==2:\n",
    "        root = '/hpc/group/vossenlab/kam264/Qmom2'\n",
    "    if iii==3:\n",
    "        root = '/hpc/group/vossenlab/kam264/Qmom3'   \n",
    "    dataset = MyOwnDataset(\n",
    "            root,\n",
    "            transform=None, #T.Compose([T.ToUndirected(),T.KNNGraph(k=6)]),\n",
    "            pre_transform=None,\n",
    "            pre_filter=None\n",
    "        ) \n",
    "    dataset\n",
    "    model = GCN(dataset.num_node_features,64,2)\n",
    "    fracs = [0.9, 0.08, 0.02] #percent of dataset used for training testing and validatoin 80%,10%,10% #NOTE: SHOULD CHECK np.sum(fracs) == 1 and len(fracs)==3\n",
    "    fracs = [torch.sum(torch.tensor(fracs[:idx])) for idx in range(1,len(fracs)+1)] #get the indexes for training ... parts to use. \n",
    "    #print(fracs)\n",
    "    split1, split2 = [int(len(dataset)*frac) for frac in fracs[:-1]] \n",
    "    train_dataset = dataset[:split1]\n",
    "    val_dataset = dataset[split1:split2]\n",
    "    test_dataset = dataset[split2:]   \n",
    "     \n",
    "    print(f'Number of training graphs: {len(train_dataset)}')\n",
    "    print(f'Number of validation graphs: {len(val_dataset)}') \n",
    "\n",
    "    from torch_geometric.loader import DataLoader \n",
    "    #from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=8, shuffle=True)#, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset,  batch_size=batch_size,num_workers=8,  shuffle=True)\n",
    "    model = GCN(dataset.num_node_features, dataset.num_classes, 1).to(device) #initiate the model, #2 is the number of outputs here is 2 as pion_z, proton_z \n",
    "    model = model.to(device)  \n",
    "    del dataset\n",
    "    del train_dataset\n",
    "    del val_dataset\n",
    "    del test_dataset \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr= LR)  \n",
    "    \n",
    "    losss = RMSELoss  \n",
    "    nepochs = 800     \n",
    "    \n",
    "    for epoch in range(nepochs):  \n",
    "  \n",
    "        #print(\"BEFORE TRAIN()\")                                                                                                                                                                                                                  \n",
    "        train() \n",
    "        #print(\"BEFORE TEST(TRAIN_LOADER)\")                                                                                                                                                                                                       \n",
    "        #train_mse, train_rmse, train_mse_pi, train_rmse_pi, train_mse_p, train_rmse_p = test(train_loader)\n",
    "        train_mse = test(train_loader)\n",
    "\n",
    "        train_metrics['mse'].append(train_mse) \n",
    "\n",
    "        vall_mse =test(val_loader) \n",
    "\n",
    "\n",
    "        vall_metrics['mse'].append(vall_mse) \n",
    "\n",
    "        if epoch%9==0:\n",
    "            print(\"Epoch \",epoch,\" Train mse: \",train_mse)\n",
    "            print(\"Epoch \",epoch,\" Validation mse: \",vall_mse)\n",
    "        if epoch==(nepochs-1):\n",
    "            #PATH = '/work/clas12/users/mfmce/CLAS12_Lambda_resolution_REU_2023/model_best_auc.pt' \n",
    "\n",
    "            #a = print_out()\n",
    "            #b=  print_outb() \n",
    "            print(\"Epoch \",epoch,\" Train mse: \",train_mse)\n",
    "            print(\"Epoch \",epoch,\" Validation mse: \",vall_mse) \n",
    "\n",
    "root = '/hpc/group/vossenlab/kam264/Hmom4' \n",
    "#root = '/hpc/group/vossenlab/kam264/QuatorL_Last'\n",
    "dataset = MyOwnDataset(\n",
    "        root,\n",
    "        transform=None, #T.Compose([T.ToUndirected(),T.KNNGraph(k=6)]),\n",
    "        pre_transform=None,\n",
    "        pre_filter=None\n",
    "    )    \n",
    "\n",
    "model.eval() #initailize the model                                                                                                                                                                                                        \n",
    "#for i, data in tqdm(enumerate(train_loader)): #perhaps tqdm(enumerate(train_loader)), i is index, data jsut moves through all the dtaa in trainingg      \n",
    "\n",
    "outt= []  \n",
    "pi_mas = []\n",
    "#for i,data in enumerate(dataset): \n",
    "for i,data in enumerate(dataset):\n",
    "    data = data.to(device) #switch to GPU                                                                                                                                                                                                 \n",
    "    #optimizer.zero_grad() #                                                                                                                                                                                                               \n",
    "    out = model(data.x, data.edge_index, data.batch).to(device)  # Perform a single forward pass                                                                                                                                          \n",
    "    out = out.cpu()\n",
    "    #print(out) \n",
    "    #print(data.elc) \n",
    "\n",
    "   # neww = []\n",
    "    #for j in range(len(data)):\n",
    "        #neww.append(out[j].item() -data.elc[j][0])\n",
    "     #   neww.append(out[j].item() -data.elc[j])\n",
    "    #outt+=[[neww]]\n",
    "    #outt+=[[out[0][0].item()-data.elc[0]]] \n",
    "    outt+=[[out[0][0].item()-data.elc[0]]]\n",
    "    for j in range(0,int(len(data.imass))): \n",
    "        pi_mas.append(data.imass[j] ) \n",
    "#return outt  \n",
    "#b= print_outbb() \n",
    "b=outt \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "pi_x = np.zeros(len(outt))\n",
    "for i in range(len(outt)):\n",
    "    pi_x[i] = outt[i][0]\n",
    "pi_yy = [] \n",
    "#p_yy = [] \n",
    "#pi_x = outt\n",
    "pi_rec_y = []\n",
    "#p_rec_y = []\n",
    "for i, data in enumerate(dataset):\n",
    "    data = data#.to(device) #put to GPU\n",
    "    #break\n",
    "    jj= 0\n",
    "    for j in range(0,int(len(data.y))): \n",
    "        pi_yy.append(data.y[j].item() -data.elc[j] ) \n",
    "      \n",
    "    for j in range(len(data.rec)):\n",
    "        pi_rec_y.append(data.rec[j]-data.elc[j])\n",
    "       # p_rec_y.append(data.rec[j][0][1])\n",
    "\n",
    "#plott_pi = np.zeros((len(pi_x),4))\n",
    "#plott_p = np.zeros((len(p_x),2)) \n",
    "plott_pi = np.zeros((len(pi_x),3))\n",
    "for i in range(len(pi_x)):\n",
    "#for i in range(982):\n",
    "    plott_pi[i][0] = pi_yy[i]\n",
    "    plott_pi[i][1] = pi_x[i]# [0]\n",
    "    #plott_pi[i][3] = pi_val[i]\n",
    "   # plott_pi[i][2] = pi_rec_y[i] \n",
    "   # plott_p[i][0] = p_yy[i]\n",
    "   # plott_p[i][1] = p_x[i]\n",
    "    #plott_p[i][2]  = p_rec_y[i] \n",
    "for i in range(len(pi_x)): \n",
    "    plott_pi[i][2] = pi_rec_y[i] \n",
    "#plott_pi \n",
    "plt.figure(0) ##################\n",
    "plt.title('location of Pion vertex') \n",
    "#plt.axvline(x = np.mean(pi_x), color = 'blue') \n",
    "#plt.axvline(x = np.mean(pi_yy), color = 'orange') \n",
    "plt.hist(x = plott_pi, histtype ='step', color = ['orange', 'blue', 'green'], bins = 500 ) \n",
    "#plt.hist(x = plott_pi, histtype ='step', bins = 500 )\n",
    "#plt.legend(['Predicted', 'True', 'REC']) \n",
    "#plt.legend(['True', 'Predicted', 'REC']) \n",
    "plt.xlim((-10,15)) \n",
    "plt.ylabel('count') \n",
    "plt.xlabel('Location of Pion vertex (cm)')  \n",
    " #############################################################\n",
    "\n",
    "plt.figure(1) #######################\n",
    "pi_yy = np.array(pi_yy)\n",
    "pi_x = np.array(pi_x)\n",
    "pi_rec_y = np.array(pi_rec_y)#np.logical_and\n",
    "indices_two = np.where(np.logical_and(pi_yy < -1.5, pi_yy > -2) ) \n",
    "#indices_two = np.where(pi_yy <-1.5 and pi_yy>-2 )\n",
    "#print(indices_two)\n",
    "#print(pi_x[indices_two])  \n",
    "\n",
    "diff_x = [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
    "diff_rec  = [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
    "for j in range(1,24):\n",
    "    indices = np.where(np.logical_and(pi_yy < -2+j/2, pi_yy > -2+j/2-.5) ) \n",
    "    dif_x =[ abs(pi_x[indices] - pi_yy[indices])]\n",
    "    dif_rec = [abs(pi_rec_y[indices] - pi_yy[indices])]\n",
    "    diff_x[j-1].append(dif_x)\n",
    "    diff_rec[j-1].append(dif_rec)\n",
    "mean_x = np.zeros(24)\n",
    "mean_rec = np.zeros(24)\n",
    "error_x = np.zeros(24)\n",
    "error_rec = np.zeros(24)\n",
    "for i in range(24): \n",
    "    mean_x[i] = np.nanmean(diff_x[i])\n",
    "    mean_rec[i] = np.nanmean(diff_rec[i]) \n",
    "    error_x[i] = np.nanstd(diff_x[i])\n",
    "    error_rec[i] = np.nanstd(diff_rec[i])\n",
    "xxx = np.linspace(-2,10,24)\n",
    "plt.scatter(xxx, mean_x)\n",
    "plt.scatter(xxx, mean_rec, alpha = .5 )  \n",
    "plt.errorbar(xxx, mean_x,  yerr=error_x, fmt=\"o\")   \n",
    "plt.errorbar(xxx, mean_rec,  yerr=error_rec, fmt=\"o\", alpha = .5)\n",
    "plt.title('absolute error based on pion vertex, orange is rec, blue is GNN')\n",
    "plt.xlabel('Pion vertex (cm)') \n",
    "plt.ylabel('abolute error from True (cm)')\n",
    " #############################################################\n",
    "plt.figure(2) ####################### \n",
    "diff_x = [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
    "diff_rec  = [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
    "for j in range(1,24):\n",
    "    indices = np.where(np.logical_and(pi_yy < -2+j/2, pi_yy > -2+j/2-.5) ) \n",
    "    dif_x =[ (pi_x[indices] - pi_yy[indices])]\n",
    "    dif_rec = [(pi_rec_y[indices] - pi_yy[indices])]\n",
    "    diff_x[j-1].append(dif_x)\n",
    "    diff_rec[j-1].append(dif_rec)\n",
    "mean_x = np.zeros(24)\n",
    "mean_rec = np.zeros(24)\n",
    "error_x = np.zeros(24)\n",
    "error_rec = np.zeros(24)\n",
    "for i in range(24): \n",
    "    mean_x[i] = np.nanmean(diff_x[i])\n",
    "    mean_rec[i] = np.nanmean(diff_rec[i]) \n",
    "    error_x[i] = np.nanstd(diff_x[i])\n",
    "    error_rec[i] = np.nanstd(diff_rec[i])\n",
    "xxx = np.linspace(-2,10,24)\n",
    "plt.scatter(xxx, mean_x)\n",
    "plt.scatter(xxx, mean_rec, alpha = .5 )  \n",
    "plt.errorbar(xxx, mean_x,  yerr=error_x, fmt=\"o\")   \n",
    "plt.errorbar(xxx, mean_rec,  yerr=error_rec, fmt=\"o\", alpha = .5)\n",
    "plt.title(' error based on pion vertex, orange is rec, blue is GNN')\n",
    "plt.xlabel('Pion vertex (cm)') \n",
    "plt.ylabel(' error from True (cm)')\n",
    " #############################################################\n",
    "\n",
    "pi_mas = []\n",
    "pi_LamT = []\n",
    "#p_rec_y = []\n",
    "for i, data in enumerate(dataset):\n",
    "    data = data#.to(device) #put to GPU\n",
    "    #break\n",
    "    #print(data.LamT)\n",
    "    for j in range(0,int(len(data.imass))): \n",
    "        pi_mas.append(data.imass[j]) \n",
    "        pi_LamT.append(data.LamT[j]) \n",
    "Total_signal = pi_LamT.count(1)\n",
    "print('S/B')\n",
    "print('Purity') \n",
    "print('Efficienty')\n",
    "s_to_b_values = []\n",
    "purity_values = []\n",
    "efficiency_values = []\n",
    "for j in range(0, 20):\n",
    "    pi_LamTt = []\n",
    "    #imass\n",
    "    print('cut ',j/2)\n",
    "    for i in range(len(pi_x)): \n",
    "        if pi_x[i]>j/2:\n",
    "            #imass.append(pi_mas[i])\n",
    "            pi_LamTt.append(pi_LamT[i])\n",
    "    print(pi_LamTt.count(1))\n",
    "    #print(pi_LamT.count(0))\n",
    "    print(pi_LamTt.count(1)/pi_LamTt.count(0))\n",
    "    print(pi_LamTt.count(1)/(len(pi_LamTt)))\n",
    "    print(pi_LamTt.count(1)/(Total_signal))\n",
    "    #print(pi_LamTt.count(1)/len(pi_LamTt))\n",
    "    s_to_b = pi_LamTt.count(1)/pi_LamTt.count(0)\n",
    "    purity = pi_LamTt.count(1)/(len(pi_LamTt))\n",
    "    efficiency = pi_LamTt.count(1)/(Total_signal)\n",
    "    s_to_b_values.append(s_to_b)\n",
    "    purity_values.append(purity)\n",
    "    efficiency_values.append(efficiency)\n",
    "    print()\n",
    "    plt.figure(3)\n",
    "    plt.scatter(j/2, pi_LamTt.count(1)/pi_LamTt.count(0), color = 'blue')\n",
    "    plt.xlabel('pion vetex cut')\n",
    "    plt.ylabel('S/B') \n",
    "    plt.title('Graph of S/B as pion vertex cuts')\n",
    "    \n",
    "    plt.figure(4)\n",
    "    plt.scatter(j/2, pi_LamTt.count(1)/(len(pi_LamTt)), color = 'red')\n",
    "    plt.xlabel('pion vetex cut')\n",
    "    plt.ylabel('Purity') \n",
    "    plt.title('Graph of Purity as pion vertex cuts')\n",
    "                \n",
    "    plt.figure(5)\n",
    "    plt.scatter(j/2, pi_LamTt.count(1)/(Total_signal), color = 'purple') \n",
    "    plt.xlabel('pion vetex cut')\n",
    "    plt.ylabel('Efficiency ') \n",
    "    plt.title('Graph of Efficiency  as pion vertex cuts')\n",
    "    #print(len(pi_LamT))\n",
    "    #plt.figure()\n",
    "    #plt.title('cut'+str( j/2))  \n",
    "    #plt.hist(imass, bins = 25)\n",
    "##################################################################new stufff. \n",
    "pi_x = np.zeros(len(outt)) \n",
    "for i in range(len(outt)):\n",
    "    pi_x[i] = outt[i][0]\n",
    "pi_yy = [] \n",
    "#p_yy = [] \n",
    "#pi_x = outt\n",
    "pi_rec_y = []\n",
    "#p_rec_y = []\n",
    "for i, data in enumerate(dataset):\n",
    "    data = data#.to(device) #put to GPU\n",
    "    jj= 0\n",
    "    for j in range(0,int(len(data.y))): \n",
    "        pi_yy.append(data.y[j].item() -data.elc[j] ) \n",
    "      \n",
    "    for j in range(len(data.rec)):\n",
    "        pi_rec_y.append(data.rec[j]-data.elc[j])\n",
    "\n",
    "plott_pi = np.zeros((len(pi_x),3))\n",
    "for i in range(len(pi_x)):\n",
    "    if pi_rec_y[i]>4:\n",
    "        if pi_rec_y[i]<6:\n",
    "            plott_pi[i][0] = pi_yy[i]\n",
    "            plott_pi[i][1] = pi_x[i]# [0]\n",
    "\n",
    "for i in range(len(pi_x)): \n",
    "    if pi_rec_y[i]>4:\n",
    "        if pi_rec_y[i]<6:\n",
    "            plott_pi[i][2] = pi_rec_y[i] \n",
    "#plott_pi \n",
    "\n",
    "plt.figure(7) #############################################################\n",
    "pi_yy = np.array(pi_yy)\n",
    "pi_x = np.array(pi_x)\n",
    "pi_rec_y = np.array(pi_rec_y)#np.logical_and\n",
    "indices_two = np.where(np.logical_and(pi_yy < -1.5, pi_yy > -2) ) \n",
    "#indices_two = np.where(pi_yy <-1.5 and pi_yy>-2 )\n",
    "#print(indices_two)\n",
    "#print(pi_x[indices_two])  \n",
    "\n",
    "diff_x = [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
    "diff_rec  = [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
    "for j in range(1,24):\n",
    "    indices = np.where(np.logical_and(pi_rec_y < -2+j/2, pi_rec_y > -2+j/2-.5) ) \n",
    "    dif_x =[ abs(pi_x[indices] - pi_yy[indices])]\n",
    "    dif_rec = [abs(pi_rec_y[indices] - pi_yy[indices])]\n",
    "    diff_x[j-1].append(dif_x)\n",
    "    diff_rec[j-1].append(dif_rec)\n",
    "mean_x = np.zeros(24)\n",
    "mean_rec = np.zeros(24)\n",
    "error_x = np.zeros(24)\n",
    "error_rec = np.zeros(24)\n",
    "for i in range(24): \n",
    "    mean_x[i] = np.nanmean(diff_x[i])\n",
    "    mean_rec[i] = np.nanmean(diff_rec[i]) \n",
    "    error_x[i] = np.nanstd(diff_x[i])\n",
    "    error_rec[i] = np.nanstd(diff_rec[i])\n",
    "xxx = np.linspace(-2,10,24)\n",
    "print('based on rec')\n",
    "print(mean_x)\n",
    "print(mean_rec)\n",
    "print(error_x)\n",
    "print(error_rec)\n",
    "print()\n",
    "plt.scatter(xxx, mean_x)\n",
    "plt.scatter(xxx, mean_rec, alpha = .5 )  \n",
    "plt.errorbar(xxx, mean_x,  yerr=error_x, fmt=\"o\")   \n",
    "plt.errorbar(xxx, mean_rec,  yerr=error_rec, fmt=\"o\", alpha = .5)\n",
    "plt.title('Error Based on rec Vertex')\n",
    "plt.xlabel('Pion rec Vertex (cm)') \n",
    "plt.ylabel('Abolute Error from True (cm)')\n",
    "#plt.savefig('Newstuff2.png')\n",
    "\n",
    "plt.figure(8) #############################################################\n",
    "pi_yy = np.array(pi_yy)\n",
    "pi_x = np.array(pi_x)\n",
    "pi_rec_y = np.array(pi_rec_y)#np.logical_and\n",
    "indices_two = np.where(np.logical_and(pi_yy < -1.5, pi_yy > -2) ) \n",
    "#indices_two = np.where(pi_yy <-1.5 and pi_yy>-2 )\n",
    "#print(indices_two)\n",
    "#print(pi_x[indices_two])  \n",
    "\n",
    "diff_x = [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
    "diff_rec  = [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
    "for j in range(1,24):\n",
    "    indices = np.where(np.logical_and(pi_x < -2+j/2, pi_x > -2+j/2-.5) ) \n",
    "    dif_x =[ abs(pi_x[indices] - pi_yy[indices])]\n",
    "    dif_rec = [abs(pi_rec_y[indices] - pi_yy[indices])]\n",
    "    diff_x[j-1].append(dif_x)\n",
    "    diff_rec[j-1].append(dif_rec)\n",
    "mean_x = np.zeros(24)\n",
    "mean_rec = np.zeros(24)\n",
    "error_x = np.zeros(24)\n",
    "error_rec = np.zeros(24)\n",
    "for i in range(24): \n",
    "    mean_x[i] = np.nanmean(diff_x[i])\n",
    "    mean_rec[i] = np.nanmean(diff_rec[i]) \n",
    "    error_x[i] = np.nanstd(diff_x[i])\n",
    "    error_rec[i] = np.nanstd(diff_rec[i])\n",
    "xxx = np.linspace(-2,10,24)\n",
    "print('based on GNN')\n",
    "print(mean_x)\n",
    "print(mean_rec)\n",
    "print(error_x)\n",
    "print(error_rec)\n",
    "print() \n",
    "plt.scatter(xxx, mean_x)\n",
    "plt.scatter(xxx, mean_rec, alpha = .5 )  \n",
    "plt.errorbar(xxx, mean_x,  yerr=error_x, fmt=\"o\")   \n",
    "plt.errorbar(xxx, mean_rec,  yerr=error_rec, fmt=\"o\", alpha = .5)\n",
    "plt.title('Error based on GNN vertex (abs)')\n",
    "plt.xlabel('Pion GNN vertex (cm)') \n",
    "plt.ylabel('Abolute Error from True (cm)')\n",
    "#plt.savefig('Newstuff3.png') \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "outt= [] \n",
    "pi_momentum  = []\n",
    "#for i,data in enumerate(dataset): \n",
    "for i,data in enumerate(dataset):\n",
    "    data = data.to(device) #switch to GPU                                                                                                                                                                                                 \n",
    "    #optimizer.zero_grad() #                                                                                                                                                                                                               \n",
    "    #out = model(data.x, data.edge_index, data.batch).to(device)  # Perform a single forward pass                                                                                                                                          \n",
    "    #out = out.cpu()\n",
    "    #print(out) \n",
    "    #print(data.elc)  \n",
    "   # neww = []\n",
    "    #for j in range(len(data)):\n",
    "        #neww.append(out[j].item() -data.elc[j][0])\n",
    "     #   neww.append(out[j].item() -data.elc[j])\n",
    "    #outt+=[[neww]]\n",
    "    #outt+=[[out[0][0].item()-data.elc[0]]] \n",
    "    #outt+=[[out[0][0].item()-data.elc[0]]]\n",
    "    for j in range(0,int(len(data.momentumt))): \n",
    "        pi_momentum.append(data.momentumt[j] )  \n",
    "        \n",
    "pi_momentum = np.array(pi_momentum) \n",
    "plt.figure(9) ####################### \n",
    "diff_x = [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
    "diff_rec  = [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
    "for j in range(1,20):\n",
    "    indices = np.where(np.logical_and(pi_momentum < 0+j/4, pi_momentum > 0+j/4-.25) )  \n",
    "    dif_x =[ abs(pi_x[indices] - pi_yy[indices])]\n",
    "    dif_rec = [abs(pi_rec_y[indices] - pi_yy[indices])]\n",
    "    diff_x[j-1].append(dif_x)\n",
    "    diff_rec[j-1].append(dif_rec)\n",
    "mean_x = np.zeros(20)\n",
    "mean_rec = np.zeros(20)  \n",
    "error_x = np.zeros(20)\n",
    "error_rec = np.zeros(20)\n",
    "for i in range(20): \n",
    "    mean_x[i] = np.nanmean(diff_x[i])\n",
    "    mean_rec[i] = np.nanmean(diff_rec[i]) \n",
    "    error_x[i] = np.nanstd(diff_x[i])\n",
    "    error_rec[i] = np.nanstd(diff_rec[i])\n",
    "xxx = np.linspace(0,5,20) \n",
    "plt.scatter(xxx, mean_x)\n",
    "plt.scatter(xxx, mean_rec, alpha = .5 )  \n",
    "plt.errorbar(xxx, mean_x,  yerr=error_x, fmt=\"o\")   \n",
    "plt.errorbar(xxx, mean_rec,  yerr=error_rec, fmt=\"o\", alpha = .5)\n",
    "plt.title(' Error based on Momentum')\n",
    "plt.xlabel('Pion momentum (cm)') \n",
    "plt.ylabel(' Error from True (cm)')  ##################################################\n",
    "\n",
    "\n",
    "outt= []  \n",
    "pi_mas = []\n",
    "#for i,data in enumerate(dataset): \n",
    "for i,data in enumerate(dataset):\n",
    "    data = data.to(device) #switch to GPU                                                                                                                                                                                                 \n",
    "    #optimizer.zero_grad() #                                                                                                                                                                                                               \n",
    "    out = model(data.x, data.edge_index, data.batch).to(device)  # Perform a single forward pass                                                                                                                                          \n",
    "    out = out.cpu()\n",
    "    #print(out) \n",
    "    #print(data.elc) \n",
    "\n",
    "   # neww = []\n",
    "    #for j in range(len(data)):\n",
    "        #neww.append(out[j].item() -data.elc[j][0])\n",
    "     #   neww.append(out[j].item() -data.elc[j])\n",
    "    #outt+=[[neww]]\n",
    "    #outt+=[[out[0][0].item()-data.elc[0]]] \n",
    "    outt+=[[out[0][0].item()-data.elc[0]]]\n",
    "    for j in range(0,int(len(data.imass))): \n",
    "        pi_mas.append(data.imass[j] ) \n",
    "#return outt  \n",
    "#b= print_outbb() \n",
    "b=outt \n",
    "\n",
    "\n",
    "pi_x = np.zeros(len(outt))\n",
    "for i in range(len(outt)):\n",
    "    pi_x[i] = outt[i][0]\n",
    "pi_yy = [] \n",
    "#p_yy = [] \n",
    "#pi_x = outt\n",
    "pi_rec_y = []\n",
    "#p_rec_y = []\n",
    "for i, data in enumerate(dataset):\n",
    "    data = data#.to(device) #put to GPU\n",
    "    #break\n",
    "    jj= 0\n",
    "    for j in range(0,int(len(data.y))): \n",
    "        pi_yy.append(data.y[j].item() -data.elc[j] ) \n",
    "      \n",
    "    for j in range(len(data.rec)):\n",
    "        pi_rec_y.append(data.rec[j]-data.elc[j])\n",
    "       # p_rec_y.append(data.rec[j][0][1])\n",
    "       \n",
    "pi_yy = np.array(pi_yy)\n",
    "pi_rec_y = np.array(pi_rec_y)\n",
    "plt.figure(10)\n",
    "\n",
    "plt.scatter(pi_x, pi_rec_y, color = 'blue')\n",
    "plt.title('Correlation Between GNN and REC') \n",
    "#plt.xlim((-10,15)) \n",
    "plt.ylabel('REC Vertex (cm)') \n",
    "plt.xlabel('GNN Vertex (cm)') \n",
    "\n",
    "plt.figure(11)\n",
    "\n",
    "plt.scatter(pi_x, pi_yy, color = 'green')\n",
    "plt.title('Correlation Between GNN and Truth') \n",
    "#plt.xlim((-10,15)) \n",
    "plt.ylabel('MC Vertex (cm)') \n",
    "plt.xlabel('GNN Vertex (cm)') \n",
    "\n",
    "plt.figure(12) \n",
    "\n",
    "plt.scatter(pi_rec_y, pi_yy, color = 'green')\n",
    "plt.title('Correlation Between REC and Truth') \n",
    "#plt.xlim((-10,15)) \n",
    "plt.ylabel('MC Vertex (cm)') \n",
    "plt.xlabel('REC Vertex (cm)')  \n",
    "\n",
    "\n",
    "plt.figure(13)\n",
    "plt.hist2d(pi_rec_y, pi_yy, bins=50, cmap='viridis')\n",
    "plt.colorbar(label='Density')\n",
    "plt.title('Correlation Between REC and Truth')\n",
    "plt.ylabel('MC Vertex (cm)')\n",
    "plt.xlabel('REC Vertex (cm)')\n",
    "\n",
    "plt.figure(14)\n",
    "plt.hist2d(pi_x, pi_yy, bins=50, cmap='viridis')\n",
    "plt.colorbar(label='Density')\n",
    "plt.title('Correlation Between GNN and Truth')\n",
    "plt.ylabel('MC Vertex (cm)')\n",
    "plt.xlabel('GNN Vertex (cm)')\n",
    "\n",
    "plt.figure(15)\n",
    "plt.hist2d(pi_x, pi_rec_y, bins=50, cmap='viridis')\n",
    "plt.colorbar(label='Density')\n",
    "plt.title('Correlation Between GNN and Truth')\n",
    "plt.ylabel('REC Vertex (cm)')\n",
    "plt.xlabel('GNN Vertex (cm)')\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.stats import gaussian_kde\n",
    "xyz = np.vstack([pi_x, pi_yy, pi_rec_y])\n",
    "density = gaussian_kde(xyz)(xyz)\n",
    "\n",
    "# Create the 3D scatter plot\n",
    "fig = plt.figure(16)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "sc = ax.scatter(pi_x, pi_yy, pi_rec_y, c=density, cmap='viridis')\n",
    "ax.set_xlim(-50, 150)\n",
    "ax.set_ylim(-50, 150)\n",
    "ax.set_zlim(-150, 150)\n",
    "\n",
    "# Add color bar which maps values to colors\n",
    "cbar = plt.colorbar(sc)\n",
    "cbar.set_label('Density')\n",
    "\n",
    "# Create the 3D scatter plot\n",
    "fig = plt.figure(17)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "sc = ax.scatter(pi_x, pi_yy, pi_rec_y, c=density, cmap='viridis')\n",
    "ax.set_xlim(-50, 75)\n",
    "ax.set_ylim(-50, 150)\n",
    "ax.set_zlim(-150, 150)\n",
    "\n",
    "# Add color bar which maps values to colors\n",
    "cbar = plt.colorbar(sc)\n",
    "cbar.set_label('Density')\n",
    "\n",
    "for iiii in range(18):  \n",
    "    plt.figure(iiii)\n",
    "    idddddk = '800_finall'+str(iiii)+'.png'\n",
    "    #plt.savefig(idddddk)   \n",
    "print('doneeeeeee')\n",
    "np.save('s_to_b_valuesHnrecsingle1100.npy', s_to_b_values) \n",
    "np.save('purity_valuesHnrecsingle1100.npy', purity_values) \n",
    "np.save('efficiency_valuesHnrecsingle1100.npy', efficiency_values)\n",
    "\n",
    "PATH = '/hpc/group/vossenlab/kam264/800_model_testt'\n",
    "torch.save(model.state_dict(), PATH)\n",
    "print('done with test')'''      \n",
    "\n",
    "f = open(\"/hpc/group/vossenlab/kam264/dir_name_2/batch_filer.py\", \"w\")  \n",
    "f.write(code)              \n",
    "f.close()         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d01172-2377-4539-81f4-2a7b90b71ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "15498747, idddddk = 'letsgoo'+str(iiii)+'.png', single dataset, 200 epochs. \n",
    "15553908, testing for scatter. 15554469, for all the sactters. \n",
    "new plots, 500 single 15556937, plotsingle#.png\n",
    "15556977, new plots, four, 60 each, plotsfour#.png \n",
    "15653361, quikc test for saving. \n",
    "\n",
    "15653435 , 800 single, new grpahss, single_\n",
    "\n",
    "16012885, final. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dfb98f6-eecd-4934-9aa1-2db75f9f37d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([    0,     1,     2, ..., 18629, 18632, 18634]),)\n"
     ]
    }
   ],
   "source": [
    "import os   \n",
    "import numpy as np \n",
    "import numpy.ma as ma   \n",
    "#import awkward as ak \n",
    "from tqdm import tqdm \n",
    "import torch \n",
    "import torch_geometric as tg  \n",
    "import torch_geometric \n",
    "from torch_geometric.data import Data \n",
    "#import torch \n",
    "from torch_geometric.data import InMemoryDataset, download_url  \n",
    "import torch_geometric.transforms as T \n",
    "\n",
    "#NOTE: NEW 2/20/23      \n",
    "from typing import List, Union     \n",
    "\n",
    "from torch_geometric.data import Data, HeteroData \n",
    "from torch_geometric.data.datapipes import functional_transform\n",
    "from torch_geometric.transforms import BaseTransform       \n",
    "torch.cuda.empty_cache()  \n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn.norm import GraphNorm, BatchNorm \n",
    "from torch.utils.data import random_split \n",
    "from torch_geometric.loader import DataLoader \n",
    "\n",
    "class MyOwnDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.load(self.processed_paths[0])\n",
    "        # For PyG<2.4:\n",
    "        # self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['some_file_1', 'some_file_2']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "    def process(self):\n",
    "        # Read data into huge `Data` list.\n",
    "        data_list = None\n",
    "\n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    "\n",
    "        self.save(data_list, self.processed_paths[0]) \n",
    "root = '/hpc/group/vossenlab/kam264/Hmom4' \n",
    "#root = '/hpc/group/vossenlab/kam264/QuatorL_Last'\n",
    "dataset = MyOwnDataset(\n",
    "        root,\n",
    "        transform=None, #T.Compose([T.ToUndirected(),T.KNNGraph(k=6)]),\n",
    "        pre_transform=None,\n",
    "        pre_filter=None\n",
    "    )    \n",
    "\n",
    "pi_momentum  = []\n",
    "for i,data in enumerate(dataset):\n",
    "    data = data#.to(device) #switch to GPU                                                                                                                                                                                                 \n",
    "    #optimizer.zero_grad() #                                                                                                                                                                                                               \n",
    "    #out = model(data.x, data.edge_index, data.batch).to(device)  # Perform a single forward pass                                                                                                                                          \n",
    "    #out = out.cpu()\n",
    "    #print(out) \n",
    "    #print(data.elc)  \n",
    "   # neww = []\n",
    "    #for j in range(len(data)):\n",
    "        #neww.append(out[j].item() -data.elc[j][0])\n",
    "     #   neww.append(out[j].item() -data.elc[j])\n",
    "    #outt+=[[neww]]\n",
    "    #outt+=[[out[0][0].item()-data.elc[0]]] \n",
    "    #outt+=[[out[0][0].item()-data.elc[0]]]\n",
    "    for j in range(0,int(len(data.momentumt))): \n",
    "        pi_momentum.append(data.momentumt[j] ) \n",
    "pi_momentum = np.array(pi_momentum) \n",
    "j=2 \n",
    "indices = np.where(np.logical_and(pi_momentum < 0+j/2, pi_momentum > 0+j/2-.5) )\n",
    "print(indices)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e947f4-c3b5-49db-9bf4-9a6c8ad9ff85",
   "metadata": {},
   "outputs": [],
   "source": [
    "15427603 ,  15427911, 15428311 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c782ab-e686-43fe-bc7c-78ca44149ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(10) #############################################################\n",
    "pi_yy = np.array(pi_yy)\n",
    "pi_x = np.array(pi_x)\n",
    "pi_rec_y = np.array(pi_rec_y)#np.logical_and\n",
    "indices_two = np.where(np.logical_and(pi_yy < -1.5, pi_yy > -2) ) \n",
    "#indices_two = np.where(pi_yy <-1.5 and pi_yy>-2 )\n",
    "#print(indices_two)\n",
    "#print(pi_x[indices_two])  \n",
    "\n",
    "diff_x = [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
    "diff_rec  = [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
    "for j in range(1,24):\n",
    "    indices = np.where(np.logical_and(pi_yy < -2+j/2, pi_yy > -2+j/2-.5) ) \n",
    "    dif_x =[ pi_x[indices]]\n",
    "    dif_rec = [pi_rec_y[indices]]\n",
    "    diff_x[j-1].append(dif_x)\n",
    "    diff_rec[j-1].append(dif_rec)\n",
    "mean_x = np.zeros(24)\n",
    "mean_rec = np.zeros(24)\n",
    "error_x = np.zeros(24)\n",
    "error_rec = np.zeros(24)\n",
    "for i in range(24): \n",
    "    mean_x[i] = np.nanmean(diff_x[i])\n",
    "    mean_rec[i] = np.nanmean(diff_rec[i]) \n",
    "    error_x[i] = np.nanstd(diff_x[i])\n",
    "    error_rec[i] = np.nanstd(diff_rec[i])\n",
    "xxx = np.linspace(-2,10,24)\n",
    "print('biined distribution mean. ')\n",
    "print(mean_x)\n",
    "print(mean_rec)\n",
    "print(error_x)\n",
    "print(error_rec)\n",
    "plt.scatter(xxx, mean_x)\n",
    "plt.scatter(xxx, mean_rec, alpha = .5 )  \n",
    "plt.errorbar(xxx, mean_x,  yerr=error_x, fmt=\"o\")   \n",
    "plt.errorbar(xxx, mean_rec,  yerr=error_rec, fmt=\"o\", alpha = .5)\n",
    "plt.title('binned distibution of pion vertex based on true vertex, orange is rec, blue is GNN')\n",
    "plt.xlabel('Pion vertex (cm)') \n",
    "plt.ylabel('abolute error from True (cm)')\n",
    "#plt.savefig('Newstuff4.png')   ##############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e01f33e-a058-442a-948c-6a88bb9f8142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8304e5ee-5a1a-401c-8c37-70bbae535237",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basecode what I showed on Monday.  , go to end for new code. \n",
    "code = '''import os   \n",
    "import numpy as np\n",
    "import numpy.ma as ma   \n",
    "#import awkward as ak \n",
    "from tqdm import tqdm \n",
    "import torch \n",
    "import torch_geometric as tg  \n",
    "import torch_geometric \n",
    "from torch_geometric.data import Data \n",
    "#import torch \n",
    "from torch_geometric.data import InMemoryDataset, download_url  \n",
    "import torch_geometric.transforms as T \n",
    "\n",
    "#NOTE: NEW 2/20/23      \n",
    "from typing import List, Union     \n",
    "\n",
    "from torch_geometric.data import Data, HeteroData \n",
    "from torch_geometric.data.datapipes import functional_transform\n",
    "from torch_geometric.transforms import BaseTransform       \n",
    "torch.cuda.empty_cache()  \n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn.norm import GraphNorm, BatchNorm \n",
    "from torch.utils.data import random_split \n",
    "from torch_geometric.loader import DataLoader \n",
    "\n",
    "class MyOwnDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.load(self.processed_paths[0])\n",
    "        # For PyG<2.4:\n",
    "        # self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['some_file_1', 'some_file_2']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "    def process(self):\n",
    "        # Read data into huge `Data` list.\n",
    "        data_list = None\n",
    "\n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    "\n",
    "        self.save(data_list, self.processed_paths[0]) \n",
    "\n",
    "#root = '/hpc/group/vossenlab/kam264/L_imass3'\n",
    "#root = '/hpc/group/vossenlab/kam264/onlylambda_25k' \n",
    "#root = '/hpc/group/vossenlab/kam264/Lambda_13_16'\n",
    "#root = '/hpc/group/vossenlab/kam264/Lambda_1all' \n",
    "\n",
    "batch_size = 1 \n",
    "LR =1e-2\n",
    "torch.cuda.empty_cache()   \n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels): \n",
    "        super(GCN, self).__init__()\n",
    "#         torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)#.jittable() #NOTE: NEEDED FOR DEPLOYMENT IN CMAKE\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)#.jittable()\n",
    "        #self.block2 = nn.DataParallel(self.block2)\n",
    "        #self.conv2 = torch.nn.DataParallel(self.conv2) #this was trying the parallization thing. \n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)#.jittable()\n",
    "        #self.conv3 = torch.nn.DataParallel(self.conv3)\n",
    "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin3 = Linear(hidden_channels, out_channels)\n",
    "        self.bn1 = torch_geometric.nn.norm.GraphNorm(hidden_channels)\n",
    "        self.bn2 = torch_geometric.nn.norm.GraphNorm(hidden_channels)\n",
    "        self.bn3 = torch_geometric.nn.norm.GraphNorm(hidden_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):  \n",
    "        \n",
    "        x = self.conv1(x, edge_index) #input layer                             \n",
    "                                                      \n",
    "        x = self.bn1(x) #normalize it                                          \n",
    "\n",
    "        x = x.relu() #activation                                               \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = x.relu()\n",
    "#         print(\"x.relu() = \",x)  \n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = self.bn3(x)\n",
    "#         # 2. Readout layer                                                   \n",
    "        x = global_mean_pool(x, batch)\n",
    "        # 3. Apply a final classifier                                          \n",
    "        x = F.dropout(x, p=0.5, training=self.training) #for overfittin        \n",
    "        x = self.lin3(x) \n",
    " \n",
    "        return x\n",
    "def RMSELoss(out,y):\n",
    "    return torch.sqrt(torch.mean((out-y)**2))\n",
    "def train():\n",
    "    model.train() #initailize the model                                                                                                                                                                                                        \n",
    "    #for i, data in tqdm(enumerate(train_loader)): #perhaps tqdm(enumerate(train_loader)), i is index, data jsut moves through all the dtaa in trainingg                                                                                      \n",
    "    for i,data in enumerate(train_loader):\n",
    "        data = data.to(device) #switch to GPU                                                                                                                                                                                                 \n",
    "        optimizer.zero_grad() #                                                                                                                                                                                                               \n",
    "        out = model(data.x, data.edge_index, data.batch).to(device)  # Perform a single forward pass                                                                                                                                          \n",
    "        yy = []\n",
    "        for j in range(0,len(out)):\n",
    "            yy+= [[data.y[j].item()]]\n",
    "\n",
    "        yy = torch.tensor(yy).to(device) \n",
    "        #print(out)\n",
    "        #print(yy)\n",
    "        loss = losss(out, yy).to(device) #compute the loss  \n",
    "        #print(loss)\n",
    "        loss.backward() #get the gradients.                                                                                                                                                                                                   \n",
    "        optimizer.step() \n",
    "def test(loader): \n",
    "    length = len(loader.dataset)\n",
    "    model.eval() #evaluate teh model.                                                                                                                                                                                                         \n",
    "\n",
    "    #mse_tot = []                                                                                                                                                                                                                             \n",
    "    mse_total = 0\n",
    "    mse_pi = 0\n",
    "    mse_p = 0\n",
    "    #r                                                                                                                                                                                                                                        \n",
    "    #for data in tqdm(loader):  # Iterate in batches over the training/test dataset.                                                                                                                                                          \n",
    "    for data in loader:\n",
    "        data = data.to(device) #put to GPU                                                                                                                                                                                                    \n",
    "        out = model(data.x, data.edge_index, data.batch).to(device) #evalueate                                                                                                                                                                \n",
    "        #this and the for loop is converting data.y to a tensor in the same shape as out rows and 2 columns first is y_pion second is y_proton                                                                                                \n",
    "        yy = []\n",
    "        for j in range(0,len(out)):\n",
    "            yy+= [[data.y[j].item()]]\n",
    "        yy = torch.tensor(yy).to(device) \n",
    "        loss = losss(out, yy).cpu() #getting teh loss function                                                                                                                                                                                \n",
    "        mse_total+=loss.item() #getting the mse (total)                                                                                                                                                                                       \n",
    "\n",
    "    return mse_total/length \n",
    "def print_out():\n",
    "    model.eval() #initailize the model                                                                                                                                                                                                        \n",
    "    #for i, data in tqdm(enumerate(train_loader)): #perhaps tqdm(enumerate(train_loader)), i is index, data jsut moves through all the dtaa in trainingg                                                                                      \n",
    "    outt= []\n",
    "    for i,data in enumerate(test_loader):\n",
    "        data = data.to(device) #switch to GPU                                                                                                                                                                                                 \n",
    "        optimizer.zero_grad() #                                                                                                                                                                                                               \n",
    "        out = model(data.x, data.edge_index, data.batch).to(device)  # Perform a single forward pass                                                                                                                                          \n",
    "        out = out.cpu()\n",
    "\n",
    "        outt+=[[out.detach().numpy()]]\n",
    "    return outt\n",
    "\n",
    "def print_outb():\n",
    "    model.eval() #initailize the model                                                                                                                                                                                                        \n",
    "    #for i, data in tqdm(enumerate(train_loader)): #perhaps tqdm(enumerate(train_loader)), i is index, data jsut moves through all the dtaa in trainingg      \n",
    "\n",
    "    outt= [] \n",
    "    #for i,data in enumerate(dataset):\n",
    "    for i,data in enumerate(test_loader):\n",
    "        data = data.to(device) #switch to GPU                                                                                                                                                                                                 \n",
    "        optimizer.zero_grad() #                                                                                                                                                                                                               \n",
    "        out = model(data.x, data.edge_index, data.batch).to(device)  # Perform a single forward pass                                                                                                                                          \n",
    "        out = out.cpu()\n",
    "\n",
    "        neww = []\n",
    "        for j in range(len(data)):\n",
    "            neww.append(out[j].item() -data.elc[j][0]) \n",
    "        outt+=[[neww]]\n",
    "    return outt  \n",
    "#model = GCN(dataset.num_node_features,64,2)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') \n",
    "#devicee = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu') \n",
    "print(\"Device = \",device) \n",
    "\n",
    "print(\"DEBUGGING: torch.cuda.is_available() = \",torch.cuda.is_available())\n",
    "train_metrics = {'mse':[], \"rmse\":[] } \n",
    "vall_metrics = {'mse':[], \"rmse\":[] }\n",
    "\n",
    "for iii in range(1):   \n",
    "    if iii>0: \n",
    "        #del dataset\n",
    "        del train_loader  \n",
    "        del val_loader \n",
    "    if iii==0:\n",
    "        root = '/hpc/group/vossenlab/kam264/80k_neww'   \n",
    "    dataset = MyOwnDataset(\n",
    "            root,\n",
    "            transform=None, #T.Compose([T.ToUndirected(),T.KNNGraph(k=6)]),\n",
    "            pre_transform=None,\n",
    "            pre_filter=None\n",
    "        ) \n",
    "    dataset\n",
    "    model = GCN(dataset.num_node_features,64,2)\n",
    "    fracs = [0.9, 0.08, 0.02] #percent of dataset used for training testing and validatoin 80%,10%,10% #NOTE: SHOULD CHECK np.sum(fracs) == 1 and len(fracs)==3\n",
    "    fracs = [torch.sum(torch.tensor(fracs[:idx])) for idx in range(1,len(fracs)+1)] #get the indexes for training ... parts to use. \n",
    "    #print(fracs)\n",
    "    split1, split2 = [int(len(dataset)*frac) for frac in fracs[:-1]] \n",
    "    train_dataset = dataset[:split1]\n",
    "    val_dataset = dataset[split1:split2]\n",
    "    test_dataset = dataset[split2:]   \n",
    "     \n",
    "    print(f'Number of training graphs: {len(train_dataset)}')\n",
    "    print(f'Number of validation graphs: {len(val_dataset)}') \n",
    "\n",
    "    from torch_geometric.loader import DataLoader \n",
    "    #from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=8, shuffle=True)#, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset,  batch_size=batch_size,num_workers=8,  shuffle=True)\n",
    "    model = GCN(dataset.num_node_features, dataset.num_classes, 1).to(device) #initiate the model, #2 is the number of outputs here is 2 as pion_z, proton_z \n",
    "    model = model.to(device)  \n",
    "    del dataset\n",
    "    del train_dataset\n",
    "    del val_dataset\n",
    "    del test_dataset \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr= LR)  \n",
    "    \n",
    "    losss = RMSELoss  \n",
    "    nepochs = 1      \n",
    "    \n",
    "    for epoch in range(nepochs):  \n",
    "  \n",
    "        #print(\"BEFORE TRAIN()\")                                                                                                                                                                                                                  \n",
    "        train() \n",
    "        #print(\"BEFORE TEST(TRAIN_LOADER)\")                                                                                                                                                                                                       \n",
    "        #train_mse, train_rmse, train_mse_pi, train_rmse_pi, train_mse_p, train_rmse_p = test(train_loader)\n",
    "        train_mse = test(train_loader)\n",
    "\n",
    "        train_metrics['mse'].append(train_mse) \n",
    "\n",
    "        vall_mse =test(val_loader) \n",
    "\n",
    "\n",
    "        vall_metrics['mse'].append(vall_mse) \n",
    "\n",
    "        if epoch%9==0:\n",
    "            print(\"Epoch \",epoch,\" Train mse: \",train_mse)\n",
    "            print(\"Epoch \",epoch,\" Validation mse: \",vall_mse)\n",
    "        if epoch==(nepochs-1):\n",
    "            #PATH = '/work/clas12/users/mfmce/CLAS12_Lambda_resolution_REU_2023/model_best_auc.pt' \n",
    "\n",
    "            #a = print_out()\n",
    "            #b=  print_outb() \n",
    "            print(\"Epoch \",epoch,\" Train mse: \",train_mse)\n",
    "            print(\"Epoch \",epoch,\" Validation mse: \",vall_mse)\n",
    "\n",
    "root = '/hpc/group/vossenlab/kam264/Hmom4' \n",
    "#root = '/hpc/group/vossenlab/kam264/QuatorL_Last'\n",
    "dataset = MyOwnDataset(\n",
    "        root,\n",
    "        transform=None, #T.Compose([T.ToUndirected(),T.KNNGraph(k=6)]),\n",
    "        pre_transform=None,\n",
    "        pre_filter=None\n",
    "    )    \n",
    "\n",
    "model.eval() #initailize the model                                                                                                                                                                                                        \n",
    "#for i, data in tqdm(enumerate(train_loader)): #perhaps tqdm(enumerate(train_loader)), i is index, data jsut moves through all the dtaa in trainingg      \n",
    "\n",
    "outt= []  \n",
    "pi_mas = []\n",
    "#for i,data in enumerate(dataset): \n",
    "for i,data in enumerate(dataset):\n",
    "    data = data.to(device) #switch to GPU                                                                                                                                                                                                 \n",
    "    #optimizer.zero_grad() #                                                                                                                                                                                                               \n",
    "    out = model(data.x, data.edge_index, data.batch).to(device)  # Perform a single forward pass                                                                                                                                          \n",
    "    out = out.cpu()\n",
    "    #print(out) \n",
    "    #print(data.elc) \n",
    "\n",
    "   # neww = []\n",
    "    #for j in range(len(data)):\n",
    "        #neww.append(out[j].item() -data.elc[j][0])\n",
    "     #   neww.append(out[j].item() -data.elc[j])\n",
    "    #outt+=[[neww]]\n",
    "    #outt+=[[out[0][0].item()-data.elc[0]]] \n",
    "    outt+=[[out[0][0].item()-data.elc[0]]]\n",
    "    for j in range(0,int(len(data.imass))): \n",
    "        pi_mas.append(data.imass[j] ) \n",
    "#return outt  \n",
    "#b= print_outbb() \n",
    "b=outt \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "pi_x = np.zeros(len(outt))\n",
    "for i in range(len(outt)):\n",
    "    pi_x[i] = outt[i][0]\n",
    "pi_yy = [] \n",
    "#p_yy = [] \n",
    "#pi_x = outt\n",
    "pi_rec_y = []\n",
    "#p_rec_y = []\n",
    "for i, data in enumerate(dataset):\n",
    "    data = data#.to(device) #put to GPU\n",
    "    #break\n",
    "    jj= 0\n",
    "    for j in range(0,int(len(data.y))): \n",
    "        pi_yy.append(data.y[j].item() -data.elc[j] ) \n",
    "      \n",
    "    for j in range(len(data.rec)):\n",
    "        pi_rec_y.append(data.rec[j]-data.elc[j])\n",
    "       # p_rec_y.append(data.rec[j][0][1])\n",
    "\n",
    "#plott_pi = np.zeros((len(pi_x),4))\n",
    "#plott_p = np.zeros((len(p_x),2)) \n",
    "plott_pi = np.zeros((len(pi_x),3))\n",
    "for i in range(len(pi_x)):\n",
    "#for i in range(982):\n",
    "    plott_pi[i][0] = pi_yy[i]\n",
    "    plott_pi[i][1] = pi_x[i]# [0]\n",
    "    #plott_pi[i][3] = pi_val[i]\n",
    "   # plott_pi[i][2] = pi_rec_y[i] \n",
    "   # plott_p[i][0] = p_yy[i]\n",
    "   # plott_p[i][1] = p_x[i]\n",
    "    #plott_p[i][2]  = p_rec_y[i] \n",
    "for i in range(len(pi_x)): \n",
    "    plott_pi[i][2] = pi_rec_y[i] \n",
    "#plott_pi \n",
    "plt.figure(0) ##################\n",
    "plt.title('location of Pion vertex') \n",
    "#plt.axvline(x = np.mean(pi_x), color = 'blue') \n",
    "#plt.axvline(x = np.mean(pi_yy), color = 'orange') \n",
    "plt.hist(x = plott_pi, histtype ='step', color = ['orange', 'blue', 'green'], bins = 500 ) \n",
    "#plt.hist(x = plott_pi, histtype ='step', bins = 500 )\n",
    "#plt.legend(['Predicted', 'True', 'REC']) \n",
    "#plt.legend(['True', 'Predicted', 'REC']) \n",
    "plt.xlim((-10,15)) \n",
    "plt.ylabel('count') \n",
    "plt.xlabel('Location of Pion vertex (cm)')  \n",
    " #############################################################\n",
    "\n",
    "plt.figure(1) #######################\n",
    "pi_yy = np.array(pi_yy)\n",
    "pi_x = np.array(pi_x)\n",
    "pi_rec_y = np.array(pi_rec_y)#np.logical_and\n",
    "indices_two = np.where(np.logical_and(pi_yy < -1.5, pi_yy > -2) ) \n",
    "#indices_two = np.where(pi_yy <-1.5 and pi_yy>-2 )\n",
    "#print(indices_two)\n",
    "#print(pi_x[indices_two])  \n",
    "\n",
    "diff_x = [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
    "diff_rec  = [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
    "for j in range(1,24):\n",
    "    indices = np.where(np.logical_and(pi_yy < -2+j/2, pi_yy > -2+j/2-.5) ) \n",
    "    dif_x =[ abs(pi_x[indices] - pi_yy[indices])]\n",
    "    dif_rec = [abs(pi_rec_y[indices] - pi_yy[indices])]\n",
    "    diff_x[j-1].append(dif_x)\n",
    "    diff_rec[j-1].append(dif_rec)\n",
    "mean_x = np.zeros(24)\n",
    "mean_rec = np.zeros(24)\n",
    "error_x = np.zeros(24)\n",
    "error_rec = np.zeros(24)\n",
    "for i in range(24): \n",
    "    mean_x[i] = np.nanmean(diff_x[i])\n",
    "    mean_rec[i] = np.nanmean(diff_rec[i]) \n",
    "    error_x[i] = np.nanstd(diff_x[i])\n",
    "    error_rec[i] = np.nanstd(diff_rec[i])\n",
    "xxx = np.linspace(-2,10,24)\n",
    "plt.scatter(xxx, mean_x)\n",
    "plt.scatter(xxx, mean_rec, alpha = .5 )  \n",
    "plt.errorbar(xxx, mean_x,  yerr=error_x, fmt=\"o\")   \n",
    "plt.errorbar(xxx, mean_rec,  yerr=error_rec, fmt=\"o\", alpha = .5)\n",
    "plt.title('absolute error based on pion vertex, orange is rec, blue is GNN')\n",
    "plt.xlabel('Pion vertex (cm)') \n",
    "plt.ylabel('abolute error from True (cm)')\n",
    " #############################################################\n",
    "plt.figure(2) ####################### \n",
    "diff_x = [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
    "diff_rec  = [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
    "for j in range(1,24):\n",
    "    indices = np.where(np.logical_and(pi_yy < -2+j/2, pi_yy > -2+j/2-.5) ) \n",
    "    dif_x =[ (pi_x[indices] - pi_yy[indices])]\n",
    "    dif_rec = [(pi_rec_y[indices] - pi_yy[indices])]\n",
    "    diff_x[j-1].append(dif_x)\n",
    "    diff_rec[j-1].append(dif_rec)\n",
    "mean_x = np.zeros(24)\n",
    "mean_rec = np.zeros(24)\n",
    "error_x = np.zeros(24)\n",
    "error_rec = np.zeros(24)\n",
    "for i in range(24): \n",
    "    mean_x[i] = np.nanmean(diff_x[i])\n",
    "    mean_rec[i] = np.nanmean(diff_rec[i]) \n",
    "    error_x[i] = np.nanstd(diff_x[i])\n",
    "    error_rec[i] = np.nanstd(diff_rec[i])\n",
    "xxx = np.linspace(-2,10,24)\n",
    "plt.scatter(xxx, mean_x)\n",
    "plt.scatter(xxx, mean_rec, alpha = .5 )  \n",
    "plt.errorbar(xxx, mean_x,  yerr=error_x, fmt=\"o\")   \n",
    "plt.errorbar(xxx, mean_rec,  yerr=error_rec, fmt=\"o\", alpha = .5)\n",
    "plt.title(' error based on pion vertex, orange is rec, blue is GNN')\n",
    "plt.xlabel('Pion vertex (cm)') \n",
    "plt.ylabel(' error from True (cm)')\n",
    " #############################################################\n",
    "\n",
    "pi_mas = []\n",
    "pi_LamT = []\n",
    "#p_rec_y = []\n",
    "for i, data in enumerate(dataset):\n",
    "    data = data#.to(device) #put to GPU\n",
    "    #break\n",
    "    #print(data.LamT)\n",
    "    for j in range(0,int(len(data.imass))): \n",
    "        pi_mas.append(data.imass[j]) \n",
    "        pi_LamT.append(data.LamT[j]) \n",
    "Total_signal = pi_LamT.count(1)\n",
    "print('S/B')\n",
    "print('Purity') \n",
    "print('Efficienty')\n",
    "s_to_b_values = []\n",
    "purity_values = []\n",
    "efficiency_values = []\n",
    "for j in range(0, 20):\n",
    "    pi_LamTt = []\n",
    "    #imass\n",
    "    print('cut ',j/2)\n",
    "    for i in range(len(pi_x)): \n",
    "        if pi_x[i]>j/2:\n",
    "            #imass.append(pi_mas[i])\n",
    "            pi_LamTt.append(pi_LamT[i])\n",
    "    print(pi_LamTt.count(1))\n",
    "    #print(pi_LamT.count(0))\n",
    "    print(pi_LamTt.count(1)/pi_LamTt.count(0))\n",
    "    print(pi_LamTt.count(1)/(len(pi_LamTt)))\n",
    "    print(pi_LamTt.count(1)/(Total_signal))\n",
    "    #print(pi_LamTt.count(1)/len(pi_LamTt))\n",
    "    s_to_b = pi_LamTt.count(1)/pi_LamTt.count(0)\n",
    "    purity = pi_LamTt.count(1)/(len(pi_LamTt))\n",
    "    efficiency = pi_LamTt.count(1)/(Total_signal)\n",
    "    s_to_b_values.append(s_to_b)\n",
    "    purity_values.append(purity)\n",
    "    efficiency_values.append(efficiency)\n",
    "    print()\n",
    "    plt.figure(3)\n",
    "    plt.scatter(j/2, pi_LamTt.count(1)/pi_LamTt.count(0), color = 'blue')\n",
    "    plt.xlabel('pion vetex cut')\n",
    "    plt.ylabel('S/B') \n",
    "    plt.title('Graph of S/B as pion vertex cuts')\n",
    "    \n",
    "    plt.figure(4)\n",
    "    plt.scatter(j/2, pi_LamTt.count(1)/(len(pi_LamTt)), color = 'red')\n",
    "    plt.xlabel('pion vetex cut')\n",
    "    plt.ylabel('Purity') \n",
    "    plt.title('Graph of Purity as pion vertex cuts')\n",
    "                \n",
    "    plt.figure(5)\n",
    "    plt.scatter(j/2, pi_LamTt.count(1)/(Total_signal), color = 'purple') \n",
    "    plt.xlabel('pion vetex cut')\n",
    "    plt.ylabel('Efficiency ') \n",
    "    plt.title('Graph of Efficiency  as pion vertex cuts')\n",
    "    #print(len(pi_LamT))\n",
    "    #plt.figure()\n",
    "    #plt.title('cut'+str( j/2))  \n",
    "    #plt.hist(imass, bins = 25)\n",
    "##################################################################new stufff. \n",
    "pi_x = np.zeros(len(outt)) \n",
    "for i in range(len(outt)):\n",
    "    pi_x[i] = outt[i][0]\n",
    "pi_yy = [] \n",
    "#p_yy = [] \n",
    "#pi_x = outt\n",
    "pi_rec_y = []\n",
    "#p_rec_y = []\n",
    "for i, data in enumerate(dataset):\n",
    "    data = data#.to(device) #put to GPU\n",
    "    jj= 0\n",
    "    for j in range(0,int(len(data.y))): \n",
    "        pi_yy.append(data.y[j].item() -data.elc[j] ) \n",
    "      \n",
    "    for j in range(len(data.rec)):\n",
    "        pi_rec_y.append(data.rec[j]-data.elc[j])\n",
    "\n",
    "plott_pi = np.zeros((len(pi_x),3))\n",
    "for i in range(len(pi_x)):\n",
    "    if pi_rec_y[i]>4:\n",
    "        if pi_rec_y[i]<6:\n",
    "            plott_pi[i][0] = pi_yy[i]\n",
    "            plott_pi[i][1] = pi_x[i]# [0]\n",
    "\n",
    "for i in range(len(pi_x)): \n",
    "    if pi_rec_y[i]>4:\n",
    "        if pi_rec_y[i]<6:\n",
    "            plott_pi[i][2] = pi_rec_y[i] \n",
    "#plott_pi \n",
    "\n",
    "plt.figure(7) #############################################################\n",
    "pi_yy = np.array(pi_yy)\n",
    "pi_x = np.array(pi_x)\n",
    "pi_rec_y = np.array(pi_rec_y)#np.logical_and\n",
    "indices_two = np.where(np.logical_and(pi_yy < -1.5, pi_yy > -2) ) \n",
    "#indices_two = np.where(pi_yy <-1.5 and pi_yy>-2 )\n",
    "#print(indices_two)\n",
    "#print(pi_x[indices_two])  \n",
    "\n",
    "diff_x = [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
    "diff_rec  = [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
    "for j in range(1,24):\n",
    "    indices = np.where(np.logical_and(pi_rec_y < -2+j/2, pi_rec_y > -2+j/2-.5) ) \n",
    "    dif_x =[ abs(pi_x[indices] - pi_yy[indices])]\n",
    "    dif_rec = [abs(pi_rec_y[indices] - pi_yy[indices])]\n",
    "    diff_x[j-1].append(dif_x)\n",
    "    diff_rec[j-1].append(dif_rec)\n",
    "mean_x = np.zeros(24)\n",
    "mean_rec = np.zeros(24)\n",
    "error_x = np.zeros(24)\n",
    "error_rec = np.zeros(24)\n",
    "for i in range(24): \n",
    "    mean_x[i] = np.nanmean(diff_x[i])\n",
    "    mean_rec[i] = np.nanmean(diff_rec[i]) \n",
    "    error_x[i] = np.nanstd(diff_x[i])\n",
    "    error_rec[i] = np.nanstd(diff_rec[i])\n",
    "xxx = np.linspace(-2,10,24)\n",
    "print('based on rec')\n",
    "print(mean_x)\n",
    "print(mean_rec)\n",
    "print(error_x)\n",
    "print(error_rec)\n",
    "print()\n",
    "plt.scatter(xxx, mean_x)\n",
    "plt.scatter(xxx, mean_rec, alpha = .5 )  \n",
    "plt.errorbar(xxx, mean_x,  yerr=error_x, fmt=\"o\")   \n",
    "plt.errorbar(xxx, mean_rec,  yerr=error_rec, fmt=\"o\", alpha = .5)\n",
    "plt.title('Error Based on rec Vertex')\n",
    "plt.xlabel('Pion rec Vertex (cm)') \n",
    "plt.ylabel('Abolute Error from True (cm)')\n",
    "#plt.savefig('Newstuff2.png')\n",
    "\n",
    "plt.figure(8) #############################################################\n",
    "pi_yy = np.array(pi_yy)\n",
    "pi_x = np.array(pi_x)\n",
    "pi_rec_y = np.array(pi_rec_y)#np.logical_and\n",
    "indices_two = np.where(np.logical_and(pi_yy < -1.5, pi_yy > -2) ) \n",
    "#indices_two = np.where(pi_yy <-1.5 and pi_yy>-2 )\n",
    "#print(indices_two)\n",
    "#print(pi_x[indices_two])  \n",
    "\n",
    "diff_x = [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
    "diff_rec  = [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
    "for j in range(1,24):\n",
    "    indices = np.where(np.logical_and(pi_x < -2+j/2, pi_x > -2+j/2-.5) ) \n",
    "    dif_x =[ abs(pi_x[indices] - pi_yy[indices])]\n",
    "    dif_rec = [abs(pi_rec_y[indices] - pi_yy[indices])]\n",
    "    diff_x[j-1].append(dif_x)\n",
    "    diff_rec[j-1].append(dif_rec)\n",
    "mean_x = np.zeros(24)\n",
    "mean_rec = np.zeros(24)\n",
    "error_x = np.zeros(24)\n",
    "error_rec = np.zeros(24)\n",
    "for i in range(24): \n",
    "    mean_x[i] = np.nanmean(diff_x[i])\n",
    "    mean_rec[i] = np.nanmean(diff_rec[i]) \n",
    "    error_x[i] = np.nanstd(diff_x[i])\n",
    "    error_rec[i] = np.nanstd(diff_rec[i])\n",
    "xxx = np.linspace(-2,10,24)\n",
    "print('based on GNN')\n",
    "print(mean_x)\n",
    "print(mean_rec)\n",
    "print(error_x)\n",
    "print(error_rec)\n",
    "print() \n",
    "plt.scatter(xxx, mean_x)\n",
    "plt.scatter(xxx, mean_rec, alpha = .5 )  \n",
    "plt.errorbar(xxx, mean_x,  yerr=error_x, fmt=\"o\")   \n",
    "plt.errorbar(xxx, mean_rec,  yerr=error_rec, fmt=\"o\", alpha = .5)\n",
    "plt.title('Error based on GNN vertex (abs)')\n",
    "plt.xlabel('Pion GNN vertex (cm)') \n",
    "plt.ylabel('Abolute Error from True (cm)')\n",
    "#plt.savefig('Newstuff3.png') \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "outt= [] \n",
    "pi_momentum  = []\n",
    "#for i,data in enumerate(dataset): \n",
    "for i,data in enumerate(dataset):\n",
    "    data = data.to(device) #switch to GPU                                                                                                                                                                                                 \n",
    "    #optimizer.zero_grad() #                                                                                                                                                                                                               \n",
    "    #out = model(data.x, data.edge_index, data.batch).to(device)  # Perform a single forward pass                                                                                                                                          \n",
    "    #out = out.cpu()\n",
    "    #print(out) \n",
    "    #print(data.elc)  \n",
    "   # neww = []\n",
    "    #for j in range(len(data)):\n",
    "        #neww.append(out[j].item() -data.elc[j][0])\n",
    "     #   neww.append(out[j].item() -data.elc[j])\n",
    "    #outt+=[[neww]]\n",
    "    #outt+=[[out[0][0].item()-data.elc[0]]] \n",
    "    #outt+=[[out[0][0].item()-data.elc[0]]]\n",
    "    for j in range(0,int(len(data.momentumt))): \n",
    "        pi_momentum.append(data.momentumt[j] )  \n",
    "        \n",
    "pi_momentum = np.array(pi_momentum) \n",
    "plt.figure(9) ####################### \n",
    "diff_x = [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
    "diff_rec  = [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
    "for j in range(1,20):\n",
    "    indices = np.where(np.logical_and(pi_momentum < 0+j/4, pi_momentum > 0+j/4-.25) )  \n",
    "    dif_x =[ abs(pi_x[indices] - pi_yy[indices])]\n",
    "    dif_rec = [abs(pi_rec_y[indices] - pi_yy[indices])]\n",
    "    diff_x[j-1].append(dif_x)\n",
    "    diff_rec[j-1].append(dif_rec)\n",
    "mean_x = np.zeros(20)\n",
    "mean_rec = np.zeros(20)  \n",
    "error_x = np.zeros(20)\n",
    "error_rec = np.zeros(20)\n",
    "for i in range(20): \n",
    "    mean_x[i] = np.nanmean(diff_x[i])\n",
    "    mean_rec[i] = np.nanmean(diff_rec[i]) \n",
    "    error_x[i] = np.nanstd(diff_x[i])\n",
    "    error_rec[i] = np.nanstd(diff_rec[i])\n",
    "xxx = np.linspace(0,5,20) \n",
    "plt.scatter(xxx, mean_x)\n",
    "plt.scatter(xxx, mean_rec, alpha = .5 )  \n",
    "plt.errorbar(xxx, mean_x,  yerr=error_x, fmt=\"o\")   \n",
    "plt.errorbar(xxx, mean_rec,  yerr=error_rec, fmt=\"o\", alpha = .5)\n",
    "plt.title(' Error based on Momentum')\n",
    "plt.xlabel('Pion momentum (cm)') \n",
    "plt.ylabel(' Error from True (cm)')  \n",
    "\n",
    "\n",
    "\n",
    "for iiii in range(10):  \n",
    "    plt.figure(iiii)\n",
    "    idddddk = 'big_test'+str(iiii)+'.png'\n",
    "    plt.savefig(idddddk)    \n",
    "print('doneeeeeee')\n",
    "np.save('s_to_b_valuesHnrecsingle1100.npy', s_to_b_values)\n",
    "np.save('purity_valuesHnrecsingle1100.npy', purity_values) \n",
    "np.save('efficiency_valuesHnrecsingle1100.npy', efficiency_values)'''      \n",
    "\n",
    "f = open(\"/hpc/group/vossenlab/kam264/dir_name_2/batch_filer.py\", \"w\")   \n",
    "#f = open(\"/hpc/group/vossenlab/mfm45/forkeegan/batch_filerr.py\", \"w\")  \n",
    "f.write(code)              \n",
    "f.close()          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a544b1b3-4870-4b90-a1bd-36504cd9c829",
   "metadata": {},
   "outputs": [],
   "source": [
    " 15500514"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3fcd05-56b4-4383-b534-5464d4332d50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fe25d3-1d3c-4c3b-97f5-cbf8b1218a6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
